{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308330da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from rouge) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from transformers import T5Tokenizer\n",
    "from tqdm import trange\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from utils import compute_rouges, save_dataset, read_dataset, set_seed, save_model\n",
    "from model.modeling_genmc import GenMC\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "def get_input_feature(samples, max_source_length, max_len_gen, choice_num, external_sent_num=None):\n",
    "    sep = ' \\\\n '\n",
    "    output_clue = []\n",
    "    answers = []\n",
    "    input_ids_q, attention_mask_q = [], []\n",
    "    input_ids_qo, attention_mask_qo = [], []\n",
    "    for sample in samples:\n",
    "        if 'answerKey' in sample:\n",
    "            answerKey = sample['answerKey']\n",
    "        else:\n",
    "            answerKey = \"A\"\n",
    "        question = sample['question']['stem']\n",
    "        while len(sample['question']['choices']) < choice_num:\n",
    "            sample['question']['choices'].append({\"text\": \"error\", \"para\": \"\", \"label\":chr(ord('A')+len(sample)-1)})\n",
    "        for o_i, (opt, opt_name) in enumerate(zip(sample['question']['choices'], 'ABCDEFGH'[:choice_num])):\n",
    "            option = opt['text']\n",
    "            content = \"\"\n",
    "            if external_sent_num is not None and 'para' in opt:\n",
    "                para = opt[\"para\"]\n",
    "                if isinstance(para, list):\n",
    "                    if len(para) > external_sent_num:\n",
    "                        para = para[:external_sent_num]\n",
    "                    content = sep + \" \".join(para)\n",
    "                elif isinstance(para, str):\n",
    "                    para = para.split(\".\")\n",
    "                    if len(para) > external_sent_num:\n",
    "                        para = para[:external_sent_num]\n",
    "                    content = sep + \" \".join(para)\n",
    "                else:\n",
    "                    print('lack retrieval')\n",
    "                    # exit(0)\n",
    "            input_ids_qo.append(question + sep + option + content)\n",
    "\n",
    "\n",
    "        input_ids_q.append(question + sep)\n",
    "        if answerKey in '123456':\n",
    "            answer = ord(answerKey) - ord('1')\n",
    "        else:\n",
    "            answer = ord(answerKey) - ord('A')\n",
    "        answers.append(answer)\n",
    "        output_clue.append(sample['question']['choices'][answer]['text'])\n",
    "\n",
    "    def tokenizer_fun(input_ids, max_len):\n",
    "        encoding = tokenizer(input_ids,\n",
    "                             padding='longest',\n",
    "                             max_length=max_len,\n",
    "                             truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        ids = encoding.input_ids.to(device)\n",
    "        mask = encoding.attention_mask.to(device)\n",
    "        return ids, mask\n",
    "\n",
    "    q_ids, q_mask = tokenizer_fun(input_ids_q, max_source_length)\n",
    "    qo_ids, qo_mask = tokenizer_fun(input_ids_qo, max_source_length)\n",
    "    clue_ids, _ = tokenizer_fun(output_clue, max_len_gen)\n",
    "    clue_ids = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in\n",
    "        clue_ids\n",
    "    ]\n",
    "    clue_ids = torch.tensor(clue_ids, dtype=torch.long).to(device)\n",
    "    answers = torch.tensor(answers, dtype=torch.long).to(device)\n",
    "    return q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, output_clue\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, test_examples, tokenizer, eval_batch_size, choice_num, max_len, max_len_gen, external_sent_num):\n",
    "    count, count_right = 0, 0\n",
    "    results = []\n",
    "    model.eval()\n",
    "    step_count = len(test_examples) // eval_batch_size\n",
    "    if step_count * eval_batch_size < len(test_examples):\n",
    "        step_count += 1\n",
    "    step_trange = trange(step_count)\n",
    "    sources, targets = [], []\n",
    "    for step in step_trange:\n",
    "        beg_index = step * eval_batch_size\n",
    "        end_index = min((step + 1) * eval_batch_size, len(test_examples))\n",
    "        batch_example = [example for example in test_examples[beg_index:end_index]]\n",
    "        q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, output_clue = get_input_feature(batch_example,\n",
    "                                                                                           max_len, max_len_gen,\n",
    "                                                                                           args.choice_num,\n",
    "                                                                                           external_sent_num)\n",
    "        scores, output_sequences = model(q_ids, q_mask, qo_ids, qo_mask, choice_num)\n",
    "\n",
    "        scores = scores.cpu().detach().tolist()\n",
    "        answers = answers.cpu().detach().tolist()\n",
    "        p_anss = []\n",
    "        for p, a, example in zip(scores, answers, batch_example):\n",
    "            p_ans = p.index(max(p))\n",
    "            p_anss.append(example['question']['choices'][p_ans]['label'])\n",
    "            if p_ans == a:\n",
    "                count_right += 1\n",
    "            count += 1\n",
    "        for sample, p_ans in zip(batch_example, p_anss):\n",
    "            qid = sample['id']\n",
    "            results.append(qid + \",\" + p_ans)\n",
    "        predicts = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "        sources += predicts\n",
    "        targets += output_clue\n",
    "\n",
    "    rouge_score = compute_rouges(sources, targets)['rouge-l']\n",
    "\n",
    "    return count_right / count, rouge_score, results\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfef1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_path\",\n",
    "                    default='t5-base',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--choice_num\",\n",
    "                    default=5,\n",
    "                    type=int)\n",
    "parser.add_argument(\"--data_path_train\",\n",
    "                    default='./data/csqa/in_hourse/train.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--data_path_dev\",\n",
    "                    default='./data/csqa/in_hourse/dev.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--data_path_test\",\n",
    "                    default='./data/csqa/in_hourse/test.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--results_save_path\",\n",
    "                    default='./results/',\n",
    "                    type=str)\n",
    "parser.add_argument(\"--train_batch_size\",\n",
    "                    default=64,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                    default=8,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument('--gradient_accumulation_steps',\n",
    "                    type=int,\n",
    "                    default=4,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "\n",
    "parser.add_argument(\"--output_dir\",\n",
    "                    default='./outputs/',\n",
    "                    type=str,\n",
    "                    help=\"The output dreader2ctory whretriever the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--init_checkpoint\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"Initial checkpoint (usually from a pre-trained BERT model)\")\n",
    "parser.add_argument(\"--max_len\",\n",
    "                    default=64,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_len_gen\",\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total output sequence length for decoder\")\n",
    "parser.add_argument(\"--lr\",\n",
    "                    default=1e-5,\n",
    "                    type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--epoch_num\",\n",
    "                    default=30,\n",
    "                    type=int,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument('--num_hidden_layers',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"The number of hidden layer for co-matching and encoder-decoder interaction transformer\")\n",
    "parser.add_argument('--alpha',\n",
    "                    type=float,\n",
    "                    default=1)\n",
    "parser.add_argument('--beta',\n",
    "                    type=float,\n",
    "                    default=1)\n",
    "parser.add_argument('--seed',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--name_save_prix\",\n",
    "                    default='GenMC_CSQA',\n",
    "                    type=str)\n",
    "parser.add_argument('--external_sent_num',\n",
    "                    type=int,\n",
    "                    default=None,\n",
    "                    help=\"The number of retrieved sentences\")\n",
    "\n",
    "args = parser.parse_args([\"--model_path\", \"t5-base\", \"--choice_num\", \"5\", \n",
    "                          \"--data_path_train\", \"./data/csqa/in_hourse/train.jsonl\",  \n",
    "                          \"--data_path_dev\", \"./data/csqa/in_hourse/dev.jsonl\",  \n",
    "                          \"--data_path_test\", \"./data/csqa/in_hourse/test.jsonl\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affe1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"lr\": 1e-05,\n",
      "  \"model\": \"t5-base\",\n",
      "  \"seed\": 1,\n",
      "  \"bs\": 64,\n",
      "  \"gradient_accumulation_steps\": 4,\n",
      "  \"epoch\": 30,\n",
      "  \"train_path\": \"./data/csqa/in_hourse/train.jsonl\",\n",
      "  \"dev_path\": \"./data/csqa/in_hourse/dev.jsonl\",\n",
      "  \"test_path\": \"./data/csqa/in_hourse/test.jsonl\",\n",
      "  \"train_size\": 9741,\n",
      "  \"dev_size\": 1221,\n",
      "  \"test_size\": 1221,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"external_sent_num\": null,\n",
      "  \"alpha\": 1,\n",
      "  \"beta\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [01:11<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_dev_acc: 0.16953316953316952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [05:57<00:00,  1.71it/s,  Epoch:0 loss:7.708]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:32<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.37346437346437344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:33<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.37346437346437344 test_acc: 0.37346437346437344 rouge: 0.21395244651900808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [04:31<00:00,  2.25it/s,  Epoch:1 loss:6.5409]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:29<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.4406224406224406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:29<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.4406224406224406 test_acc: 0.4406224406224406 rouge: 0.2946587899800346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:59<00:00,  2.55it/s,  Epoch:2 loss:5.9917]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.4488124488124488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:25<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.4488124488124488 test_acc: 0.4488124488124488 rouge: 0.3242695505297438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:44<00:00,  2.71it/s,  Epoch:3 loss:5.6409]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5004095004095004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5004095004095004 test_acc: 0.5004095004095004 rouge: 0.3443168763312211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:46<00:00,  2.68it/s,  Epoch:4 loss:5.388]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5085995085995086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5085995085995086 test_acc: 0.5085995085995086 rouge: 0.3548056480788199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:46<00:00,  2.69it/s,  Epoch:5 loss:5.1928]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5274365274365275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5274365274365275 test_acc: 0.5274365274365275 rouge: 0.3611989637739838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:43<00:00,  2.72it/s,  Epoch:6 loss:5.0396]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5413595413595413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5413595413595413 test_acc: 0.5413595413595413 rouge: 0.3652083759696278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:41<00:00,  2.75it/s,  Epoch:7 loss:4.9096]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5446355446355446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5446355446355446 test_acc: 0.5446355446355446 rouge: 0.3709850497265586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:43<00:00,  2.73it/s,  Epoch:8 loss:4.7989]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5634725634725635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5634725634725635 test_acc: 0.5634725634725635 rouge: 0.37435872753233346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:40<00:00,  2.76it/s,  Epoch:9 loss:4.7026]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5708435708435708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:25<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5708435708435708 test_acc: 0.5708435708435708 rouge: 0.37805412301352137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:33<00:00,  2.85it/s,  Epoch:10 loss:4.618]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5872235872235873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:24<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best dev acc: 0.5872235872235873 test_acc: 0.5872235872235873 rouge: 0.3817213981111087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:31<00:00,  2.89it/s,  Epoch:11 loss:4.5439]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.579033579033579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:30<00:00,  2.89it/s,  Epoch:12 loss:4.4755]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5855855855855856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:30<00:00,  2.90it/s,  Epoch:13 loss:4.4135]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:22<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5864045864045864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:30<00:00,  2.90it/s,  Epoch:14 loss:4.356]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:23<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5831285831285832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 609/609 [03:39<00:00,  2.78it/s,  Epoch:15 loss:4.3028]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:25<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc: 0.5823095823095823\n",
      "best dev acc: 0.5872235872235873 best_test_acc: 0.5872235872235873 best_dev_rouge_score: 0.3817213981111087 best_test_rouge_score: 0.3817213981111087\n"
     ]
    }
   ],
   "source": [
    "file_name = f'lr_{args.lr}_seed_{args.seed}_bs_{args.train_batch_size}_ga_{args.gradient_accumulation_steps}_layer_num_{args.num_hidden_layers}_alpha_{args.alpha}_beta_{args.beta}'\n",
    "output_model_path = './outputs/' + args.name_save_prix + '/' + file_name + \"/\"\n",
    "path_save_result = './results/' + args.name_save_prix + '/' + file_name + \"/\"\n",
    "\n",
    "os.makedirs(path_save_result, exist_ok=True)\n",
    "set_seed(args.seed)\n",
    "train_examples = read_dataset(args.data_path_train)\n",
    "dev_examples = read_dataset(args.data_path_dev)\n",
    "test_examples = read_dataset(args.data_path_test)\n",
    "\n",
    "train_examples = train_examples + dev_examples\n",
    "dev_examples = test_examples\n",
    "test_examples = test_examples\n",
    "\n",
    "print(json.dumps({\"lr\": args.lr, \"model\": args.model_path, \"seed\": args.seed,\n",
    "                  \"bs\": args.train_batch_size,\n",
    "                  'gradient_accumulation_steps': args.gradient_accumulation_steps,\n",
    "                  \"epoch\": args.epoch_num,\n",
    "                  \"train_path\": args.data_path_train,\n",
    "                  \"dev_path\": args.data_path_dev,\n",
    "                  \"test_path\": args.data_path_test,\n",
    "                  \"train_size\": len(train_examples),\n",
    "                  \"dev_size\": len(dev_examples),\n",
    "                  \"test_size\": len(test_examples),\n",
    "                  'num_hidden_layers': args.num_hidden_layers,\n",
    "                  'external_sent_num': args.external_sent_num,\n",
    "                  \"alpha\": args.alpha, \"beta\": args.beta}, indent=2))\n",
    "\n",
    "train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "tokenizer = T5Tokenizer.from_pretrained(args.model_path)\n",
    "model = GenMC(args.model_path, args.num_hidden_layers, args.alpha, args.beta)\n",
    "\n",
    "if args.init_checkpoint is not None:\n",
    "    checkpoint = torch.load(args.init_checkpoint, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "\n",
    "step_count, step_all, early_stop = 0, 0, 0\n",
    "best_dev_rouge_score, best_test_rouge_score = 0, 0\n",
    "tr_loss, nb_tr_steps = 0, 0\n",
    "\n",
    "best_dev_acc, _, _ = eval(model, dev_examples, tokenizer, args.eval_batch_size, args.choice_num, args.max_len,\n",
    "                          args.max_len_gen, args.external_sent_num)\n",
    "print('best_dev_acc:',best_dev_acc)\n",
    "best_test_acc = 0\n",
    "for epoch in range(args.epoch_num):\n",
    "    early_stop += 1\n",
    "    order = list(range(len(train_examples)))\n",
    "    random.seed(args.seed + epoch)\n",
    "    random.shuffle(order)\n",
    "    model.train()\n",
    "    step_count = len(train_examples) // train_batch_size\n",
    "    if step_count * train_batch_size < len(train_examples):\n",
    "        step_count += 1\n",
    "    step_trange = trange(step_count)\n",
    "    for step in step_trange:\n",
    "        step_all += 1\n",
    "        beg_index = step * train_batch_size\n",
    "        end_index = min((step + 1) * train_batch_size, len(train_examples))\n",
    "        order_index = order[beg_index:end_index]\n",
    "        batch_example = [train_examples[index] for index in order_index]\n",
    "        q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, output_clue = get_input_feature(\n",
    "            batch_example,\n",
    "            max_source_length=args.max_len,\n",
    "            max_len_gen=args.max_len_gen,\n",
    "            choice_num=args.choice_num,\n",
    "            external_sent_num=args.external_sent_num)\n",
    "        loss = model(q_ids, q_mask, qo_ids, qo_mask, args.choice_num, clue_ids, answers)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_show = ' Epoch:' + str(epoch) + \" loss:\" + str(round(tr_loss / nb_tr_steps, 4))\n",
    "        step_trange.set_postfix_str(loss_show)\n",
    "\n",
    "    dev_acc, dev_rouge_score, results_dev = eval(model, dev_examples, tokenizer, args.eval_batch_size,\n",
    "                                                 args.choice_num, args.max_len, args.max_len_gen,\n",
    "                                                 args.external_sent_num)\n",
    "    print('dev_acc:', dev_acc)\n",
    "    if dev_acc > best_dev_acc:\n",
    "        save_dataset(path_save_result + '/dev.csv', results_dev)\n",
    "        early_stop = 0\n",
    "        test_acc, test_rouge_score, results_test = eval(model, test_examples, tokenizer, args.eval_batch_size,\n",
    "                                                        args.choice_num, args.max_len, args.max_len_gen,\n",
    "                                                        args.external_sent_num)\n",
    "        save_dataset(path_save_result + '/test.csv', results_test)\n",
    "        best_dev_acc, best_test_acc, best_dev_rouge_score, best_test_rouge_score = dev_acc, test_acc, dev_rouge_score, test_rouge_score\n",
    "\n",
    "        # save_model(output_model_path, model, optimizer)\n",
    "        print('new best dev acc:', dev_acc, 'test_acc:', test_acc, 'rouge:', dev_rouge_score)\n",
    "\n",
    "    if early_stop >= 5:\n",
    "        break\n",
    "\n",
    "print('best dev acc:', best_dev_acc, 'best_test_acc:', best_test_acc,\n",
    "      'best_dev_rouge_score:', best_dev_rouge_score, 'best_test_rouge_score:', best_test_rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
