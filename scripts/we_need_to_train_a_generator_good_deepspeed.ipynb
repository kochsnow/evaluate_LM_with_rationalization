{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3494161",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n",
    "!pip install --upgrade wandb -qq # experiment tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e42ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30300ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt3\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Any, NewType\n",
    "\n",
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "#from feature_conversion_methods import format_instance\n",
    "\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments\n",
    ")\n",
    "from metrics import evaluate\n",
    "import torch\n",
    "import datasets\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import random \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "from copy import deepcopy \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers.logging.set_verbosity_info()\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "set_global_logging_level(logging.ERROR, [\"datasets\"])\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "\n",
    "\n",
    "def set_other_seeds(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# inspired by DefaultDataCollator from:\n",
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "# modified to perform batch-level padding.\n",
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa5a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\"\"\"\n",
    "Example-to-Feature conversion methods\n",
    "Modified from\n",
    "https://github.com/salesforce/cos-e/blob/master/code/generation/train_commonsenseqa_v1.0.py and \"\"_v1.11.py (identical)\n",
    "as well as Tensorflow code for WTF?: \n",
    "https://github.com/google-research/google-research/blob/master/wt5/wt5/preprocessors.py\n",
    "\"\"\"\n",
    "# This code is based on https://github.com/allenai/label_rationale_association/blob/main/feature_conversion_methods.py\n",
    "\n",
    "unified_qa_esnli_label_mapping = {0: 'yes', 1: 'maybe', 2: 'no'}\n",
    "unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'} \n",
    "wt5_esnli_label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'} \n",
    "unified_qa_sbic_label_mapping = {\"offensive\": 'Yes', \"not offensive\": 'No'}\n",
    "\n",
    "def format_instance(\n",
    "        example,\n",
    "        tokenizer,\n",
    "        explanation_sep,\n",
    "        max_seq_length=None,\n",
    "        datasource=None,\n",
    "        io_format=None, \n",
    "):\n",
    "    assert datasource in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "    if datasource in [\"cos_e\", \"ecqa\"]:\n",
    "        input_string, answer_string = cqa_formatting(example, io_format, explanation_sep, datasource)\n",
    "    elif datasource == \"esnli\":\n",
    "        input_string, answer_string = esnli_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sbic':\n",
    "        input_string, answer_string = sbic_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sensemaking':\n",
    "        input_string, answer_string = sensemaking_formatting(example, io_format, explanation_sep)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task. Currently supported: esnli, cos_e, sbic, sensemaking, ecqa.\")\n",
    "    \n",
    "    if 'unified' in io_format and 'unifew' not in io_format:\n",
    "        input_string += '</s>'\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        input_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # note even with \"lm_labels.shift_right()\", the decoder attention mask length is still correct since we remove the last token\n",
    "    dec = tokenizer.encode_plus(\n",
    "        answer_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    encodings[\"labels\"] = dec[\"input_ids\"]\n",
    "    encodings[\"decoder_attention_mask\"] = dec[\"attention_mask\"]\n",
    "    encodings[\"question_encoding\"] = encodings[\"input_ids\"]\n",
    "\n",
    "    #return encodings\n",
    "    return {**example, **encodings}\n",
    "\n",
    "#这里很简单 定义好输入输出string就可以的\n",
    "def cqa_formatting(item, io_format, explanation_sep, datasource):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    abstr_expl = item[\"abstractive_explanation\"].lower() if datasource == 'cos_e' else item[\"explanation\"].lower()\n",
    "\n",
    "\n",
    "    if io_format == 't5_fewshot_infilling_with_choices':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" The answer is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain {datasource} question: {question} context: \" + ', '.join(item['choices']) # explain cos_e question: When getting in shape you need to have this in between workouts? context: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == \"record\": \n",
    "        # might not work because cos_e doesn't have a passage \n",
    "        input_string = f\"explain {datasource} query: {question} entities: \" + ', '.join(item['choices']) # explain cos_e query: When getting in shape you need to have this in between workouts? entities: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == 'unifiedqa_matching':\n",
    "        choice_ids = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        input_string = f'explain {question.lower()} \\\\n'\n",
    "        for choice_id, choice in zip(choice_ids, item[\"choices\"]):\n",
    "            input_string += f' {choice_id} {choice.lower()}'\n",
    "        answer_string = f\"{answer.lower()} {explanation_sep} {abstr_expl.lower()}\"\n",
    "        answer_string = answer_string.lower()\n",
    "    elif io_format == 't5_fewshot_infilling_without_choices_use_refined_expl':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        input_string = f\"explain {datasource} question: {question} answer: {answer}\" + f\" {explanation_sep} <extra_id_0>\"\n",
    "        answer_string = f\"<extra_id_0> {item['our_explanation']} <extra_id_1>\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "    \n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def esnli_formatting(item, io_format, explanation_sep):\n",
    "\n",
    "    premise = item[\"premise\"]\n",
    "    hypothesis = item[\"hypothesis\"]\n",
    "    answer = unified_qa_esnli_label_mapping[item[\"label\"]] if 'unified' in io_format else wt5_esnli_label_mapping[item[\"label\"]]\n",
    "    abstr_expl = item[\"explanation_1\"].lower() \n",
    "    # Dev/test instances have more than one explanation annotated; merge them into one sequence separated by [SEP] \n",
    "    for k in [2,3]:\n",
    "        if f\"explanation_{k}\" in item and item[f'explanation_{k}']!='': \n",
    "            abstr_expl += f\" [SEP] {item[f'explanation_{k}'].lower()}\"\n",
    "\n",
    "    if io_format == 'standard':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: {hypothesis} {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\" \n",
    "    elif io_format == \"squad_endswith_what\":\n",
    "        input_string = f\"explain nli question: What is this? context: {hypothesis} {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix_endswith_what\":  \n",
    "        input_string = f\"explain nli question: What is this? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"   \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        hypothesis = hypothesis.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        answer = unified_qa_esnli_label_mapping_upper[item[\"label\"]]\n",
    "        input_string = f'explain {premise} Is {hypothesis}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_unifew_nli_mix':\n",
    "        premise = premise.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        input_string = f'explain hypothesis: {hypothesis} Is premise: {premise}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}' \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm_with_choices': \n",
    "        input_string = f'explain is this entailment? \\\\n (A) yes (B) maybe (C) no \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_with_choices_v2': \n",
    "        input_string = f'explain what is this? \\\\n (A) entailment (B) neutral (C) contradiction \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"     \n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported.\")\n",
    "\n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def sbic_formatting(item, io_format, explanation_sep):\n",
    "    # We pre-processed the SBIC dataset such that we join multiple implied statements with the [SEP] token for dev/test instances \n",
    "    # Each annotation in the training split is a separate instance\n",
    "    post = item['post']\n",
    "    answer = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] if 'unified' in io_format else item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "    abstr_expl = item[\"targetStereotype\"]\n",
    "    \n",
    "\n",
    "    if io_format == 'cola':\n",
    "        input_string = f\"explain sbic sentence: {post}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain sbic sentence: {post} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':\n",
    "        answer_bool = 'True' if item[\"offensiveYN\"] == 'offensive' else 'False'\n",
    "        input_string = f\"explain sbic sentence: {post} It is <extra_id_0> that this is offensive {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain sbic sentence: {post} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 'squad_yn': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_yn_with_tags': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: post: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_what': \n",
    "        input_string = f\"explain sbic question: What is this? context: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'squad_what_with_tags': \n",
    "        input_string = f\"explain sbic question: What is this? context: post: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'unified_qa_yn':\n",
    "        input_string = f'explain is this offensive? \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\" \n",
    "    elif io_format == 'unified_qa_yn_with_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_yn_with_choices':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_yn_with_choices_and_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what':\n",
    "        input_string = f'explain what is this? \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_tags':\n",
    "        input_string = f'explain what is this? \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_choices':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_what_with_choices_and_tags':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        input_string = f\"Topic? \\\\n (A) offensive (B) not_offensive \\\\n {post}\"\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string\n",
    "\n",
    "def sensemaking_formatting(item, io_format, explanation_sep):\n",
    "    # TODO: explore whether removing periods makes difference? \n",
    "    sent0 = item['sent0']\n",
    "    sent1 = item['sent1']\n",
    "    nonsensical_sentence = str(int(item['label'])+1)\n",
    "    explanation = item['explanation'].lower()\n",
    "\n",
    "    if io_format == 'copa_with_question':\n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} question: nonsensical\"\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 'copa_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} Less common is choice2\"\n",
    "        answer_string = f\"{answer_bool} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 't5_fewshot_infilling':  \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> choice{nonsensical_sentence} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} It is <extra_id_0> that choice2 is less common {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer_bool} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == \"squad_yn\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_yn_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"record\": \n",
    "        input_string = f\"explain sensemaking query: What is more nonsensical? entities: choice1, choice2 passage: choice1: {sent0} choice2: {sent1}\" # explain sensemaking query: What is nonsensical? entities: choice1, choice2 passage: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation.\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" # choice1 because state flowers are unique to each state.\n",
    "    elif io_format == 'unifiedqa_yn_with_choices':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n (A) yes (B) no \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\" \n",
    "    elif io_format == 'unifiedqa_yn':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_yn_no_tags':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_with_choices':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n (A) choice1 (B) choice2 \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what_no_tags':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde6c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "11/19/2022 23:22:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/19/2022 23:22:06 - INFO - __main__ -   Save path: ./cos_e_output/111922_232206\n",
      "11/19/2022 23:22:06 - INFO - __main__ -   Git branch: dev\n",
      "11/19/2022 23:22:06 - INFO - __main__ -   Git hash: 1cbb5c3b4e53baf31cbafc20d9655c63f091f901\n"
     ]
    }
   ],
   "source": [
    "og_start_time = time.time()\n",
    "\n",
    "#parser = HfArgumentParser(\n",
    "#    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#)\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, unused_args = parser.parse_args_into_dataclasses(\n",
    "    [\"--model_type\", \"t5-3b\",\n",
    "     \"--tokenizer_name\", \"t5-3b\",\n",
    "     \"--task_name\", \"cos_e\", \n",
    "     \"--output_dir\", \"./cos_e_output\", \n",
    "     \"--n_shots\", \"10\",\n",
    "     \"--do_train\", \"True\"], return_remaining_strings=True)\n",
    "if unused_args != []:\n",
    "    raise ValueError(f\"Received unused arguments: {unused_args}\")\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "\n",
    "if model_args.use_gpt3:\n",
    "    assert training_args.do_train\n",
    "    assert not training_args.do_eval\n",
    "    assert data_args.generations_filepath is None\n",
    "    if data_args.gpt3_max_eval_size is not None:\n",
    "        assert data_args.gpt3_max_eval_size <= data_args.fewshot_eval_size\n",
    "        assert data_args.gpt3_max_eval_size % 2 == 0\n",
    "        assert data_args.gpt3_max_eval_size % 3 == 0\n",
    "\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "if not training_args.do_train and data_args.generations_filepath is None:\n",
    "    if not model_args.pretrained_model_file:\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation\"\n",
    "        )\n",
    "\n",
    "if training_args.do_train:\n",
    "    # create a save directory and a logfile\n",
    "    training_args.output_dir = os.path.join(\n",
    "        training_args.output_dir, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "            os.path.exists(training_args.output_dir)\n",
    "            and os.listdir(training_args.output_dir)\n",
    "            and training_args.do_train\n",
    "            and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "model_class = \"t5\"\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "            os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "set_other_seeds(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a3172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/19/2022 23:22:06 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "11/19/2022 23:22:07 - INFO - filelock -   Lock 140595353080888 acquired on /home/huangyongfeng/.cache/huggingface/transformers/529487bfb232bc6331b488e0e3f011af7d700beb874529a38613f0c162994f36.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d.lock\n",
      "https://huggingface.co/t5-3b/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /home/huangyongfeng/.cache/huggingface/transformers/tmp39uwevsa\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018988847732543945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 791656,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1968c92f112b4b21815bc99ad9773a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/t5-3b/resolve/main/spiece.model in cache at /home/huangyongfeng/.cache/huggingface/transformers/529487bfb232bc6331b488e0e3f011af7d700beb874529a38613f0c162994f36.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "creating metadata file for /home/huangyongfeng/.cache/huggingface/transformers/529487bfb232bc6331b488e0e3f011af7d700beb874529a38613f0c162994f36.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "11/19/2022 23:22:09 - INFO - filelock -   Lock 140595353080888 released on /home/huangyongfeng/.cache/huggingface/transformers/529487bfb232bc6331b488e0e3f011af7d700beb874529a38613f0c162994f36.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d.lock\n",
      "11/19/2022 23:22:13 - INFO - filelock -   Lock 140595353080888 acquired on /home/huangyongfeng/.cache/huggingface/transformers/8cc0c6618e070737993bd96f1f5251e1cc850a347fa1ff28c378c89c66e66c80.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529.lock\n",
      "https://huggingface.co/t5-3b/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/huangyongfeng/.cache/huggingface/transformers/tmp8cbpw8nf\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013580560684204102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1389353,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea66e340f9b4a4cb506bdd7986bdcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/t5-3b/resolve/main/tokenizer.json in cache at /home/huangyongfeng/.cache/huggingface/transformers/8cc0c6618e070737993bd96f1f5251e1cc850a347fa1ff28c378c89c66e66c80.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "creating metadata file for /home/huangyongfeng/.cache/huggingface/transformers/8cc0c6618e070737993bd96f1f5251e1cc850a347fa1ff28c378c89c66e66c80.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "11/19/2022 23:22:15 - INFO - filelock -   Lock 140595353080888 released on /home/huangyongfeng/.cache/huggingface/transformers/8cc0c6618e070737993bd96f1f5251e1cc850a347fa1ff28c378c89c66e66c80.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529.lock\n",
      "loading file https://huggingface.co/t5-3b/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/529487bfb232bc6331b488e0e3f011af7d700beb874529a38613f0c162994f36.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-3b/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-3b/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-3b/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-3b/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/8cc0c6618e070737993bd96f1f5251e1cc850a347fa1ff28c378c89c66e66c80.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-3b/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/9548aafe2574163cda830b603f9821f06bbded327f57ecf09d4ad292d00f4b09.9872b28381f35eb45b1f1f839ef14943e64a0a448f611993c2e987c4d3e0844c\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-3b/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/9548aafe2574163cda830b603f9821f06bbded327f57ecf09d4ad292d00f4b09.9872b28381f35eb45b1f1f839ef14943e64a0a448f611993c2e987c4d3e0844c\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "11/19/2022 23:22:18 - INFO - filelock -   Lock 140612505794040 acquired on /home/huangyongfeng/.cache/huggingface/transformers/289687014f876f6cef27445bdc8175fd03dc36a99201b68d8e925957e4987b4f.885f97e8ec826518534b45f8b52caea4ca553494390d587dcf49492d95732676.lock\n",
      "https://huggingface.co/t5-3b/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/huangyongfeng/.cache/huggingface/transformers/tmp7j4tx612\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013636112213134766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 11406547936,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b52224eafad4ea5ae7b997816ade87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/t5-3b/resolve/main/pytorch_model.bin in cache at /home/huangyongfeng/.cache/huggingface/transformers/289687014f876f6cef27445bdc8175fd03dc36a99201b68d8e925957e4987b4f.885f97e8ec826518534b45f8b52caea4ca553494390d587dcf49492d95732676\n",
      "creating metadata file for /home/huangyongfeng/.cache/huggingface/transformers/289687014f876f6cef27445bdc8175fd03dc36a99201b68d8e925957e4987b4f.885f97e8ec826518534b45f8b52caea4ca553494390d587dcf49492d95732676\n",
      "11/19/2022 23:32:49 - INFO - filelock -   Lock 140612505794040 released on /home/huangyongfeng/.cache/huggingface/transformers/289687014f876f6cef27445bdc8175fd03dc36a99201b68d8e925957e4987b4f.885f97e8ec826518534b45f8b52caea4ca553494390d587dcf49492d95732676.lock\n",
      "loading weights file https://huggingface.co/t5-3b/resolve/main/pytorch_model.bin from cache at /home/huangyongfeng/.cache/huggingface/transformers/289687014f876f6cef27445bdc8175fd03dc36a99201b68d8e925957e4987b4f.885f97e8ec826518534b45f8b52caea4ca553494390d587dcf49492d95732676\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-3b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "model_class = \"t5\"\n",
    "tokenizer_name = TOKENIZER_MAPPING[model_class]\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "model_args.tokenizer_name='t5-3b'\n",
    "tokenizer = tokenizer_name.from_pretrained(model_args.tokenizer_name)#, cache_dir=model_args.cache_dir)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62eef1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(task_name='cos_e', early_stopping_patience=10, overwrite_cache=False, train_predict=False, test_predict=False, dev_predict=False, version_name='v1.11', generations_filepath=None, n_shots=10, fewshot_eval_size=350, io_format='t5_fewshot_infilling_without_choices_use_refined_expl', explanation_sep='explanation', data_path=None, gpt3_max_eval_size=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "data_args.io_format=\"t5_fewshot_infilling_without_choices_use_refined_expl\"\n",
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff0230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01740860939025879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3506c242501c40c6801d42a046f4d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['choices', 'question', 'extractive_explanation', 'id', 'answer', 'abstractive_explanation'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(data_args.task_name, data_args.version_name)\n",
    "train_ids_list=[x['id'] for x in dataset[\"train\"]]\n",
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12fd4f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01284027099609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b09f522ff148e1aa55a46d6e5567ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6723, 3269, 12, 6020, 7815, 0, 13, 7, 3556, 4587, 3166, 2607, 1999, 332, 8571, 2, 3520, 6, 1425, 2361, 3764, 7728, 8352, 1273, 5191, 496, 11, 8143, 6929, 4552, 4652, 7764, 6736, 401, 147, 8339, 3128, 5556, 8769, 4417, 8778, 7145, 1279, 6297, 7149, 5884, 10, 5747, 6273, 8, 9504, 8057, 1307, 2183, 1699, 88, 3228, 7178, 2140, 4048, 1041, 1, 3, 5868, 111, 7889, 5370, 8886, 2137, 6980, 9117, 1891, 7827, 1484, 14, 3668, 7090, 5, 7817, 6213, 4656, 5074, 7514, 5896, 4, 9, 7991, 58, 1887, 1598, 3994, 2212, 2274, 6830, 8826, 1604, 9008, 2013, 6521, 4793, 4338, 3923, 8091, 4904, 3938, 3900, 8156, 5806, 8363, 1787, 1496, 5753, 6351, 1104, 1222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'id': 'cabdfc174953b4bdb8bdcc89d6592c74',\n",
       "  'question': 'What is someone not legal to buy alcohol?',\n",
       "  'choices': ['underage', 'banned', 'adult', 'rules', 'black market'],\n",
       "  'answer': 'underage',\n",
       "  'abstractive_explanation': '21 is the legal age',\n",
       "  'extractive_explanation': 'not legal to buy alcohol',\n",
       "  'our_explanation': '21 is the legal age to buy alcohol in the US'},\n",
       " {'choices': \"['underage', 'banned', 'adult', 'rules', 'black market']\",\n",
       "  'question': 'What is someone not legal to buy alcohol?',\n",
       "  'id': 'cabdfc174953b4bdb8bdcc89d6592c74',\n",
       "  'orig_explanation': '21 is the legal age',\n",
       "  'our_explanation': '21 is the legal age to buy alcohol in the US',\n",
       "  'answer': 'underage',\n",
       "  'type': 'evaluation'},\n",
       " {'choices': ['underage', 'banned', 'adult', 'rules', 'black market'],\n",
       "  'question': 'What is someone not legal to buy alcohol?',\n",
       "  'extractive_explanation': 'not legal to buy alcohol',\n",
       "  'id': 'cabdfc174953b4bdb8bdcc89d6592c74',\n",
       "  'answer': 'underage',\n",
       "  'abstractive_explanation': '21 is the legal age'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rationale generation labeled data construction 115\n",
    "import pandas as pd\n",
    "scr_csqa_labeled_path=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/handwritten_cose_v1.11_examples.csv\"\n",
    "scr_csqa_label_df=pd.read_csv(scr_csqa_labeled_path)\n",
    "scr_csqa_label_data=datasets.load_dataset('csv', data_files=scr_csqa_labeled_path)\n",
    "scr_csqa_label_ids_list=[x['id'] for x in scr_csqa_label_data['train']]\n",
    "scr_csqa_indexs_list=[train_ids_list.index(id_) for id_ in scr_csqa_label_ids_list]\n",
    "scr_csqa_label_our_explanations_list=[x['our_explanation'] for x in scr_csqa_label_data['train']]\n",
    "print(scr_csqa_indexs_list)\n",
    "data_splits={}\n",
    "data_splits['train']=dataset['train'].select(scr_csqa_indexs_list)\n",
    "\n",
    "refine_train_data=[]\n",
    "for kk, (ex,da) in enumerate(zip(scr_csqa_label_our_explanations_list, data_splits['train'])):\n",
    "#     print(da)\n",
    "    data_splits['train'][kk]['our_explanation']=ex\n",
    "    #print(type(data_splits['train'][kk]),data_splits['train'][kk].keys())\n",
    "    da['our_explanation']=ex\n",
    "    refine_train_data.append(da)\n",
    "\n",
    "\n",
    "refine_train_data[0],scr_csqa_label_data['train'][0],data_splits['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ee22d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012872695922851562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f38593a96a481ca5f403057e51f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '5b8a3081c3235d62bc77e2d15f3ad454',\n",
       " 'question': 'A town between two mountains is located in a what?',\n",
       " 'choices': ['valley', 'hospital', 'state', 'train station', 'michigan'],\n",
       " 'answer': 'valley',\n",
       " 'abstractive_explanation': 'valleys are always between two mountains',\n",
       " 'extractive_explanation': 'A town between two mountains',\n",
       " 'our_explanation': 'A town in between mountains presumably would be in a valley, in which case it is plausable that it would be surrounded by heights in every direction.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rationale unlabeled data construction 991\n",
    "import pdb\n",
    "scr_csqa_unlabeled_test_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=scr_csqa_unlabeled_test_file)\n",
    "scr_csqa_unlabeled_test_df=pd.read_csv(scr_csqa_unlabeled_test_file)\n",
    "fse_csqa_dev_data_dict={}\n",
    "for kk, da in enumerate(fse_csqa_dev_dataset['train']):\n",
    "    #pdb.set_trace()\n",
    "    id_=da['Input.id']\n",
    "    if da['Answer.acceptable']:\n",
    "        answer_accept=set(da['Answer.acceptable'].split('|'))\n",
    "    else:\n",
    "        answer_accept=set()\n",
    "    explanation_list=[da['Input.explanation_1'], \n",
    "                      da['Input.explanation_2'],\n",
    "                      da['Input.explanation_3'],\n",
    "                      da['Input.explanation_4'],\n",
    "                      da['Input.explanation_5']]\n",
    "    if id_ not in fse_csqa_dev_data_dict.keys():\n",
    "        fse_csqa_dev_data_dict[id_]={\"index\":kk,\"id\":id_,\n",
    "                                     \"accept_set_list\":[answer_accept],\n",
    "                                     \"explanation_list\":explanation_list}\n",
    "        #[[kk, id_, answer_accept, explanation_list]]\n",
    "    else:\n",
    "        fse_csqa_dev_data_dict[id_][\"accept_set_list\"].append(answer_accept)\n",
    "#discriminate 3/3 id\n",
    "id_accept_expl_list=[]\n",
    "our_accept_expl_list=[]\n",
    "id_unaccept_expl_list=[]\n",
    "for k,v in fse_csqa_dev_data_dict.items():\n",
    "    accept_set_list=v['accept_set_list']\n",
    "    assert len(accept_set_list)==3\n",
    "    #pdb.set_trace()\n",
    "    common_accept_expl_sample=set.intersection(accept_set_list[0], accept_set_list[1], accept_set_list[2])\n",
    "    #print(accept_set_list,common_accept_expl_sample)\n",
    "    #pdb.set_trace()\n",
    "    if common_accept_expl_sample:\n",
    "        id_accept_expl_list.append(k)\n",
    "        our_expl=\"\"\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            if len(v['explanation_list'][idx]) > len(our_expl):\n",
    "                our_expl = v['explanation_list'][idx]\n",
    "                our_accept_expl_list.append(our_expl)\n",
    "    else:\n",
    "        id_unaccept_expl_list.append(k)\n",
    "\n",
    "dev_ids_list=[x['id'] for x in dataset['validation']]\n",
    "dev_accept_indexs_list=[dev_ids_list.index(id_) for id_ in id_accept_expl_list]\n",
    "dev_unaccpet_indexs_list=[dev_ids_list.index(id_) for id_ in id_unaccept_expl_list]\n",
    "dev_accept_data=dataset['validation'].select(dev_accept_indexs_list)\n",
    "dev_unaccept_data=dataset['validation'].select(dev_unaccpet_indexs_list)\n",
    "\n",
    "\n",
    "\n",
    "new_dev_accept_data=[]\n",
    "new_dev_unaccept_data=[]\n",
    "for oexp, da in zip(our_accept_expl_list,dev_accept_data):\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_dev_accept_data.append(da)\n",
    "for da in dev_unaccept_data:\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_dev_unaccept_data.append(da)\n",
    "        \n",
    "new_dev_accept_data[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76059bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01313018798828125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f6d0ca3b00408ea65cc688bd4006e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ed53cbea1f21072fab892031b31192d1',\n",
       " 'question': 'Where can you likely buy many poems?',\n",
       " 'choices': ['book of poetry',\n",
       "  'literature book',\n",
       "  'book store',\n",
       "  'poetry book',\n",
       "  'bookshelf'],\n",
       " 'answer': 'book store',\n",
       " 'abstractive_explanation': 'book store book',\n",
       " 'extractive_explanation': 'buy many poems',\n",
       " 'our_explanation': 'A bookstore sells a variety of books, including poetry books; chains of bookstores sometimes specialize in categories such as poetry or literature.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rationale unlabeled data construction 991\n",
    "import pdb\n",
    "scr_csqa_unlabeled_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=scr_csqa_unlabeled_train_file)\n",
    "scr_csqa_unlabeled_train_df=pd.read_csv(scr_csqa_unlabeled_train_file)\n",
    "fse_csqa_train_data_dict={}\n",
    "for kk, da in enumerate(fse_csqa_train_dataset['train']):\n",
    "    #pdb.set_trace()\n",
    "    id_=da['Input.id']\n",
    "    if da['Answer.acceptable']:\n",
    "        answer_accept=set(da['Answer.acceptable'].split('|'))\n",
    "    else:\n",
    "        answer_accept=set()\n",
    "    explanation_list=[da['Input.explanation_1'], \n",
    "                      da['Input.explanation_2'],\n",
    "                      da['Input.explanation_3'],\n",
    "                      da['Input.explanation_4'],\n",
    "                      da['Input.explanation_5']]\n",
    "    if id_ not in fse_csqa_train_data_dict.keys():\n",
    "        fse_csqa_train_data_dict[id_]={\"index\":kk,\"id\":id_,\n",
    "                                     \"accept_set_list\":[answer_accept],\n",
    "                                     \"explanation_list\":explanation_list}\n",
    "        #[[kk, id_, answer_accept, explanation_list]]\n",
    "    else:\n",
    "        fse_csqa_train_data_dict[id_][\"accept_set_list\"].append(answer_accept)\n",
    "#discriminate 3/3 id\n",
    "id_accept_expl_list=[]\n",
    "our_accept_expl_list=[]\n",
    "id_unaccept_expl_list=[]\n",
    "for k,v in fse_csqa_train_data_dict.items():\n",
    "    accept_set_list=v['accept_set_list']\n",
    "    assert len(accept_set_list)==3\n",
    "    #pdb.set_trace()\n",
    "    common_accept_expl_sample=set.intersection(accept_set_list[0], accept_set_list[1], accept_set_list[2])\n",
    "    #print(accept_set_list,common_accept_expl_sample)\n",
    "    #pdb.set_trace()\n",
    "    if common_accept_expl_sample:\n",
    "        id_accept_expl_list.append(k)\n",
    "        our_expl=\"\"\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            if len(v['explanation_list'][idx]) > len(our_expl):\n",
    "                our_expl = v['explanation_list'][idx]\n",
    "                our_accept_expl_list.append(our_expl)\n",
    "    else:\n",
    "        id_unaccept_expl_list.append(k)\n",
    "\n",
    "train_ids_list=[x['id'] for x in dataset['train']]\n",
    "train_accept_indexs_list=[train_ids_list.index(id_) for id_ in id_accept_expl_list]\n",
    "train_unaccpet_indexs_list=[train_ids_list.index(id_) for id_ in id_unaccept_expl_list]\n",
    "train_accept_data=dataset['train'].select(train_accept_indexs_list)\n",
    "train_unaccept_data=dataset['train'].select(train_unaccpet_indexs_list)\n",
    "\n",
    "\n",
    "\n",
    "new_train_accept_data=[]\n",
    "new_train_unaccept_data=[]\n",
    "for oexp, da in zip(our_accept_expl_list,train_accept_data):\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_train_accept_data.append(da)\n",
    "for da in train_unaccept_data:\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_train_unaccept_data.append(da)\n",
    "        \n",
    "new_train_accept_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22bb5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), set())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_train_data=refine_train_data\n",
    "refine_dev_data=new_train_accept_data + new_dev_accept_data\n",
    "refine_test_data=new_train_unaccept_data + new_dev_unaccept_data\n",
    "\n",
    "refine_train_ids_list=[x['id'] for x in refine_train_data]\n",
    "refine_dev_ids_list=[x['id'] for x in refine_dev_data]\n",
    "refine_test_ids_list=[x['id'] for x in refine_test_data]\n",
    "\n",
    "set(refine_train_ids_list).intersection(set(refine_dev_ids_list)),set(refine_train_ids_list).intersection(set(refine_test_ids_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b23973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation', 'our_explanation'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data_splits={}\n",
    "our_data_splits['train']=refine_train_data\n",
    "our_data_splits['dev']=refine_dev_data\n",
    "our_data_splits['test']=refine_test_data\n",
    "refine_train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bedcf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2dict(refine_data):\n",
    "    refine_data_dict={}\n",
    "    for key in refine_data[0].keys():\n",
    "        refine_data_dict[key]=[x[key] for x in refine_data]\n",
    "    return refine_data_dict\n",
    "\n",
    "refine_train_data_dict = list2dict(refine_train_data)\n",
    "our_data_splits['train'] = datasets.Dataset.from_dict(refine_train_data_dict)\n",
    "\n",
    "refine_dev_data_dict = list2dict(refine_dev_data)\n",
    "our_data_splits['dev'] = datasets.Dataset.from_dict(refine_dev_data_dict)\n",
    "\n",
    "refine_test_data_dict = list2dict(refine_test_data)\n",
    "our_data_splits['test'] = datasets.Dataset.from_dict(refine_test_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ab0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "class SequenceCollator:\n",
    "    def __init__(self, pad_token):\n",
    "        # self.pad_token_mapping = {\n",
    "        #     \"lm_labels\": -100,\n",
    "        #     \"attention_mask\": 0,\n",
    "        #     \"decoder_attention_mask\": 0,\n",
    "        #     \"input_ids\": pad_token,\n",
    "        # }\n",
    "        # self.columns = [\n",
    "        #     \"input_ids\",\n",
    "        #     \"attention_mask\",\n",
    "        #     \"lm_labels\",\n",
    "        #     \"decoder_attention_mask\",\n",
    "        # ]\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def collate_batch(self, examples):\n",
    "\n",
    "        # batch inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "# dataset = datasets.load_dataset(data_args.task_name, data_args.version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219866fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013131141662597656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 115,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedf596434344e0986e668de38ac65eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012603521347045898,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 986,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ce7a8de0049d6a30663b3638c6267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/986 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013090133666992188,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 255,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff949b1a52b4389b2aa4755d1455434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seq_collector = SequenceCollator(0)\n",
    "# train_ds = seq_collector.__call__(dataset['train'])\n",
    "# train_ds\n",
    "# dataset['train'][0].keys()\n",
    "for split in ['train','dev','test']:\n",
    "    our_data_splits[split] = our_data_splits[split].map(\n",
    "            lambda x: format_instance(\n",
    "                x,\n",
    "                tokenizer,\n",
    "                data_args.explanation_sep,\n",
    "                datasource=data_args.task_name,\n",
    "                io_format=data_args.io_format\n",
    "            ),\n",
    "            batched=False,\n",
    "            load_from_cache_file=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_dict="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de79fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=True,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=EvaluationStrategy.EPOCH,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./cos_e_output/111922_232206,\n",
       "logging_first_step=False,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=eval_loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "output_dir=./cos_e_output/111922_232206,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=1,\n",
       "per_device_train_batch_size=1,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=cos_e_output,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=None,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard', 'wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./cos_e_output,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.per_device_eval_batch_size=1\n",
    "training_args.per_device_train_batch_size=1\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80480ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "# original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "# data_args.io_format=\"t5_fewshot_infilling_with_choices\"\n",
    "# # Data loading from huggingface's datasets\n",
    "# if data_args.task_name in {\"cos_e\", \"esnli\"}:\n",
    "#     version_arg = None\n",
    "#     if data_args.task_name == \"cos_e\":\n",
    "#         assert data_args.version_name in {\"v1.11\", \"v1.0\"}\n",
    "#         version_arg = data_args.version_name\n",
    "\n",
    "#     load_train = True\n",
    "#     if (not training_args.do_train\n",
    "#         and not training_args.do_eval\n",
    "#         and not data_args.train_predict\n",
    "#     ):\n",
    "#         # don't load training dataset\n",
    "#         dataset = {}\n",
    "#         dataset[\"train\"] = None\n",
    "#         dataset[\"validation\"] = datasets.load_dataset(\n",
    "#             data_args.task_name, version_arg, split=\"validation\"\n",
    "#         )\n",
    "#         data_splits['validation'] = dataset[\"validation\"]\n",
    "\n",
    "#         if data_args.task_name == \"esnli\":\n",
    "#             dataset[\"test\"] = datasets.load_dataset(data_args.task_name, split=\"test\")\n",
    "#             data_splits['test'] = dataset[\"test\"]\n",
    "#         load_train = False\n",
    "#     else:\n",
    "#         dataset = datasets.load_dataset(data_args.task_name, version_arg)\n",
    "\n",
    "#         if data_args.n_shots > 0: # Shots = number of training examples **per label** \n",
    "#             if data_args.task_name == 'esnli': # Construct a *balanced* random sample of the size `data_args.n_shots*len(labels)` (for train) or `data_args.fewshot_eval_size` (for eval)\n",
    "#                 for split in [\"train\", \"validation\", \"test\"]:\n",
    "#                     split_data = dataset[split]\n",
    "#                     label_subsets = []\n",
    "#                     labels = split_data.features['label'].names\n",
    "#                     sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size/len(labels))\n",
    "#                     if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "#                         assert len(labels) == 3\n",
    "#                         sample_size = data_args.gpt3_max_eval_size // len(labels)\n",
    "#                     for label in labels:\n",
    "#                         # The following is a hack to only run on `neutral` labels of `esnli` to get data for human eval\n",
    "#                         # if data_args.gpt3_max_eval_size is not None and split != 'train' and label != 'neutral':\n",
    "#                         #     continue\n",
    "#                         label_int = split_data.features['label'].str2int(label)\n",
    "#                         label_set = split_data.filter(lambda example: example['label'] == label_int).shuffle() # all instances of labeled as `label`\n",
    "#                         label_subset = label_set.select(range(sample_size)) #select `sample_size` random instances labeled as `label`\n",
    "#                         label_subsets.append(label_subset)\n",
    "#                     dataset[split] = datasets.concatenate_datasets(label_subsets) #merge all label-specific instances\n",
    "#             elif data_args.task_name == 'cos_e': \n",
    "#                 for split in [\"train\", \"validation\"]: \n",
    "#                     split_data = dataset[split]\n",
    "#                     sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size) #Shots for QA are not label-specific, i.e., `n_shots` is the training data size\n",
    "#                     if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "#                         sample_size = data_args.gpt3_max_eval_size\n",
    "#                     dataset[split] = split_data#.shuffle().select(range(sample_size)) # select `sample_size` random instances\n",
    "#             else: \n",
    "#                 raise ValueError('Only cos_e and esnli are supported by Huggingface datasets.')\n",
    "#     # Apply method, and format dataset to torch.Tensor outputs\n",
    "# #     fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "# #     fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "# #     fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "# #     fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "# #     train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "# #     dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "# #     fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "# #     fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n",
    "# #     fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "# #     fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "# #     print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "# #     # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "# #     fse_data_splits={}\n",
    "# #     data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "# #     data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)\n",
    "#     for split in dataset.keys():\n",
    "#         if dataset[split] is not None:\n",
    "#             dataset[split] = dataset[split].map(\n",
    "#                 lambda x: format_instance(\n",
    "#                     x,\n",
    "#                     tokenizer,\n",
    "#                     data_args.explanation_sep,\n",
    "#                     datasource=data_args.task_name,\n",
    "#                     io_format=data_args.io_format\n",
    "#                 ),\n",
    "#                 batched=False,\n",
    "#                 load_from_cache_file=False,\n",
    "#             )\n",
    "#     data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "#     data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "#     if data_args.task_name == \"esnli\":\n",
    "#         data_splits[\"test\"] = deepcopy(dataset[\"test\"])\n",
    "\n",
    "#     original_data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "#     original_data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "#     if data_args.task_name == \"esnli\":\n",
    "#         original_data_splits[\"test\"] = deepcopy(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f4cb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # new_data_splits={'train': None, 'validation': None}\n",
    "# # new_data_splits['train']=deepcopy(dataset[\"train\"])\n",
    "# # new_data_splits['validation']=deepcopy(dataset[\"validation\"])\n",
    "# fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "# fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "# # fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "# # fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "\n",
    "# train_df=pd.read_csv(fse_csqa_train_file)\n",
    "\n",
    "# dev_df=pd.read_csv(fse_csqa_dev_file)\n",
    "\n",
    "# dev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42739e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(fse_train_ids_list),len(list(set(fse_train_ids_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c78ea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "# dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "# fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "# fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80395518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "# fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "# print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "# # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "# fse_data_splits={}\n",
    "# fse_data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "# fse_data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d186954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fse_csqa_train_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66dd71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f144515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: choices, question_encoding, question, extractive_explanation, id, our_explanation, answer, abstractive_explanation.\n",
      "***** Running training *****\n",
      "  Num examples = 115\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 345\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdengdenghuang\u001b[0m (\u001b[33mcuhk_lavilab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/scripts/wandb/run-20221119_233351-363pc30a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cuhk_lavilab/huggingface/runs/363pc30a\" target=\"_blank\">./cos_e_output</a></strong> to <a href=\"https://wandb.ai/cuhk_lavilab/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 39.59 GiB total capacity; 37.31 GiB already allocated; 61.19 MiB free; 37.48 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2384992/1636010974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpt3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg_sq\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg_sq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 39.59 GiB total capacity; 37.31 GiB already allocated; 61.19 MiB free; 37.48 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
    "if data_args.generations_filepath is None:\n",
    "    callbacks = [TensorBoardCallback()]\n",
    "    if data_args.early_stopping_patience > 0:\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=data_args.early_stopping_patience))\n",
    "        training_args.load_best_model_at_end = True\n",
    "    else:\n",
    "        training_args.load_best_model_at_end = False  # use the last model state\n",
    "    training_args.metric_for_best_model = 'eval_loss'\n",
    "    training_args.greater_is_better = False\n",
    "    if training_args.eval_steps is None:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "    else:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=our_data_splits['train'],\n",
    "        eval_dataset=our_data_splits['dev'],\n",
    "        data_collator=SequenceCollator(\n",
    "            model=model_class, pad_token=tokenizer.pad_token_id\n",
    "        ),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "# Training. Don't train if it is use_gpt3\n",
    "if training_args.do_train and not model_args.use_gpt3:\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    model = trainer.model\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    train_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - "
     ]
    }
   ],
   "source": [
    "!conda install -y mpi4py \n",
    "!pip -qq install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb25e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d118847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo ${WANDB_PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68a7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
