{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7deb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.modeling_utils import (\n",
    "    ModuleUtilsMixin, PushToHubMixin,\n",
    "    logging, Union, Optional, Callable, unwrap_model, get_parameter_dtype,\n",
    "    FLAX_WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME, WEIGHTS_NAME,\n",
    "    is_offline_mode, is_remote_url, hf_bucket_url, cached_path\n",
    ")\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class PushToHubFriendlyModel(nn.Module, ModuleUtilsMixin, PushToHubMixin):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def save_pretrained(\n",
    "            self,\n",
    "            save_directory: Union[str, os.PathLike],\n",
    "            save_config: bool = True,\n",
    "            state_dict: Optional[dict] = None,\n",
    "            save_function: Callable = torch.save,\n",
    "            push_to_hub: bool = False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
    "        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "\n",
    "        Arguments:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory to which to save. Will be created if it doesn't exist.\n",
    "            save_config (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to save the config of the model. Useful when in distributed training like TPUs and need\n",
    "                to call this function on all processes. In this case, set :obj:`save_config=True` only on the main\n",
    "                process to avoid race conditions.\n",
    "            state_dict (nested dictionary of :obj:`torch.Tensor`):\n",
    "                The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to\n",
    "                only save parts of the model or if special precautions need to be taken when recovering the state\n",
    "                dictionary of a model (like when using model parallelism).\n",
    "            save_function (:obj:`Callable`):\n",
    "                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
    "                need to replace :obj:`torch.save` by another method.\n",
    "            push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to push your model to the Hugging Face model hub after saving it.\n",
    "\n",
    "                .. warning::\n",
    "\n",
    "                    Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
    "                    :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
    "                    pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
    "                    instead.\n",
    "\n",
    "            kwargs:\n",
    "                Additional key word arguments passed along to the\n",
    "                :meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
    "            return\n",
    "\n",
    "        if push_to_hub:\n",
    "            commit_message = kwargs.pop(\"commit_message\", None)\n",
    "            repo = self._create_or_get_repo(save_directory, **kwargs)\n",
    "\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Only save the model itself if we are using distributed training\n",
    "        model_to_save = unwrap_model(self)\n",
    "\n",
    "        # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n",
    "        # we currently don't use this setting automatically, but may start to use with v5\n",
    "        dtype = get_parameter_dtype(model_to_save)\n",
    "        self.pretrain_model.config.torch_dtype = str(dtype).split(\".\")[1]\n",
    "\n",
    "        # Attach architecture to the config\n",
    "        self.pretrain_model.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "        # Save the config\n",
    "        if save_config:\n",
    "            self.pretrain_model.config.save_pretrained(save_directory)\n",
    "\n",
    "        # Save the model\n",
    "        if state_dict is None:\n",
    "            state_dict = model_to_save.state_dict()\n",
    "\n",
    "        # Handle the case where some state_dict keys shouldn't be saved\n",
    "        # if self._keys_to_ignore_on_save is not None:\n",
    "        #     state_dict = {k: v for k, v in state_dict.items() if k not in self._keys_to_ignore_on_save}\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "        save_function(state_dict, output_model_file)\n",
    "\n",
    "        logger.info(f\"Model weights saved in {output_model_file}\")\n",
    "\n",
    "        if push_to_hub:\n",
    "            url = self._push_to_hub(repo, commit_message=commit_message)\n",
    "            logger.info(f\"Model pushed to the hub in this commit: {url}\")\n",
    "\n",
    "    def load(self, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adopted and simplified from transformers.modeling_utils from_pretrained,\n",
    "        but more similiar to load_state_dict(load the weight from anywhere into a create model).\n",
    "\n",
    "        Just for downloading from huggingface platform.\n",
    "\n",
    "        @param pretrained_model_name_or_path:\n",
    "        @param model_args:\n",
    "        @param kwargs:\n",
    "        \"\"\"\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        state_dict = kwargs.pop(\"state_dict\", None)\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        from_tf = kwargs.pop(\"from_tf\", False)\n",
    "        from_flax = kwargs.pop(\"from_flax\", False)\n",
    "        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        mirror = kwargs.pop(\"mirror\", None)\n",
    "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
    "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
    "        _fast_init = kwargs.pop(\"_fast_init\", True)\n",
    "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
    "\n",
    "        from_pt = not (from_tf | from_flax)\n",
    "\n",
    "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
    "        if from_pipeline is not None:\n",
    "            user_agent[\"using_pipeline\"] = from_pipeline\n",
    "\n",
    "        if is_offline_mode() and not local_files_only:\n",
    "            logger.info(\"Offline mode: forcing local_files_only=True\")\n",
    "            local_files_only = True\n",
    "\n",
    "        # Load model\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
    "                    # Load from a TF 1.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
    "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
    "                    # Load from a TF 2.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
    "                elif from_flax and os.path.isfile(os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)):\n",
    "                    # Load from a Flax checkpoint in priority if from_flax\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)\n",
    "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
    "                    # Load from a PyTorch checkpoint\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
    "                else:\n",
    "                    raise EnvironmentError(\n",
    "                        f\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\n",
    "                        f\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\n",
    "                    )\n",
    "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                archive_file = pretrained_model_name_or_path\n",
    "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
    "                if not from_tf:\n",
    "                    raise ValueError(\n",
    "                        f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n",
    "                        \"from_tf to True to load from this checkpoint.\"\n",
    "                    )\n",
    "                archive_file = pretrained_model_name_or_path + \".index\"\n",
    "            else:\n",
    "                # set correct filename\n",
    "                if from_tf:\n",
    "                    filename = TF2_WEIGHTS_NAME\n",
    "                elif from_flax:\n",
    "                    filename = FLAX_WEIGHTS_NAME\n",
    "                else:\n",
    "                    filename = WEIGHTS_NAME\n",
    "\n",
    "                archive_file = hf_bucket_url(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    filename=filename,\n",
    "                    revision=revision,\n",
    "                    mirror=mirror,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                # Load from URL or cache if already cached\n",
    "                resolved_archive_file = cached_path(\n",
    "                    archive_file,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    local_files_only=local_files_only,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                    user_agent=user_agent,\n",
    "                )\n",
    "            except EnvironmentError as err:\n",
    "                logger.error(err)\n",
    "                msg = (\n",
    "                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n",
    "                )\n",
    "                raise EnvironmentError(msg)\n",
    "\n",
    "            if resolved_archive_file == archive_file:\n",
    "                logger.info(f\"loading weights file {archive_file}\")\n",
    "            else:\n",
    "                logger.info(f\"loading weights file {archive_file} from cache at {resolved_archive_file}\")\n",
    "        else:\n",
    "            resolved_archive_file = None\n",
    "\n",
    "        # load pt weights early so that we know which dtype to init the model under\n",
    "        if from_pt:\n",
    "            if state_dict is None:\n",
    "                try:\n",
    "                    state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "                except Exception:\n",
    "                    raise OSError(\n",
    "                        f\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\n",
    "                        f\"at '{resolved_archive_file}'\"\n",
    "                        \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
    "                    )\n",
    "        self.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9804003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "# from PushToHubFriendlyModel\n",
    "from modeling_auto import AutoModelForSeq2SeqLM\n",
    "from modeling_bart import BartForConditionalGeneration\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class Model(PushToHubFriendlyModel):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        \"\"\"The prefix-tuning code\"\"\"\n",
    "\n",
    "        self.preseqlen = args.prefix_tuning.prefix_sequence_length\n",
    "        self.mid_dim = args.prefix_tuning.mid_dim\n",
    "\n",
    "        print(\"prefix-tuning sequence length is {}.\".format(self.preseqlen))\n",
    "\n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.bert.location, use_fast=False)\n",
    "        self.pretrain_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.bert.location\n",
    "        )\n",
    "        self.config = self.pretrain_model.config\n",
    "\n",
    "        if isinstance(self.pretrain_model, BartForConditionalGeneration):\n",
    "            self.match_n_layer = self.config.decoder_layers\n",
    "            self.match_n_head = self.config.decoder_attention_heads\n",
    "        elif isinstance(self.pretrain_model, (T5ForConditionalGeneration)):\n",
    "            self.match_n_layer = self.config.num_decoder_layers\n",
    "            self.match_n_head = self.config.num_heads\n",
    "        else:\n",
    "            raise ValueError(\"Other models are not supported yet!\")\n",
    "\n",
    "        self.n_embd = self.config.d_model\n",
    "        assert self.n_embd % self.match_n_head == 0\n",
    "        self.match_n_embd = self.n_embd // self.match_n_head\n",
    "\n",
    "        if args.special_tokens:\n",
    "            self.tokenizer.add_tokens([v for k, v in args.special_tokens])\n",
    "            self.pretrain_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # Prefix related.\n",
    "        self.register_buffer('input_tokens', torch.arange(self.preseqlen).long())\n",
    "\n",
    "        self.wte = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_enc = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_enc = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_enc = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_dec = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_dec = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "\n",
    "        # Knowledge prompt.\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_dec = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(args.prefix_tuning.prefix_dropout)\n",
    "\n",
    "        if self.args.model.freeze_plm:\n",
    "            for param in self.pretrain_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.args.model.freeze_prefix:\n",
    "            for param in self.wte.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.wte_dec.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans_dec.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.wte_enc.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans_enc.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def get_prompt(self, bsz=None, sample_size=1, description=None, knowledge=None):\n",
    "        old_bsz = bsz\n",
    "        bsz = bsz * sample_size\n",
    "        input_tokens = self.input_tokens.unsqueeze(0).expand(bsz, -1)\n",
    "        temp_control = self.wte(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control = temp_control + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values = self.control_trans(temp_control)  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values = torch.cat([past_key_values, self.knowledge_trans(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Cross prefix\n",
    "        temp_control_dec = self.wte_dec(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control_dec = temp_control_dec + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values_dec = self.control_trans_dec(\n",
    "            temp_control_dec\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_dec = torch.cat([past_key_values_dec, self.knowledge_trans_dec(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values_dec.shape\n",
    "        past_key_values_dec = past_key_values_dec.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values_dec = self.dropout(past_key_values_dec)\n",
    "        past_key_values_dec = past_key_values_dec.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Encoder prefix\n",
    "        input_tokens_enc = (\n",
    "            self.input_tokens.unsqueeze(0).expand(old_bsz, -1)\n",
    "        )\n",
    "        temp_control_enc = self.wte_enc(input_tokens_enc)\n",
    "        if description is not None:\n",
    "            temp_control_enc = temp_control_enc + description.unsqueeze(1)\n",
    "        past_key_values_enc = self.control_trans_enc(\n",
    "            temp_control_enc\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_enc = torch.cat([past_key_values_enc, self.knowledge_trans_enc(knowledge)], dim=1)\n",
    "\n",
    "        bsz_enc, seqlen, _ = past_key_values_enc.shape\n",
    "        past_key_values_enc = past_key_values_enc.view(\n",
    "            bsz_enc,\n",
    "            seqlen,\n",
    "            self.match_n_layer * 2,\n",
    "            self.match_n_head,\n",
    "            self.match_n_embd,\n",
    "        )\n",
    "        past_key_values_enc = self.dropout(past_key_values_enc)\n",
    "        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        result = []\n",
    "        for i, key_val in enumerate(past_key_values):\n",
    "            temp = dict()\n",
    "            temp[\"decoder_prompt\"] = {\n",
    "                \"prev_key\": key_val[0].contiguous(),\n",
    "                \"prev_value\": key_val[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val.device)\n",
    "                    .bool()\n",
    "                # bsz, preseqlen\n",
    "            }\n",
    "            key_val_dec = past_key_values_dec[i]\n",
    "            temp[\"cross_attention_prompt\"] = {\n",
    "                \"prev_key\": key_val_dec[0].contiguous(),\n",
    "                \"prev_value\": key_val_dec[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val_dec.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            key_val_enc = past_key_values_enc[i]\n",
    "            temp[\"encoder_prompt\"] = {\n",
    "                \"prev_key\": key_val_enc[0].contiguous(),\n",
    "                \"prev_value\": key_val_enc[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz_enc, seqlen)\n",
    "                    .to(key_val_enc.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            result.append(temp)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_description_representation(self, kwargs):\n",
    "        if self.args.model.use_description and self.args.model.map_description:\n",
    "            description_input_ids = kwargs.pop(\"description_input_ids\")\n",
    "            description_attention_mask = kwargs.pop(\"description_attention_mask\")\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                description_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                description_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        else:\n",
    "            description = None\n",
    "\n",
    "        return description\n",
    "\n",
    "    def get_knowledge_representation(self, kwargs):\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            knowledge_input_ids = kwargs.pop(\"knowledge_input_ids\", None)\n",
    "            knowledge_attention_mask = kwargs.pop(\"knowledge_attention_mask\", None)\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                knowledge_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                knowledge_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        elif self.args.model.knowledge_usage == 'concatenate':\n",
    "            knowledge = None\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return knowledge\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                labels,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "\n",
    "        loss = self.pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_prompt=past_prompt,\n",
    "        ).loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids,\n",
    "                 attention_mask,\n",
    "                 **kwargs):\n",
    "\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, sample_size=kwargs['num_beams'], description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "        generated_ids = self.pretrain_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_prompt=past_prompt,\n",
    "            use_cache=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a6395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1e558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "DEFAULT_CONFIGURE_DIR = \"configure\"\n",
    "DEFAULT_DATASET_DIR = \"data\"\n",
    "DEFAULT_MODEL_DIR = \"models\"\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self, contain=None):\n",
    "        self.__self__ = contain\n",
    "        self.__default__ = None\n",
    "        self.__default__ = set(dir(self))\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.__self__\n",
    "\n",
    "    def __getattribute__(self, name):\n",
    "        if name[:2] == \"__\" and name[-2:] == \"__\":\n",
    "            return super().__getattribute__(name)\n",
    "        if name not in dir(self):\n",
    "            return None\n",
    "        return super().__getattribute__(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if not (value is None) or (name[:2] == \"__\" and name[-2:] == \"__\"):\n",
    "            return super().__setattr__(name, value)\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name in dir(self) and name not in self.__default__:\n",
    "            super().__delattr__(name)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # give args elements dictionary order to ensure its replicate-ability\n",
    "        return sorted(list((arg, getattr(self, arg)) for arg in set(dir(self)) - self.__default__)).__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(set(dir(self)) - self.__default__)\n",
    "\n",
    "\n",
    "class String(object):\n",
    "    @staticmethod\n",
    "    def to_basic(string):\n",
    "        \"\"\"\n",
    "        Convert the String to what it really means.\n",
    "        For example, \"true\" --> True as a bool value\n",
    "        :param string:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(string)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(string)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if string in [\"True\", \"true\"]:\n",
    "            return True\n",
    "        elif string in [\"False\", \"false\"]:\n",
    "            return False\n",
    "        else:\n",
    "            return string.strip(\"\\\"'\")  # for those we want to add space before and after the string\n",
    "\n",
    "\n",
    "class Configure(object):\n",
    "    @staticmethod\n",
    "    def get_file_cfg(file):\n",
    "        \"\"\"\n",
    "        get configurations in file.\n",
    "        :param file:\n",
    "        :return: configure args\n",
    "        \"\"\"\n",
    "        cfgargs = Args()\n",
    "        parser = configparser.ConfigParser()\n",
    "        parser.read(file)\n",
    "        for section in parser.sections():\n",
    "            setattr(cfgargs, section, Args())\n",
    "            for item in parser.items(section):\n",
    "                setattr(getattr(cfgargs, section), item[0], String.to_basic(item[1]))\n",
    "        return cfgargs\n",
    "\n",
    "    @staticmethod\n",
    "    def refresh_args_by_file_cfg(file, prev_args):\n",
    "        args = Configure.get_file_cfg(file)\n",
    "        if args.dir is not Args:\n",
    "            args.dir = Args()\n",
    "        args.dir.model = DEFAULT_MODEL_DIR\n",
    "        args.dir.dataset = DEFAULT_DATASET_DIR\n",
    "        args.dir.configure = DEFAULT_CONFIGURE_DIR\n",
    "        for arg_name, arg in prev_args:\n",
    "            if arg is None:\n",
    "                continue\n",
    "            if arg_name != \"cfg\":\n",
    "                names = arg_name.split(\".\")\n",
    "                cur = args\n",
    "                for name in names[: -1]:\n",
    "                    if getattr(cur, name) is None:\n",
    "                        setattr(cur, name, Args())\n",
    "                    cur = getattr(cur, name)\n",
    "                if getattr(cur, names[-1]) is None:\n",
    "                    setattr(cur, names[-1], arg)\n",
    "        return args\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def Get(cfg):\n",
    "        args = Configure.get_file_cfg(os.path.join(DEFAULT_CONFIGURE_DIR, cfg))\n",
    "\n",
    "        if args.dir is not Args:\n",
    "            args.dir = Args()\n",
    "        args.dir.model = DEFAULT_MODEL_DIR\n",
    "        args.dir.dataset = DEFAULT_DATASET_DIR\n",
    "        args.dir.configure = DEFAULT_CONFIGURE_DIR\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c51b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "skt_args=Configure.get_file_cfg(\"./cos_e_prefix.cfg\")\n",
    "t5_model = Model(skt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f9699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835e9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "# from transformers import T5ForConditionalGeneration\n",
    "from transformers.file_utils import ModelOutput\n",
    "import torch\n",
    "from modeling_transformer import MyTransformer, SemanticMatch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "class GenMC(nn.Module):\n",
    "    def __init__(self, model_path, num_hidden_layers, alpha, beta):\n",
    "        super(GenMC, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        #self.prefix_model=prefix_model\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        dim = self.t5_model.config.d_model\n",
    "        self.option_linear = nn.Linear(dim, 1).to(device)\n",
    "        self.option_linear.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.semantic_matching = SemanticMatch(dim, num_hidden_layers).to(device)\n",
    "        self.semantic_matching.device = device\n",
    "        num_attention_heads = dim // 64\n",
    "        self.transformer_laryer_de = MyTransformer(dim, num_attention_heads, num_hidden_layers).to(device)\n",
    "        self.transformer_laryer_de.device = device\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        layer_num = self.t5_model.config.num_layers\n",
    "        layer_per_gpu = layer_num // n_gpu\n",
    "        device_map = {}\n",
    "        for n in range(n_gpu):\n",
    "            device_map[n] = [i + n * layer_per_gpu for i in range(layer_per_gpu)]\n",
    "        remain_layer = [i + n_gpu * layer_per_gpu for i in range(layer_num - layer_per_gpu * n_gpu)]\n",
    "        device_map[n_gpu - 1] += remain_layer\n",
    "        #self.t5_model.parallelize(device_map)\n",
    "\n",
    "\n",
    "    def forward(self, q_ids, q_mask, qo_ids, qo_mask, choice_num, clue_ids=None, answers=None, rationale_ids=None):\n",
    "        self.choice_num = choice_num\n",
    "        if answers is not None and clue_ids is not None:\n",
    "            opt_score, output_sequences = self.get_option_score(q_ids, q_mask, qo_ids, qo_mask)\n",
    "            local_device = self.t5_model.device\n",
    "            t5_output = self.t5_model(input_ids=q_ids.to(local_device), attention_mask=q_mask.to(local_device),\n",
    "                                      labels=clue_ids.to(local_device))\n",
    "            loss_ans = t5_output.loss\n",
    "            loss = self.criterion(opt_score, answers)\n",
    "            \n",
    "#             loss_rationale = self.prefix_model(input_ids=q_ids.to(local_device), attention_mask=q_mask.to(local_device),\n",
    "#                                       labels=rationale_ids.to(local_device))\n",
    "            return self.alpha * loss + self.beta * loss_ans\n",
    "        else:\n",
    "            opt_score, output_sequences = self.get_option_score(q_ids, q_mask, qo_ids, qo_mask)\n",
    "            return opt_score, output_sequences\n",
    "\n",
    "    def get_option_score(self, q_ids, q_mask, qo_ids, qo_mask):\n",
    "        local_device = self.t5_model.encoder.device\n",
    "        t5_output = self.t5_model.encoder(input_ids=qo_ids.to(local_device), attention_mask=qo_mask.to(local_device))\n",
    "        encoder_qo = t5_output[0]\n",
    "\n",
    "        t5_output = self.t5_model.encoder(input_ids=q_ids.to(local_device), attention_mask=q_mask.to(local_device))\n",
    "        encoder_q = t5_output[0]\n",
    "        local_device = self.t5_model.device\n",
    "        t5_output = self.t5_model.generate(\n",
    "            encoder_outputs=ModelOutput(last_hidden_state=encoder_q.to(local_device)),\n",
    "            attention_mask=q_mask.to(local_device),\n",
    "            do_sample=False,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        output_sequences = t5_output.sequences\n",
    "        output_sequences = output_sequences[:, 1:].contiguous()\n",
    "        decoder_o = t5_output.decoder_hidden_states\n",
    "        decoder_o = [item[-1] for item in decoder_o]\n",
    "        decoder_o = torch.cat(decoder_o, dim=1)\n",
    "\n",
    "        output_sequences_mask1 = output_sequences != 0\n",
    "        output_sequences_mask2 = output_sequences != 1\n",
    "        output_sequences_mask = output_sequences_mask1 * output_sequences_mask2\n",
    "        output_sequences_mask = output_sequences_mask.long()\n",
    "        decoder_qo = torch.cat([encoder_q, decoder_o], dim=1)\n",
    "        output_sequences_mask = torch.cat([q_mask, output_sequences_mask], dim=1)\n",
    "        local_device = self.transformer_laryer_de.device\n",
    "        decoder_qo, _ = self.transformer_laryer_de(decoder_qo.to(local_device), output_sequences_mask.to(local_device))\n",
    "        output_sequences_mask_ex = output_sequences_mask.unsqueeze(dim=1)\n",
    "        output_sequences_mask_ex = output_sequences_mask_ex.expand(\n",
    "            [output_sequences_mask_ex.size(0), self.choice_num, output_sequences_mask_ex.size(-1)]).contiguous()\n",
    "        output_sequences_mask_ex = output_sequences_mask_ex.view(-1, output_sequences_mask.size(-1))\n",
    "        decoder_qo = decoder_qo.unsqueeze(dim=1)\n",
    "        decoder_qo = decoder_qo.expand(\n",
    "            [decoder_qo.size(0), self.choice_num, decoder_qo.size(-2), decoder_qo.size(-1)]).contiguous()\n",
    "        decoder_qo = decoder_qo.view(-1, decoder_qo.size(-2), decoder_qo.size(-1))\n",
    "        local_device = self.semantic_matching.device\n",
    "        semantic_vec, _, _ = self.semantic_matching(encoder_qo.to(local_device), decoder_qo.to(local_device),\n",
    "                                                    qo_mask.to(local_device), output_sequences_mask_ex.to(local_device))\n",
    "        local_device = self.option_linear.device\n",
    "        opt_score = self.option_linear(semantic_vec.to(local_device)).view(-1, self.choice_num)\n",
    "\n",
    "        return opt_score, output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4a0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# if __name__ == '__main__':\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_path\",\n",
    "                    default='t5-base',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--choice_num\",\n",
    "                    default=5,\n",
    "                    type=int)\n",
    "parser.add_argument(\"--data_path_train\",\n",
    "                    default='./data/csqa/in_hourse/train.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--data_path_dev\",\n",
    "                    default='./data/csqa/in_hourse/dev.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--data_path_test\",\n",
    "                    default='./data/csqa/in_hourse/test.jsonl',\n",
    "                    required=True,\n",
    "                    type=str)\n",
    "parser.add_argument(\"--results_save_path\",\n",
    "                    default='./results/',\n",
    "                    type=str)\n",
    "parser.add_argument(\"--train_batch_size\",\n",
    "                    default=64,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                    default=8,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument('--gradient_accumulation_steps',\n",
    "                    type=int,\n",
    "                    default=4,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "\n",
    "parser.add_argument(\"--output_dir\",\n",
    "                    default='./outputs/',\n",
    "                    type=str,\n",
    "                    help=\"The output dreader2ctory whretriever the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--init_checkpoint\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"Initial checkpoint (usually from a pre-trained BERT model)\")\n",
    "parser.add_argument(\"--max_len\",\n",
    "                    default=64,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_len_gen\",\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total output sequence length for decoder\")\n",
    "parser.add_argument(\"--lr\",\n",
    "                    default=1e-5,\n",
    "                    type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--epoch_num\",\n",
    "                    default=30,\n",
    "                    type=int,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument('--num_hidden_layers',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"The number of hidden layer for co-matching and encoder-decoder interaction transformer\")\n",
    "parser.add_argument('--alpha',\n",
    "                    type=float,\n",
    "                    default=1)\n",
    "parser.add_argument('--beta',\n",
    "                    type=float,\n",
    "                    default=1)\n",
    "parser.add_argument('--seed',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--name_save_prix\",\n",
    "                    default='GenMC_CSQA',\n",
    "                    type=str)\n",
    "parser.add_argument('--external_sent_num',\n",
    "                    type=int,\n",
    "                    default=None,\n",
    "                    help=\"The number of retrieved sentences\")\n",
    "\n",
    "args = parser.parse_args([\"--model_path\", \"t5-base\", \"--choice_num\", \"5\", \n",
    "                          \"--data_path_train\", \"../GenMC/data/csqa/in_hourse/train.jsonl\",  \n",
    "                          \"--data_path_dev\", \"../GenMC/data/csqa/in_hourse/dev.jsonl\",  \n",
    "                          \"--data_path_test\", \"../GenMC/data/csqa/in_hourse/test.jsonl\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0dd6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenMC(t5_model, args.num_hidden_layers, args.alpha, args.beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c332326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from transformers import T5Tokenizer\n",
    "from tqdm import trange\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from utils import compute_rouges, save_dataset, read_dataset, set_seed, save_model\n",
    "# from model.modeling_genmc import GenMC\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "def get_input_feature(samples, max_source_length, max_len_gen, choice_num, external_sent_num=None):\n",
    "    sep = ' \\\\n '\n",
    "    output_clue = []\n",
    "    output_rationale = []\n",
    "    answers = []\n",
    "    input_ids_q, attention_mask_q = [], []\n",
    "    input_ids_qo, attention_mask_qo = [], []\n",
    "    for sample in samples:\n",
    "        if 'answerKey' in sample:\n",
    "            answerKey = sample['answerKey']\n",
    "        else:\n",
    "            answerKey = \"A\"\n",
    "        question = sample['question']['stem']\n",
    "        while len(sample['question']['choices']) < choice_num:\n",
    "            sample['question']['choices'].append({\"text\": \"error\", \"para\": \"\", \"label\":chr(ord('A')+len(sample)-1)})\n",
    "        for o_i, (opt, opt_name) in enumerate(zip(sample['question']['choices'], 'ABCDEFGH'[:choice_num])):\n",
    "            option = opt['text']\n",
    "            content = \"\"\n",
    "            if external_sent_num is not None and 'para' in opt:\n",
    "                para = opt[\"para\"]\n",
    "                if isinstance(para, list):\n",
    "                    if len(para) > external_sent_num:\n",
    "                        para = para[:external_sent_num]\n",
    "                    content = sep + \" \".join(para)\n",
    "                elif isinstance(para, str):\n",
    "                    para = para.split(\".\")\n",
    "                    if len(para) > external_sent_num:\n",
    "                        para = para[:external_sent_num]\n",
    "                    content = sep + \" \".join(para)\n",
    "                else:\n",
    "                    print('lack retrieval')\n",
    "                    # exit(0)\n",
    "            input_ids_qo.append(question + sep + option + content)\n",
    "\n",
    "\n",
    "        input_ids_q.append(question + sep)\n",
    "        if answerKey in '123456':\n",
    "            answer = ord(answerKey) - ord('1')\n",
    "        else:\n",
    "            answer = ord(answerKey) - ord('A')\n",
    "        answers.append(answer)\n",
    "        output_clue.append(sample['question']['choices'][answer]['text'])\n",
    "        output_rationale.append(sample['cos-e'])\n",
    "\n",
    "    def tokenizer_fun(input_ids, max_len):\n",
    "        encoding = tokenizer(input_ids,\n",
    "                             padding='longest',\n",
    "                             max_length=max_len,\n",
    "                             truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        ids = encoding.input_ids.to(device)\n",
    "        mask = encoding.attention_mask.to(device)\n",
    "        return ids, mask\n",
    "\n",
    "    q_ids, q_mask = tokenizer_fun(input_ids_q, max_source_length)\n",
    "    qo_ids, qo_mask = tokenizer_fun(input_ids_qo, max_source_length)\n",
    "    clue_ids, _ = tokenizer_fun(output_clue, max_len_gen)\n",
    "    clue_ids = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in\n",
    "        clue_ids\n",
    "    ]\n",
    "    clue_ids = torch.tensor(clue_ids, dtype=torch.long).to(device)\n",
    "    answers = torch.tensor(answers, dtype=torch.long).to(device)\n",
    "    \n",
    "    rationale_ids, _ = tokenizer_fun(output_rationale, max_len_gen)\n",
    "#     rationale_ids = [\n",
    "#         [(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in\n",
    "#         rationale_ids\n",
    "#     ]\n",
    "#     rationale_ids = torch.tensor(rationale_ids, dtype=torch.long).to(device)\n",
    "    \n",
    "    return q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, rationale_ids, output_clue, output_rationale\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, test_examples, tokenizer, eval_batch_size, choice_num, max_len, max_len_gen, external_sent_num):\n",
    "    count, count_right = 0, 0\n",
    "    results = []\n",
    "    model.eval()\n",
    "    step_count = len(test_examples) // eval_batch_size\n",
    "    if step_count * eval_batch_size < len(test_examples):\n",
    "        step_count += 1\n",
    "    step_trange = trange(step_count)\n",
    "    sources, targets = [], []\n",
    "    for step in step_trange:\n",
    "        beg_index = step * eval_batch_size\n",
    "        end_index = min((step + 1) * eval_batch_size, len(test_examples))\n",
    "        batch_example = [example for example in test_examples[beg_index:end_index]]\n",
    "        q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, output_clue = get_input_feature(batch_example,\n",
    "                                                                                           max_len, max_len_gen,\n",
    "                                                                                           args.choice_num,\n",
    "                                                                                           external_sent_num)\n",
    "        scores, output_sequences = model(q_ids, q_mask, qo_ids, qo_mask, choice_num)\n",
    "\n",
    "        scores = scores.cpu().detach().tolist()\n",
    "        answers = answers.cpu().detach().tolist()\n",
    "        p_anss = []\n",
    "        for p, a, example in zip(scores, answers, batch_example):\n",
    "            p_ans = p.index(max(p))\n",
    "            p_anss.append(example['question']['choices'][p_ans]['label'])\n",
    "            if p_ans == a:\n",
    "                count_right += 1\n",
    "            count += 1\n",
    "        for sample, p_ans in zip(batch_example, p_anss):\n",
    "            qid = sample['id']\n",
    "            results.append(qid + \",\" + p_ans)\n",
    "        predicts = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "        sources += predicts\n",
    "        targets += output_clue\n",
    "\n",
    "    rouge_score = compute_rouges(sources, targets)['rouge-l']\n",
    "\n",
    "    return count_right / count, rouge_score, results\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b99d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'lr_{args.lr}_seed_{args.seed}_bs_{args.train_batch_size}_ga_{args.gradient_accumulation_steps}_layer_num_{args.num_hidden_layers}_alpha_{args.alpha}_beta_{args.beta}'\n",
    "output_model_path = './outputs/' + args.name_save_prix + '/' + file_name + \"/\"\n",
    "path_save_result = './results/' + args.name_save_prix + '/' + file_name + \"/\"\n",
    "\n",
    "os.makedirs(path_save_result, exist_ok=True)\n",
    "set_seed(args.seed)\n",
    "train_examples = read_dataset(args.data_path_train)\n",
    "dev_examples = read_dataset(args.data_path_dev)\n",
    "test_examples = read_dataset(args.data_path_test)\n",
    "\n",
    "train_examples = train_examples + dev_examples\n",
    "dev_examples = test_examples\n",
    "test_examples = test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dff7818e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Person who commits murder will have to undergo legal punishment.',\n",
       "  'No person is mortal in this world.',\n",
       "  'If  he or she is not caught, evenually he or she will have his own death.'],\n",
       " ['Ocean is a natures creation, person would not go to ocean eventually',\n",
       "  'Fear is of getting caught, person will never feel fearful once he is not caught',\n",
       "  'There is no point of imprisonment if person is not caught',\n",
       "  'There is no point of incarceration if person is not caught'],\n",
       " \"Person committing muder will have to undergo legal punishment. Although he is not caught, eventually he will have his own death at some point of time as no one is mortal. There is no question of imprisonment or incarceration if person is not caught for muder. Also person will not be fearful now as he is not caught. Ocean is a nature's creation and person would not got to ocean eventually.\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_examples), train_examples[0].keys(), \\\\\n",
    "train_examples[0]['positives'],train_examples[0]['negatives'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bf8925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Person committing muder will have to undergo legal punishment. Although he is not caught, eventually he will have his own death at some point of time as no one is mortal. There is no question of imprisonment or incarceration if person is not caught for muder. Also person will not be fearful now as he is not caught. Ocean is a nature's creation and person would not got to ocean eventually.\",\n",
       " 'death is something that occurs to everyone and is the only certainty in life.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0]['explanation'], train_examples[0]['cos-e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1bca222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"lr\": 1e-05,\n",
      "  \"model\": \"t5-base\",\n",
      "  \"seed\": 1,\n",
      "  \"bs\": 64,\n",
      "  \"gradient_accumulation_steps\": 4,\n",
      "  \"epoch\": 30,\n",
      "  \"train_path\": \"../GenMC/data/csqa/in_hourse/train.jsonl\",\n",
      "  \"dev_path\": \"../GenMC/data/csqa/in_hourse/dev.jsonl\",\n",
      "  \"test_path\": \"../GenMC/data/csqa/in_hourse/test.jsonl\",\n",
      "  \"train_size\": 9741,\n",
      "  \"dev_size\": 1221,\n",
      "  \"test_size\": 1221,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"external_sent_num\": null,\n",
      "  \"alpha\": 1,\n",
      "  \"beta\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/153 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_597054/1102946084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m best_dev_acc, _, _ = eval(model, dev_examples, tokenizer, args.eval_batch_size, args.choice_num, args.max_len,\n\u001b[0;32m---> 29\u001b[0;31m                           args.max_len_gen, args.external_sent_num)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_dev_acc:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_dev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mbest_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_597054/4069377326.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, test_examples, tokenizer, eval_batch_size, choice_num, max_len, max_len_gen, external_sent_num)\u001b[0m\n\u001b[1;32m     95\u001b[0m                                                                                            \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                                                                                            external_sent_num)\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_597054/227995632.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_ids, q_mask, qo_ids, qo_mask, choice_num, clue_ids, answers)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_ans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mopt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_option_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_597054/227995632.py\u001b[0m in \u001b[0;36mget_option_score\u001b[0;34m(self, q_ids, q_mask, qo_ids, qo_mask)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_option_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlocal_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mt5_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqo_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqo_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mencoder_qo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(json.dumps({\"lr\": args.lr, \"model\": args.model_path, \"seed\": args.seed,\n",
    "                  \"bs\": args.train_batch_size,\n",
    "                  'gradient_accumulation_steps': args.gradient_accumulation_steps,\n",
    "                  \"epoch\": args.epoch_num,\n",
    "                  \"train_path\": args.data_path_train,\n",
    "                  \"dev_path\": args.data_path_dev,\n",
    "                  \"test_path\": args.data_path_test,\n",
    "                  \"train_size\": len(train_examples),\n",
    "                  \"dev_size\": len(dev_examples),\n",
    "                  \"test_size\": len(test_examples),\n",
    "                  'num_hidden_layers': args.num_hidden_layers,\n",
    "                  'external_sent_num': args.external_sent_num,\n",
    "                  \"alpha\": args.alpha, \"beta\": args.beta}, indent=2))\n",
    "\n",
    "train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "tokenizer = T5Tokenizer.from_pretrained(args.model_path)\n",
    "# model = GenMC(args.model_path, args.num_hidden_layers, args.alpha, args.beta)\n",
    "\n",
    "if args.init_checkpoint is not None:\n",
    "    checkpoint = torch.load(args.init_checkpoint, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "\n",
    "step_count, step_all, early_stop = 0, 0, 0\n",
    "best_dev_rouge_score, best_test_rouge_score = 0, 0\n",
    "tr_loss, nb_tr_steps = 0, 0\n",
    "\n",
    "best_dev_acc, _, _ = eval(model, dev_examples, tokenizer, args.eval_batch_size, args.choice_num, args.max_len,\n",
    "                          args.max_len_gen, args.external_sent_num)\n",
    "print('best_dev_acc:',best_dev_acc)\n",
    "best_test_acc = 0\n",
    "for epoch in range(args.epoch_num):\n",
    "    early_stop += 1\n",
    "    order = list(range(len(train_examples)))\n",
    "    random.seed(args.seed + epoch)\n",
    "    random.shuffle(order)\n",
    "    model.train()\n",
    "    step_count = len(train_examples) // train_batch_size\n",
    "    if step_count * train_batch_size < len(train_examples):\n",
    "        step_count += 1\n",
    "    step_trange = trange(step_count)\n",
    "    for step in step_trange:\n",
    "        step_all += 1\n",
    "        beg_index = step * train_batch_size\n",
    "        end_index = min((step + 1) * train_batch_size, len(train_examples))\n",
    "        order_index = order[beg_index:end_index]\n",
    "        batch_example = [train_examples[index] for index in order_index]\n",
    "        q_ids, q_mask, qo_ids, qo_mask, clue_ids, answers, output_clue, rationale_ids, output_rationale = get_input_feature(\n",
    "            batch_example,\n",
    "            max_source_length=args.max_len,\n",
    "            max_len_gen=args.max_len_gen,\n",
    "            choice_num=args.choice_num,\n",
    "            external_sent_num=args.external_sent_num)\n",
    "        loss = model(q_ids, q_mask, qo_ids, qo_mask, args.choice_num, clue_ids, answers,rationale_ids)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_show = ' Epoch:' + str(epoch) + \" loss:\" + str(round(tr_loss / nb_tr_steps, 4))\n",
    "        step_trange.set_postfix_str(loss_show)\n",
    "\n",
    "    dev_acc, dev_rouge_score, results_dev = eval(model, dev_examples, tokenizer, args.eval_batch_size,\n",
    "                                                 args.choice_num, args.max_len, args.max_len_gen,\n",
    "                                                 args.external_sent_num)\n",
    "    print('dev_acc:', dev_acc)\n",
    "    if dev_acc > best_dev_acc:\n",
    "        save_dataset(path_save_result + '/dev.csv', results_dev)\n",
    "        early_stop = 0\n",
    "        test_acc, test_rouge_score, results_test = eval(model, test_examples, tokenizer, args.eval_batch_size,\n",
    "                                                        args.choice_num, args.max_len, args.max_len_gen,\n",
    "                                                        args.external_sent_num)\n",
    "        save_dataset(path_save_result + '/test.csv', results_test)\n",
    "        best_dev_acc, best_test_acc, best_dev_rouge_score, best_test_rouge_score = dev_acc, test_acc, dev_rouge_score, test_rouge_score\n",
    "\n",
    "        # save_model(output_model_path, model, optimizer)\n",
    "        print('new best dev acc:', dev_acc, 'test_acc:', test_acc, 'rouge:', dev_rouge_score)\n",
    "\n",
    "    if early_stop >= 5:\n",
    "        break\n",
    "\n",
    "print('best dev acc:', best_dev_acc, 'best_test_acc:', best_test_acc,\n",
    "      'best_dev_rouge_score:', best_dev_rouge_score, 'best_test_rouge_score:', best_test_rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48ee1b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32102, 768)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.pretrain_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0553e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
