{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac35f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt3\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Any, NewType\n",
    "\n",
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "from feature_conversion_methods import format_instance\n",
    "\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments\n",
    ")\n",
    "from metrics import evaluate\n",
    "import torch\n",
    "import datasets\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import random \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "from copy import deepcopy \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers.logging.set_verbosity_info()\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "set_global_logging_level(logging.ERROR, [\"datasets\"])\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "\n",
    "\n",
    "def set_other_seeds(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# inspired by DefaultDataCollator from:\n",
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "# modified to perform batch-level padding.\n",
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5d6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "08/25/2022 15:14:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/25/2022 15:14:56 - INFO - __main__ -   Save path: ./cos_e_output/082522_151456\n",
      "08/25/2022 15:14:56 - INFO - __main__ -   Git branch: dev\n",
      "08/25/2022 15:14:56 - INFO - __main__ -   Git hash: 640aa0d57986f7e8295dabb0e8ab189542f32d53\n"
     ]
    }
   ],
   "source": [
    "og_start_time = time.time()\n",
    "\n",
    "#parser = HfArgumentParser(\n",
    "#    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#)\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, unused_args = parser.parse_args_into_dataclasses(\n",
    "    [\"--model_type\", \"t5-base\",\n",
    "     \"--tokenizer_name\", \"t5-base\",\n",
    "     \"--task_name\", \"cos_e\", \n",
    "     \"--output_dir\", \"./cos_e_output\", \n",
    "     \"--do_train\", \"True\"], return_remaining_strings=True)\n",
    "if unused_args != []:\n",
    "    raise ValueError(f\"Received unused arguments: {unused_args}\")\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "\n",
    "if model_args.use_gpt3:\n",
    "    assert training_args.do_train\n",
    "    assert not training_args.do_eval\n",
    "    assert data_args.generations_filepath is None\n",
    "    if data_args.gpt3_max_eval_size is not None:\n",
    "        assert data_args.gpt3_max_eval_size <= data_args.fewshot_eval_size\n",
    "        assert data_args.gpt3_max_eval_size % 2 == 0\n",
    "        assert data_args.gpt3_max_eval_size % 3 == 0\n",
    "\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "if not training_args.do_train and data_args.generations_filepath is None:\n",
    "    if not model_args.pretrained_model_file:\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation\"\n",
    "        )\n",
    "\n",
    "if training_args.do_train:\n",
    "    # create a save directory and a logfile\n",
    "    training_args.output_dir = os.path.join(\n",
    "        training_args.output_dir, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "            os.path.exists(training_args.output_dir)\n",
    "            and os.listdir(training_args.output_dir)\n",
    "            and training_args.do_train\n",
    "            and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "model_class = \"t5\"\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "            os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "set_other_seeds(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128f048",
   "metadata": {},
   "source": [
    "# tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823757d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2022 15:19:09 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "model_class = \"t5\"\n",
    "tokenizer_name = TOKENIZER_MAPPING[model_class]\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "model_args.tokenizer_name='t5-base'\n",
    "tokenizer = tokenizer_name.from_pretrained(model_args.tokenizer_name)#, cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdca61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.modeling_utils import (\n",
    "    ModuleUtilsMixin, PushToHubMixin,\n",
    "    logging, Union, Optional, Callable, unwrap_model, get_parameter_dtype,\n",
    "    FLAX_WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME, WEIGHTS_NAME,\n",
    "    is_offline_mode, is_remote_url, hf_bucket_url, cached_path\n",
    ")\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class PushToHubFriendlyModel(nn.Module, ModuleUtilsMixin, PushToHubMixin):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def save_pretrained(\n",
    "            self,\n",
    "            save_directory: Union[str, os.PathLike],\n",
    "            save_config: bool = True,\n",
    "            state_dict: Optional[dict] = None,\n",
    "            save_function: Callable = torch.save,\n",
    "            push_to_hub: bool = False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
    "        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "\n",
    "        Arguments:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory to which to save. Will be created if it doesn't exist.\n",
    "            save_config (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to save the config of the model. Useful when in distributed training like TPUs and need\n",
    "                to call this function on all processes. In this case, set :obj:`save_config=True` only on the main\n",
    "                process to avoid race conditions.\n",
    "            state_dict (nested dictionary of :obj:`torch.Tensor`):\n",
    "                The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to\n",
    "                only save parts of the model or if special precautions need to be taken when recovering the state\n",
    "                dictionary of a model (like when using model parallelism).\n",
    "            save_function (:obj:`Callable`):\n",
    "                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
    "                need to replace :obj:`torch.save` by another method.\n",
    "            push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to push your model to the Hugging Face model hub after saving it.\n",
    "\n",
    "                .. warning::\n",
    "\n",
    "                    Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
    "                    :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
    "                    pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
    "                    instead.\n",
    "\n",
    "            kwargs:\n",
    "                Additional key word arguments passed along to the\n",
    "                :meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
    "            return\n",
    "\n",
    "        if push_to_hub:\n",
    "            commit_message = kwargs.pop(\"commit_message\", None)\n",
    "            repo = self._create_or_get_repo(save_directory, **kwargs)\n",
    "\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Only save the model itself if we are using distributed training\n",
    "        model_to_save = unwrap_model(self)\n",
    "\n",
    "        # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n",
    "        # we currently don't use this setting automatically, but may start to use with v5\n",
    "        dtype = get_parameter_dtype(model_to_save)\n",
    "        self.pretrain_model.config.torch_dtype = str(dtype).split(\".\")[1]\n",
    "\n",
    "        # Attach architecture to the config\n",
    "        self.pretrain_model.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "        # Save the config\n",
    "        if save_config:\n",
    "            self.pretrain_model.config.save_pretrained(save_directory)\n",
    "\n",
    "        # Save the model\n",
    "        if state_dict is None:\n",
    "            state_dict = model_to_save.state_dict()\n",
    "\n",
    "        # Handle the case where some state_dict keys shouldn't be saved\n",
    "        # if self._keys_to_ignore_on_save is not None:\n",
    "        #     state_dict = {k: v for k, v in state_dict.items() if k not in self._keys_to_ignore_on_save}\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "        save_function(state_dict, output_model_file)\n",
    "\n",
    "        logger.info(f\"Model weights saved in {output_model_file}\")\n",
    "\n",
    "        if push_to_hub:\n",
    "            url = self._push_to_hub(repo, commit_message=commit_message)\n",
    "            logger.info(f\"Model pushed to the hub in this commit: {url}\")\n",
    "\n",
    "    def load(self, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adopted and simplified from transformers.modeling_utils from_pretrained,\n",
    "        but more similiar to load_state_dict(load the weight from anywhere into a create model).\n",
    "\n",
    "        Just for downloading from huggingface platform.\n",
    "\n",
    "        @param pretrained_model_name_or_path:\n",
    "        @param model_args:\n",
    "        @param kwargs:\n",
    "        \"\"\"\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        state_dict = kwargs.pop(\"state_dict\", None)\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        from_tf = kwargs.pop(\"from_tf\", False)\n",
    "        from_flax = kwargs.pop(\"from_flax\", False)\n",
    "        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        mirror = kwargs.pop(\"mirror\", None)\n",
    "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
    "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
    "        _fast_init = kwargs.pop(\"_fast_init\", True)\n",
    "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
    "\n",
    "        from_pt = not (from_tf | from_flax)\n",
    "\n",
    "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
    "        if from_pipeline is not None:\n",
    "            user_agent[\"using_pipeline\"] = from_pipeline\n",
    "\n",
    "        if is_offline_mode() and not local_files_only:\n",
    "            logger.info(\"Offline mode: forcing local_files_only=True\")\n",
    "            local_files_only = True\n",
    "\n",
    "        # Load model\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
    "                    # Load from a TF 1.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
    "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
    "                    # Load from a TF 2.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
    "                elif from_flax and os.path.isfile(os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)):\n",
    "                    # Load from a Flax checkpoint in priority if from_flax\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)\n",
    "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
    "                    # Load from a PyTorch checkpoint\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
    "                else:\n",
    "                    raise EnvironmentError(\n",
    "                        f\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\n",
    "                        f\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\n",
    "                    )\n",
    "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                archive_file = pretrained_model_name_or_path\n",
    "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
    "                if not from_tf:\n",
    "                    raise ValueError(\n",
    "                        f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n",
    "                        \"from_tf to True to load from this checkpoint.\"\n",
    "                    )\n",
    "                archive_file = pretrained_model_name_or_path + \".index\"\n",
    "            else:\n",
    "                # set correct filename\n",
    "                if from_tf:\n",
    "                    filename = TF2_WEIGHTS_NAME\n",
    "                elif from_flax:\n",
    "                    filename = FLAX_WEIGHTS_NAME\n",
    "                else:\n",
    "                    filename = WEIGHTS_NAME\n",
    "\n",
    "                archive_file = hf_bucket_url(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    filename=filename,\n",
    "                    revision=revision,\n",
    "                    mirror=mirror,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                # Load from URL or cache if already cached\n",
    "                resolved_archive_file = cached_path(\n",
    "                    archive_file,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    local_files_only=local_files_only,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                    user_agent=user_agent,\n",
    "                )\n",
    "            except EnvironmentError as err:\n",
    "                logger.error(err)\n",
    "                msg = (\n",
    "                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n",
    "                )\n",
    "                raise EnvironmentError(msg)\n",
    "\n",
    "            if resolved_archive_file == archive_file:\n",
    "                logger.info(f\"loading weights file {archive_file}\")\n",
    "            else:\n",
    "                logger.info(f\"loading weights file {archive_file} from cache at {resolved_archive_file}\")\n",
    "        else:\n",
    "            resolved_archive_file = None\n",
    "\n",
    "        # load pt weights early so that we know which dtype to init the model under\n",
    "        if from_pt:\n",
    "            if state_dict is None:\n",
    "                try:\n",
    "                    state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "                except Exception:\n",
    "                    raise OSError(\n",
    "                        f\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\n",
    "                        f\"at '{resolved_archive_file}'\"\n",
    "                        \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
    "                    )\n",
    "        self.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data_args.generations_filepath is None:\n",
    "#     model_name = MODEL_MAPPING[model_class]\n",
    "#     if model_args.pretrained_model_file:\n",
    "#         model = T5ForConditionalGeneration.from_pretrained(model_args.pretrained_model_file)\n",
    "\n",
    "#         if model_args.dropout_rate:\n",
    "#             raise Exception(\"can't update/specify dropout currently when load pretrained model from directory\")\n",
    "\n",
    "#     elif model_args.pretrained:\n",
    "#         # load pretrained model from HuggingFace\n",
    "#         logger.info(\"Loading pretrained model\")\n",
    "#         if model_args.dropout_rate:\n",
    "#             model = model_name.from_pretrained(model_args.model_type, dropout_rate=model_args.dropout_rate)\n",
    "#         else:\n",
    "#             model = model_name.from_pretrained(model_args.model_type)\n",
    "#     else:\n",
    "#         # load model from scratch with no pretrained weights\n",
    "#         config_name = CONFIG_MAPPING[model_class]()\n",
    "#         # TODO (Sarah): NOTE THIS ONLY DOES T5-BASE; PASS IN ARGS HERE^\n",
    "#         logger.info(\n",
    "#             \"Training new model from scratch using default config (NOTE: SMALL MODELS ONLY FOR NOW)\"\n",
    "#         )\n",
    "#         if model_args.dropout_rate:\n",
    "#             raise Exception(\"sure you want to train a model from scratch?\")\n",
    "#         model = model_name.from_config(config_name)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "# else:\n",
    "#     model = None\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "from .base import PushToHubFriendlyModel\n",
    "from modeling_auto import AutoModelForSeq2SeqLM\n",
    "from modeling_bart import BartForConditionalGeneration\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class Model(PushToHubFriendlyModel):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        \"\"\"The prefix-tuning code\"\"\"\n",
    "\n",
    "        self.preseqlen = args.prefix_tuning.prefix_sequence_length\n",
    "        self.mid_dim = args.prefix_tuning.mid_dim\n",
    "\n",
    "        print(\"prefix-tuning sequence length is {}.\".format(self.preseqlen))\n",
    "\n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.bert.location, use_fast=False)\n",
    "        self.pretrain_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.bert.location\n",
    "        )\n",
    "        self.config = self.pretrain_model.config\n",
    "\n",
    "        if isinstance(self.pretrain_model, BartForConditionalGeneration):\n",
    "            self.match_n_layer = self.config.decoder_layers\n",
    "            self.match_n_head = self.config.decoder_attention_heads\n",
    "        elif isinstance(self.pretrain_model, (T5ForConditionalGeneration)):\n",
    "            self.match_n_layer = self.config.num_decoder_layers\n",
    "            self.match_n_head = self.config.num_heads\n",
    "        else:\n",
    "            raise ValueError(\"Other models are not supported yet!\")\n",
    "\n",
    "        self.n_embd = self.config.d_model\n",
    "        assert self.n_embd % self.match_n_head == 0\n",
    "        self.match_n_embd = self.n_embd // self.match_n_head\n",
    "\n",
    "        if args.special_tokens:\n",
    "            self.tokenizer.add_tokens([v for k, v in args.special_tokens])\n",
    "            self.pretrain_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # Prefix related.\n",
    "        self.register_buffer('input_tokens', torch.arange(self.preseqlen).long())\n",
    "\n",
    "        self.wte = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_enc = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_enc = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_enc = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_dec = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_dec = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "\n",
    "        # Knowledge prompt.\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_dec = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(args.prefix_tuning.prefix_dropout)\n",
    "\n",
    "        if self.args.model.freeze_plm:\n",
    "            for param in self.pretrain_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.args.model.freeze_prefix:\n",
    "            for param in self.wte.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.wte_dec.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans_dec.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.wte_enc.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.control_trans_enc.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def get_prompt(self, bsz=None, sample_size=1, description=None, knowledge=None):\n",
    "        old_bsz = bsz\n",
    "        bsz = bsz * sample_size\n",
    "        input_tokens = self.input_tokens.unsqueeze(0).expand(bsz, -1)\n",
    "        temp_control = self.wte(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control = temp_control + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values = self.control_trans(temp_control)  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values = torch.cat([past_key_values, self.knowledge_trans(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Cross prefix\n",
    "        temp_control_dec = self.wte_dec(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control_dec = temp_control_dec + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values_dec = self.control_trans_dec(\n",
    "            temp_control_dec\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_dec = torch.cat([past_key_values_dec, self.knowledge_trans_dec(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values_dec.shape\n",
    "        past_key_values_dec = past_key_values_dec.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values_dec = self.dropout(past_key_values_dec)\n",
    "        past_key_values_dec = past_key_values_dec.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Encoder prefix\n",
    "        input_tokens_enc = (\n",
    "            self.input_tokens.unsqueeze(0).expand(old_bsz, -1)\n",
    "        )\n",
    "        temp_control_enc = self.wte_enc(input_tokens_enc)\n",
    "        if description is not None:\n",
    "            temp_control_enc = temp_control_enc + description.unsqueeze(1)\n",
    "        past_key_values_enc = self.control_trans_enc(\n",
    "            temp_control_enc\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_enc = torch.cat([past_key_values_enc, self.knowledge_trans_enc(knowledge)], dim=1)\n",
    "\n",
    "        bsz_enc, seqlen, _ = past_key_values_enc.shape\n",
    "        past_key_values_enc = past_key_values_enc.view(\n",
    "            bsz_enc,\n",
    "            seqlen,\n",
    "            self.match_n_layer * 2,\n",
    "            self.match_n_head,\n",
    "            self.match_n_embd,\n",
    "        )\n",
    "        past_key_values_enc = self.dropout(past_key_values_enc)\n",
    "        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        result = []\n",
    "        for i, key_val in enumerate(past_key_values):\n",
    "            temp = dict()\n",
    "            temp[\"decoder_prompt\"] = {\n",
    "                \"prev_key\": key_val[0].contiguous(),\n",
    "                \"prev_value\": key_val[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val.device)\n",
    "                    .bool()\n",
    "                # bsz, preseqlen\n",
    "            }\n",
    "            key_val_dec = past_key_values_dec[i]\n",
    "            temp[\"cross_attention_prompt\"] = {\n",
    "                \"prev_key\": key_val_dec[0].contiguous(),\n",
    "                \"prev_value\": key_val_dec[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val_dec.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            key_val_enc = past_key_values_enc[i]\n",
    "            temp[\"encoder_prompt\"] = {\n",
    "                \"prev_key\": key_val_enc[0].contiguous(),\n",
    "                \"prev_value\": key_val_enc[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz_enc, seqlen)\n",
    "                    .to(key_val_enc.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            result.append(temp)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_description_representation(self, kwargs):\n",
    "        if self.args.model.use_description and self.args.model.map_description:\n",
    "            description_input_ids = kwargs.pop(\"description_input_ids\")\n",
    "            description_attention_mask = kwargs.pop(\"description_attention_mask\")\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                description_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                description_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        else:\n",
    "            description = None\n",
    "\n",
    "        return description\n",
    "\n",
    "    def get_knowledge_representation(self, kwargs):\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            knowledge_input_ids = kwargs.pop(\"knowledge_input_ids\", None)\n",
    "            knowledge_attention_mask = kwargs.pop(\"knowledge_attention_mask\", None)\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                knowledge_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                knowledge_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        elif self.args.model.knowledge_usage == 'concatenate':\n",
    "            knowledge = None\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return knowledge\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                labels,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "\n",
    "        loss = self.pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_prompt=past_prompt,\n",
    "        ).loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids,\n",
    "                 attention_mask,\n",
    "                 **kwargs):\n",
    "\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, sample_size=kwargs['num_beams'], description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "        generated_ids = self.pretrain_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_prompt=past_prompt,\n",
    "            use_cache=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return generated_ids\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
