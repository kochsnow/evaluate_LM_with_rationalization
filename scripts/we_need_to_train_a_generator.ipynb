{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8639d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt3\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Any, NewType\n",
    "\n",
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "#from feature_conversion_methods import format_instance\n",
    "\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments\n",
    ")\n",
    "from metrics import evaluate\n",
    "import torch\n",
    "import datasets\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import random \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "from copy import deepcopy \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers.logging.set_verbosity_info()\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "set_global_logging_level(logging.ERROR, [\"datasets\"])\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "\n",
    "\n",
    "def set_other_seeds(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# inspired by DefaultDataCollator from:\n",
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "# modified to perform batch-level padding.\n",
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95527a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\"\"\"\n",
    "Example-to-Feature conversion methods\n",
    "Modified from\n",
    "https://github.com/salesforce/cos-e/blob/master/code/generation/train_commonsenseqa_v1.0.py and \"\"_v1.11.py (identical)\n",
    "as well as Tensorflow code for WTF?: \n",
    "https://github.com/google-research/google-research/blob/master/wt5/wt5/preprocessors.py\n",
    "\"\"\"\n",
    "# This code is based on https://github.com/allenai/label_rationale_association/blob/main/feature_conversion_methods.py\n",
    "\n",
    "unified_qa_esnli_label_mapping = {0: 'yes', 1: 'maybe', 2: 'no'}\n",
    "unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'} \n",
    "wt5_esnli_label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'} \n",
    "unified_qa_sbic_label_mapping = {\"offensive\": 'Yes', \"not offensive\": 'No'}\n",
    "\n",
    "def format_instance(\n",
    "        example,\n",
    "        tokenizer,\n",
    "        explanation_sep,\n",
    "        max_seq_length=None,\n",
    "        datasource=None,\n",
    "        io_format=None, \n",
    "):\n",
    "    assert datasource in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "    if datasource in [\"cos_e\", \"ecqa\"]:\n",
    "        input_string, answer_string = cqa_formatting(example, io_format, explanation_sep, datasource)\n",
    "    elif datasource == \"esnli\":\n",
    "        input_string, answer_string = esnli_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sbic':\n",
    "        input_string, answer_string = sbic_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sensemaking':\n",
    "        input_string, answer_string = sensemaking_formatting(example, io_format, explanation_sep)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task. Currently supported: esnli, cos_e, sbic, sensemaking, ecqa.\")\n",
    "    \n",
    "    if 'unified' in io_format and 'unifew' not in io_format:\n",
    "        input_string += '</s>'\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        input_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # note even with \"lm_labels.shift_right()\", the decoder attention mask length is still correct since we remove the last token\n",
    "    dec = tokenizer.encode_plus(\n",
    "        answer_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    encodings[\"labels\"] = dec[\"input_ids\"]\n",
    "    encodings[\"decoder_attention_mask\"] = dec[\"attention_mask\"]\n",
    "    encodings[\"question_encoding\"] = encodings[\"input_ids\"]\n",
    "\n",
    "    #return encodings\n",
    "    return {**example, **encodings}\n",
    "\n",
    "#这里很简单 定义好输入输出string就可以的\n",
    "def cqa_formatting(item, io_format, explanation_sep, datasource):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    abstr_expl = item[\"abstractive_explanation\"].lower() if datasource == 'cos_e' else item[\"explanation\"].lower()\n",
    "\n",
    "\n",
    "    if io_format == 't5_fewshot_infilling_with_choices':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" The answer is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain {datasource} question: {question} context: \" + ', '.join(item['choices']) # explain cos_e question: When getting in shape you need to have this in between workouts? context: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == \"record\": \n",
    "        # might not work because cos_e doesn't have a passage \n",
    "        input_string = f\"explain {datasource} query: {question} entities: \" + ', '.join(item['choices']) # explain cos_e query: When getting in shape you need to have this in between workouts? entities: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == 'unifiedqa_matching':\n",
    "        choice_ids = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        input_string = f'explain {question.lower()} \\\\n'\n",
    "        for choice_id, choice in zip(choice_ids, item[\"choices\"]):\n",
    "            input_string += f' {choice_id} {choice.lower()}'\n",
    "        answer_string = f\"{answer.lower()} {explanation_sep} {abstr_expl.lower()}\"\n",
    "        answer_string = answer_string.lower()\n",
    "    elif io_format == 't5_fewshot_infilling_without_choices_use_refined_expl':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "    \n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def esnli_formatting(item, io_format, explanation_sep):\n",
    "\n",
    "    premise = item[\"premise\"]\n",
    "    hypothesis = item[\"hypothesis\"]\n",
    "    answer = unified_qa_esnli_label_mapping[item[\"label\"]] if 'unified' in io_format else wt5_esnli_label_mapping[item[\"label\"]]\n",
    "    abstr_expl = item[\"explanation_1\"].lower() \n",
    "    # Dev/test instances have more than one explanation annotated; merge them into one sequence separated by [SEP] \n",
    "    for k in [2,3]:\n",
    "        if f\"explanation_{k}\" in item and item[f'explanation_{k}']!='': \n",
    "            abstr_expl += f\" [SEP] {item[f'explanation_{k}'].lower()}\"\n",
    "\n",
    "    if io_format == 'standard':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: {hypothesis} {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\" \n",
    "    elif io_format == \"squad_endswith_what\":\n",
    "        input_string = f\"explain nli question: What is this? context: {hypothesis} {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix_endswith_what\":  \n",
    "        input_string = f\"explain nli question: What is this? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"   \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        hypothesis = hypothesis.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        answer = unified_qa_esnli_label_mapping_upper[item[\"label\"]]\n",
    "        input_string = f'explain {premise} Is {hypothesis}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_unifew_nli_mix':\n",
    "        premise = premise.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        input_string = f'explain hypothesis: {hypothesis} Is premise: {premise}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}' \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm_with_choices': \n",
    "        input_string = f'explain is this entailment? \\\\n (A) yes (B) maybe (C) no \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_with_choices_v2': \n",
    "        input_string = f'explain what is this? \\\\n (A) entailment (B) neutral (C) contradiction \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"     \n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported.\")\n",
    "\n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def sbic_formatting(item, io_format, explanation_sep):\n",
    "    # We pre-processed the SBIC dataset such that we join multiple implied statements with the [SEP] token for dev/test instances \n",
    "    # Each annotation in the training split is a separate instance\n",
    "    post = item['post']\n",
    "    answer = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] if 'unified' in io_format else item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "    abstr_expl = item[\"targetStereotype\"]\n",
    "    \n",
    "\n",
    "    if io_format == 'cola':\n",
    "        input_string = f\"explain sbic sentence: {post}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain sbic sentence: {post} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':\n",
    "        answer_bool = 'True' if item[\"offensiveYN\"] == 'offensive' else 'False'\n",
    "        input_string = f\"explain sbic sentence: {post} It is <extra_id_0> that this is offensive {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain sbic sentence: {post} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 'squad_yn': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_yn_with_tags': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: post: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_what': \n",
    "        input_string = f\"explain sbic question: What is this? context: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'squad_what_with_tags': \n",
    "        input_string = f\"explain sbic question: What is this? context: post: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'unified_qa_yn':\n",
    "        input_string = f'explain is this offensive? \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\" \n",
    "    elif io_format == 'unified_qa_yn_with_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_yn_with_choices':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_yn_with_choices_and_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what':\n",
    "        input_string = f'explain what is this? \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_tags':\n",
    "        input_string = f'explain what is this? \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_choices':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_what_with_choices_and_tags':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        input_string = f\"Topic? \\\\n (A) offensive (B) not_offensive \\\\n {post}\"\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string\n",
    "\n",
    "def sensemaking_formatting(item, io_format, explanation_sep):\n",
    "    # TODO: explore whether removing periods makes difference? \n",
    "    sent0 = item['sent0']\n",
    "    sent1 = item['sent1']\n",
    "    nonsensical_sentence = str(int(item['label'])+1)\n",
    "    explanation = item['explanation'].lower()\n",
    "\n",
    "    if io_format == 'copa_with_question':\n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} question: nonsensical\"\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 'copa_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} Less common is choice2\"\n",
    "        answer_string = f\"{answer_bool} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 't5_fewshot_infilling':  \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> choice{nonsensical_sentence} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} It is <extra_id_0> that choice2 is less common {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer_bool} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == \"squad_yn\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_yn_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"record\": \n",
    "        input_string = f\"explain sensemaking query: What is more nonsensical? entities: choice1, choice2 passage: choice1: {sent0} choice2: {sent1}\" # explain sensemaking query: What is nonsensical? entities: choice1, choice2 passage: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation.\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" # choice1 because state flowers are unique to each state.\n",
    "    elif io_format == 'unifiedqa_yn_with_choices':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n (A) yes (B) no \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\" \n",
    "    elif io_format == 'unifiedqa_yn':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_yn_no_tags':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_with_choices':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n (A) choice1 (B) choice2 \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what_no_tags':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254a20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "11/19/2022 22:29:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/19/2022 22:29:08 - INFO - __main__ -   Save path: ./cos_e_output/111922_222908\n",
      "11/19/2022 22:29:08 - INFO - __main__ -   Git branch: dev\n",
      "11/19/2022 22:29:08 - INFO - __main__ -   Git hash: 1cbb5c3b4e53baf31cbafc20d9655c63f091f901\n"
     ]
    }
   ],
   "source": [
    "og_start_time = time.time()\n",
    "\n",
    "#parser = HfArgumentParser(\n",
    "#    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#)\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, unused_args = parser.parse_args_into_dataclasses(\n",
    "    [\"--model_type\", \"t5-base\",\n",
    "     \"--tokenizer_name\", \"t5-base\",\n",
    "     \"--task_name\", \"cos_e\", \n",
    "     \"--output_dir\", \"./cos_e_output\", \n",
    "     \"--n_shots\", \"10\",\n",
    "     \"--do_train\", \"True\"], return_remaining_strings=True)\n",
    "if unused_args != []:\n",
    "    raise ValueError(f\"Received unused arguments: {unused_args}\")\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "\n",
    "if model_args.use_gpt3:\n",
    "    assert training_args.do_train\n",
    "    assert not training_args.do_eval\n",
    "    assert data_args.generations_filepath is None\n",
    "    if data_args.gpt3_max_eval_size is not None:\n",
    "        assert data_args.gpt3_max_eval_size <= data_args.fewshot_eval_size\n",
    "        assert data_args.gpt3_max_eval_size % 2 == 0\n",
    "        assert data_args.gpt3_max_eval_size % 3 == 0\n",
    "\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "if not training_args.do_train and data_args.generations_filepath is None:\n",
    "    if not model_args.pretrained_model_file:\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation\"\n",
    "        )\n",
    "\n",
    "if training_args.do_train:\n",
    "    # create a save directory and a logfile\n",
    "    training_args.output_dir = os.path.join(\n",
    "        training_args.output_dir, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "            os.path.exists(training_args.output_dir)\n",
    "            and os.listdir(training_args.output_dir)\n",
    "            and training_args.do_train\n",
    "            and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "model_class = \"t5\"\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "            os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "set_other_seeds(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a975bbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/19/2022 22:29:08 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/huangyongfeng/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "model_class = \"t5\"\n",
    "tokenizer_name = TOKENIZER_MAPPING[model_class]\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "model_args.tokenizer_name='t5-base'\n",
    "tokenizer = tokenizer_name.from_pretrained(model_args.tokenizer_name)#, cache_dir=model_args.cache_dir)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e2cbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(task_name='cos_e', early_stopping_patience=10, overwrite_cache=False, train_predict=False, test_predict=False, dev_predict=False, version_name='v1.11', generations_filepath=None, n_shots=10, fewshot_eval_size=350, io_format='standard', explanation_sep='explanation', data_path=None, gpt3_max_eval_size=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aafac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013048410415649414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d02a54764ab4323964541cc175a821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "class SequenceCollator:\n",
    "    def __init__(self, pad_token):\n",
    "        # self.pad_token_mapping = {\n",
    "        #     \"lm_labels\": -100,\n",
    "        #     \"attention_mask\": 0,\n",
    "        #     \"decoder_attention_mask\": 0,\n",
    "        #     \"input_ids\": pad_token,\n",
    "        # }\n",
    "        # self.columns = [\n",
    "        #     \"input_ids\",\n",
    "        #     \"attention_mask\",\n",
    "        #     \"lm_labels\",\n",
    "        #     \"decoder_attention_mask\",\n",
    "        # ]\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def collate_batch(self, examples):\n",
    "\n",
    "        # batch inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "dataset = datasets.load_dataset(data_args.task_name, data_args.version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d90c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer', 'extractive_explanation', 'id', 'abstractive_explanation', 'question', 'choices'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_collector = SequenceCollator(0)\n",
    "train_ds = seq_collector.__call__(dataset['train'])\n",
    "train_ds\n",
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596e12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c2d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013004779815673828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62953da616944ebc9fbe4cf86bfbba30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014930248260498047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9741,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0505fab1a807426ca47a21bb0eb7a412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9741 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01304316520690918,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1221,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac97130b6f824f90a8627a06b4873a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "data_args.io_format=\"t5_fewshot_infilling_with_choices\"\n",
    "# Data loading from huggingface's datasets\n",
    "if data_args.task_name in {\"cos_e\", \"esnli\"}:\n",
    "    version_arg = None\n",
    "    if data_args.task_name == \"cos_e\":\n",
    "        assert data_args.version_name in {\"v1.11\", \"v1.0\"}\n",
    "        version_arg = data_args.version_name\n",
    "\n",
    "    load_train = True\n",
    "    if (not training_args.do_train\n",
    "        and not training_args.do_eval\n",
    "        and not data_args.train_predict\n",
    "    ):\n",
    "        # don't load training dataset\n",
    "        dataset = {}\n",
    "        dataset[\"train\"] = None\n",
    "        dataset[\"validation\"] = datasets.load_dataset(\n",
    "            data_args.task_name, version_arg, split=\"validation\"\n",
    "        )\n",
    "        data_splits['validation'] = dataset[\"validation\"]\n",
    "\n",
    "        if data_args.task_name == \"esnli\":\n",
    "            dataset[\"test\"] = datasets.load_dataset(data_args.task_name, split=\"test\")\n",
    "            data_splits['test'] = dataset[\"test\"]\n",
    "        load_train = False\n",
    "    else:\n",
    "        dataset = datasets.load_dataset(data_args.task_name, version_arg)\n",
    "\n",
    "        if data_args.n_shots > 0: # Shots = number of training examples **per label** \n",
    "            if data_args.task_name == 'esnli': # Construct a *balanced* random sample of the size `data_args.n_shots*len(labels)` (for train) or `data_args.fewshot_eval_size` (for eval)\n",
    "                for split in [\"train\", \"validation\", \"test\"]:\n",
    "                    split_data = dataset[split]\n",
    "                    label_subsets = []\n",
    "                    labels = split_data.features['label'].names\n",
    "                    sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size/len(labels))\n",
    "                    if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "                        assert len(labels) == 3\n",
    "                        sample_size = data_args.gpt3_max_eval_size // len(labels)\n",
    "                    for label in labels:\n",
    "                        # The following is a hack to only run on `neutral` labels of `esnli` to get data for human eval\n",
    "                        # if data_args.gpt3_max_eval_size is not None and split != 'train' and label != 'neutral':\n",
    "                        #     continue\n",
    "                        label_int = split_data.features['label'].str2int(label)\n",
    "                        label_set = split_data.filter(lambda example: example['label'] == label_int).shuffle() # all instances of labeled as `label`\n",
    "                        label_subset = label_set.select(range(sample_size)) #select `sample_size` random instances labeled as `label`\n",
    "                        label_subsets.append(label_subset)\n",
    "                    dataset[split] = datasets.concatenate_datasets(label_subsets) #merge all label-specific instances\n",
    "            elif data_args.task_name == 'cos_e': \n",
    "                for split in [\"train\", \"validation\"]: \n",
    "                    split_data = dataset[split]\n",
    "                    sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size) #Shots for QA are not label-specific, i.e., `n_shots` is the training data size\n",
    "                    if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "                        sample_size = data_args.gpt3_max_eval_size\n",
    "                    dataset[split] = split_data#.shuffle().select(range(sample_size)) # select `sample_size` random instances\n",
    "            else: \n",
    "                raise ValueError('Only cos_e and esnli are supported by Huggingface datasets.')\n",
    "    # Apply method, and format dataset to torch.Tensor outputs\n",
    "#     fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "#     fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "#     fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "#     fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "#     train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "#     dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "#     fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "#     fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n",
    "#     fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "#     fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "#     print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "#     # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "#     fse_data_splits={}\n",
    "#     data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "#     data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)\n",
    "    for split in dataset.keys():\n",
    "        if dataset[split] is not None:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                lambda x: format_instance(\n",
    "                    x,\n",
    "                    tokenizer,\n",
    "                    data_args.explanation_sep,\n",
    "                    datasource=data_args.task_name,\n",
    "                    io_format=data_args.io_format\n",
    "                ),\n",
    "                batched=False,\n",
    "                load_from_cache_file=False,\n",
    "            )\n",
    "    data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "    data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "    if data_args.task_name == \"esnli\":\n",
    "        data_splits[\"test\"] = deepcopy(dataset[\"test\"])\n",
    "\n",
    "    original_data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "    original_data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "    if data_args.task_name == \"esnli\":\n",
    "        original_data_splits[\"test\"] = deepcopy(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6aac422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.id</th>\n",
       "      <th>Input.question</th>\n",
       "      <th>Input.answer_choices</th>\n",
       "      <th>Input.gold_label</th>\n",
       "      <th>Input.source_sample_1</th>\n",
       "      <th>Input.source_sample_2</th>\n",
       "      <th>Input.source_sample_3</th>\n",
       "      <th>Input.source_sample_4</th>\n",
       "      <th>Input.source_sample_5</th>\n",
       "      <th>Input.explanation_1</th>\n",
       "      <th>Input.explanation_2</th>\n",
       "      <th>Input.explanation_3</th>\n",
       "      <th>Input.explanation_4</th>\n",
       "      <th>Input.explanation_5</th>\n",
       "      <th>Input.nll_1</th>\n",
       "      <th>Input.nll_2</th>\n",
       "      <th>Input.nll_3</th>\n",
       "      <th>Input.nll_4</th>\n",
       "      <th>Input.nll_5</th>\n",
       "      <th>Answer.acceptable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5b8a3081c3235d62bc77e2d15f3ad454</td>\n",
       "      <td>A town between two mountains is located in a w...</td>\n",
       "      <td>michigan, valley, state, hospital, or train st...</td>\n",
       "      <td>valley</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>greedy</td>\n",
       "      <td>Because all of the mountains are to the left o...</td>\n",
       "      <td>A valley is usually between two mountains, whe...</td>\n",
       "      <td>Because rivers end in valleys; assuming the ri...</td>\n",
       "      <td>A town in between mountains presumably would b...</td>\n",
       "      <td>A valley is a low area between two mountains.</td>\n",
       "      <td>1.615639</td>\n",
       "      <td>1.460747</td>\n",
       "      <td>2.280955</td>\n",
       "      <td>1.885454</td>\n",
       "      <td>0.609790</td>\n",
       "      <td>2|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b8a3081c3235d62bc77e2d15f3ad454</td>\n",
       "      <td>A town between two mountains is located in a w...</td>\n",
       "      <td>michigan, valley, state, hospital, or train st...</td>\n",
       "      <td>valley</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>greedy</td>\n",
       "      <td>Because all of the mountains are to the left o...</td>\n",
       "      <td>A valley is usually between two mountains, whe...</td>\n",
       "      <td>Because rivers end in valleys; assuming the ri...</td>\n",
       "      <td>A town in between mountains presumably would b...</td>\n",
       "      <td>A valley is a low area between two mountains.</td>\n",
       "      <td>1.615639</td>\n",
       "      <td>1.460747</td>\n",
       "      <td>2.280955</td>\n",
       "      <td>1.885454</td>\n",
       "      <td>0.609790</td>\n",
       "      <td>2|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5b8a3081c3235d62bc77e2d15f3ad454</td>\n",
       "      <td>A town between two mountains is located in a w...</td>\n",
       "      <td>michigan, valley, state, hospital, or train st...</td>\n",
       "      <td>valley</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>greedy</td>\n",
       "      <td>Because all of the mountains are to the left o...</td>\n",
       "      <td>A valley is usually between two mountains, whe...</td>\n",
       "      <td>Because rivers end in valleys; assuming the ri...</td>\n",
       "      <td>A town in between mountains presumably would b...</td>\n",
       "      <td>A valley is a low area between two mountains.</td>\n",
       "      <td>1.615639</td>\n",
       "      <td>1.460747</td>\n",
       "      <td>2.280955</td>\n",
       "      <td>1.885454</td>\n",
       "      <td>0.609790</td>\n",
       "      <td>2|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07f108d5321a66f460685f5c7499ecb2</td>\n",
       "      <td>Where would there be an auditorium with only a...</td>\n",
       "      <td>theater, park, university campus, crowd, or li...</td>\n",
       "      <td>university campus</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>Universities have \"speaker series\" and such, a...</td>\n",
       "      <td>Public speaking auditoriums are typically loca...</td>\n",
       "      <td>On university or university-like campuses, the...</td>\n",
       "      <td>An auditorium is a large room used for lecture...</td>\n",
       "      <td>A classroom is where students actively listen ...</td>\n",
       "      <td>2.305915</td>\n",
       "      <td>1.407094</td>\n",
       "      <td>1.761875</td>\n",
       "      <td>0.909707</td>\n",
       "      <td>2.446630</td>\n",
       "      <td>1|3|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07f108d5321a66f460685f5c7499ecb2</td>\n",
       "      <td>Where would there be an auditorium with only a...</td>\n",
       "      <td>theater, park, university campus, crowd, or li...</td>\n",
       "      <td>university campus</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>Universities have \"speaker series\" and such, a...</td>\n",
       "      <td>Public speaking auditoriums are typically loca...</td>\n",
       "      <td>On university or university-like campuses, the...</td>\n",
       "      <td>An auditorium is a large room used for lecture...</td>\n",
       "      <td>A classroom is where students actively listen ...</td>\n",
       "      <td>2.305915</td>\n",
       "      <td>1.407094</td>\n",
       "      <td>1.761875</td>\n",
       "      <td>0.909707</td>\n",
       "      <td>2.446630</td>\n",
       "      <td>1|2|3|4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>a4b44a986e7f9045432e20ea75611df4</td>\n",
       "      <td>What is main benefit to exercising?</td>\n",
       "      <td>get in shape, weight loss, losing weight, heal...</td>\n",
       "      <td>get in shape</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>From a health perspective, exercising is healt...</td>\n",
       "      <td>The main benefit of exercise is that doing it ...</td>\n",
       "      <td>Exercise is a way to get in shape, which is a ...</td>\n",
       "      <td>To get in shape, work out. Exercising can help...</td>\n",
       "      <td>One major reason to exercise is to get healthi...</td>\n",
       "      <td>2.037966</td>\n",
       "      <td>1.910260</td>\n",
       "      <td>0.937910</td>\n",
       "      <td>2.019018</td>\n",
       "      <td>1.937875</td>\n",
       "      <td>1|2|3|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>a4b44a986e7f9045432e20ea75611df4</td>\n",
       "      <td>What is main benefit to exercising?</td>\n",
       "      <td>get in shape, weight loss, losing weight, heal...</td>\n",
       "      <td>get in shape</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>From a health perspective, exercising is healt...</td>\n",
       "      <td>The main benefit of exercise is that doing it ...</td>\n",
       "      <td>Exercise is a way to get in shape, which is a ...</td>\n",
       "      <td>To get in shape, work out. Exercising can help...</td>\n",
       "      <td>One major reason to exercise is to get healthi...</td>\n",
       "      <td>2.037966</td>\n",
       "      <td>1.910260</td>\n",
       "      <td>0.937910</td>\n",
       "      <td>2.019018</td>\n",
       "      <td>1.937875</td>\n",
       "      <td>1|3|4|5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>05490e6c191fbc3c2fe0033ed0bd8aa0</td>\n",
       "      <td>What can you use to store a book while traveling?</td>\n",
       "      <td>backpack, library of congress, pocket, suitcas...</td>\n",
       "      <td>suitcase</td>\n",
       "      <td>0</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>As a sort of measurement of \"lines\" and \"words...</td>\n",
       "      <td>Suitcases are often used to store books while ...</td>\n",
       "      <td>Suitcases are often used to store a variety of...</td>\n",
       "      <td>People can store books in their suitcases, esp...</td>\n",
       "      <td>Settlers once carried their houses on their ba...</td>\n",
       "      <td>2.234683</td>\n",
       "      <td>0.562469</td>\n",
       "      <td>1.569649</td>\n",
       "      <td>1.447909</td>\n",
       "      <td>2.276688</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>05490e6c191fbc3c2fe0033ed0bd8aa0</td>\n",
       "      <td>What can you use to store a book while traveling?</td>\n",
       "      <td>backpack, library of congress, pocket, suitcas...</td>\n",
       "      <td>suitcase</td>\n",
       "      <td>0</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>As a sort of measurement of \"lines\" and \"words...</td>\n",
       "      <td>Suitcases are often used to store books while ...</td>\n",
       "      <td>Suitcases are often used to store a variety of...</td>\n",
       "      <td>People can store books in their suitcases, esp...</td>\n",
       "      <td>Settlers once carried their houses on their ba...</td>\n",
       "      <td>2.234683</td>\n",
       "      <td>0.562469</td>\n",
       "      <td>1.569649</td>\n",
       "      <td>1.447909</td>\n",
       "      <td>2.276688</td>\n",
       "      <td>2|3|4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>05490e6c191fbc3c2fe0033ed0bd8aa0</td>\n",
       "      <td>What can you use to store a book while traveling?</td>\n",
       "      <td>backpack, library of congress, pocket, suitcas...</td>\n",
       "      <td>suitcase</td>\n",
       "      <td>0</td>\n",
       "      <td>greedy</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>As a sort of measurement of \"lines\" and \"words...</td>\n",
       "      <td>Suitcases are often used to store books while ...</td>\n",
       "      <td>Suitcases are often used to store a variety of...</td>\n",
       "      <td>People can store books in their suitcases, esp...</td>\n",
       "      <td>Settlers once carried their houses on their ba...</td>\n",
       "      <td>2.234683</td>\n",
       "      <td>0.562469</td>\n",
       "      <td>1.569649</td>\n",
       "      <td>1.447909</td>\n",
       "      <td>2.276688</td>\n",
       "      <td>2|3|4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Input.id  \\\n",
       "0    5b8a3081c3235d62bc77e2d15f3ad454   \n",
       "1    5b8a3081c3235d62bc77e2d15f3ad454   \n",
       "2    5b8a3081c3235d62bc77e2d15f3ad454   \n",
       "3    07f108d5321a66f460685f5c7499ecb2   \n",
       "4    07f108d5321a66f460685f5c7499ecb2   \n",
       "..                                ...   \n",
       "745  a4b44a986e7f9045432e20ea75611df4   \n",
       "746  a4b44a986e7f9045432e20ea75611df4   \n",
       "747  05490e6c191fbc3c2fe0033ed0bd8aa0   \n",
       "748  05490e6c191fbc3c2fe0033ed0bd8aa0   \n",
       "749  05490e6c191fbc3c2fe0033ed0bd8aa0   \n",
       "\n",
       "                                        Input.question  \\\n",
       "0    A town between two mountains is located in a w...   \n",
       "1    A town between two mountains is located in a w...   \n",
       "2    A town between two mountains is located in a w...   \n",
       "3    Where would there be an auditorium with only a...   \n",
       "4    Where would there be an auditorium with only a...   \n",
       "..                                                 ...   \n",
       "745                What is main benefit to exercising?   \n",
       "746                What is main benefit to exercising?   \n",
       "747  What can you use to store a book while traveling?   \n",
       "748  What can you use to store a book while traveling?   \n",
       "749  What can you use to store a book while traveling?   \n",
       "\n",
       "                                  Input.answer_choices   Input.gold_label  \\\n",
       "0    michigan, valley, state, hospital, or train st...             valley   \n",
       "1    michigan, valley, state, hospital, or train st...             valley   \n",
       "2    michigan, valley, state, hospital, or train st...             valley   \n",
       "3    theater, park, university campus, crowd, or li...  university campus   \n",
       "4    theater, park, university campus, crowd, or li...  university campus   \n",
       "..                                                 ...                ...   \n",
       "745  get in shape, weight loss, losing weight, heal...       get in shape   \n",
       "746  get in shape, weight loss, losing weight, heal...       get in shape   \n",
       "747  backpack, library of congress, pocket, suitcas...           suitcase   \n",
       "748  backpack, library of congress, pocket, suitcas...           suitcase   \n",
       "749  backpack, library of congress, pocket, suitcas...           suitcase   \n",
       "\n",
       "    Input.source_sample_1 Input.source_sample_2 Input.source_sample_3  \\\n",
       "0                       0                     3                     2   \n",
       "1                       0                     3                     2   \n",
       "2                       0                     3                     2   \n",
       "3                       0                     3                     2   \n",
       "4                       0                     3                     2   \n",
       "..                    ...                   ...                   ...   \n",
       "745                     3                     0                greedy   \n",
       "746                     3                     0                greedy   \n",
       "747                     0                greedy                     1   \n",
       "748                     0                greedy                     1   \n",
       "749                     0                greedy                     1   \n",
       "\n",
       "    Input.source_sample_4 Input.source_sample_5  \\\n",
       "0                       1                greedy   \n",
       "1                       1                greedy   \n",
       "2                       1                greedy   \n",
       "3                  greedy                     1   \n",
       "4                  greedy                     1   \n",
       "..                    ...                   ...   \n",
       "745                     1                     2   \n",
       "746                     1                     2   \n",
       "747                     3                     2   \n",
       "748                     3                     2   \n",
       "749                     3                     2   \n",
       "\n",
       "                                   Input.explanation_1  \\\n",
       "0    Because all of the mountains are to the left o...   \n",
       "1    Because all of the mountains are to the left o...   \n",
       "2    Because all of the mountains are to the left o...   \n",
       "3    Universities have \"speaker series\" and such, a...   \n",
       "4    Universities have \"speaker series\" and such, a...   \n",
       "..                                                 ...   \n",
       "745  From a health perspective, exercising is healt...   \n",
       "746  From a health perspective, exercising is healt...   \n",
       "747  As a sort of measurement of \"lines\" and \"words...   \n",
       "748  As a sort of measurement of \"lines\" and \"words...   \n",
       "749  As a sort of measurement of \"lines\" and \"words...   \n",
       "\n",
       "                                   Input.explanation_2  \\\n",
       "0    A valley is usually between two mountains, whe...   \n",
       "1    A valley is usually between two mountains, whe...   \n",
       "2    A valley is usually between two mountains, whe...   \n",
       "3    Public speaking auditoriums are typically loca...   \n",
       "4    Public speaking auditoriums are typically loca...   \n",
       "..                                                 ...   \n",
       "745  The main benefit of exercise is that doing it ...   \n",
       "746  The main benefit of exercise is that doing it ...   \n",
       "747  Suitcases are often used to store books while ...   \n",
       "748  Suitcases are often used to store books while ...   \n",
       "749  Suitcases are often used to store books while ...   \n",
       "\n",
       "                                   Input.explanation_3  \\\n",
       "0    Because rivers end in valleys; assuming the ri...   \n",
       "1    Because rivers end in valleys; assuming the ri...   \n",
       "2    Because rivers end in valleys; assuming the ri...   \n",
       "3    On university or university-like campuses, the...   \n",
       "4    On university or university-like campuses, the...   \n",
       "..                                                 ...   \n",
       "745  Exercise is a way to get in shape, which is a ...   \n",
       "746  Exercise is a way to get in shape, which is a ...   \n",
       "747  Suitcases are often used to store a variety of...   \n",
       "748  Suitcases are often used to store a variety of...   \n",
       "749  Suitcases are often used to store a variety of...   \n",
       "\n",
       "                                   Input.explanation_4  \\\n",
       "0    A town in between mountains presumably would b...   \n",
       "1    A town in between mountains presumably would b...   \n",
       "2    A town in between mountains presumably would b...   \n",
       "3    An auditorium is a large room used for lecture...   \n",
       "4    An auditorium is a large room used for lecture...   \n",
       "..                                                 ...   \n",
       "745  To get in shape, work out. Exercising can help...   \n",
       "746  To get in shape, work out. Exercising can help...   \n",
       "747  People can store books in their suitcases, esp...   \n",
       "748  People can store books in their suitcases, esp...   \n",
       "749  People can store books in their suitcases, esp...   \n",
       "\n",
       "                                   Input.explanation_5  Input.nll_1  \\\n",
       "0        A valley is a low area between two mountains.     1.615639   \n",
       "1        A valley is a low area between two mountains.     1.615639   \n",
       "2        A valley is a low area between two mountains.     1.615639   \n",
       "3    A classroom is where students actively listen ...     2.305915   \n",
       "4    A classroom is where students actively listen ...     2.305915   \n",
       "..                                                 ...          ...   \n",
       "745  One major reason to exercise is to get healthi...     2.037966   \n",
       "746  One major reason to exercise is to get healthi...     2.037966   \n",
       "747  Settlers once carried their houses on their ba...     2.234683   \n",
       "748  Settlers once carried their houses on their ba...     2.234683   \n",
       "749  Settlers once carried their houses on their ba...     2.234683   \n",
       "\n",
       "     Input.nll_2  Input.nll_3  Input.nll_4  Input.nll_5 Answer.acceptable  \n",
       "0       1.460747     2.280955     1.885454     0.609790             2|4|5  \n",
       "1       1.460747     2.280955     1.885454     0.609790             2|4|5  \n",
       "2       1.460747     2.280955     1.885454     0.609790             2|4|5  \n",
       "3       1.407094     1.761875     0.909707     2.446630           1|3|4|5  \n",
       "4       1.407094     1.761875     0.909707     2.446630           1|2|3|4  \n",
       "..           ...          ...          ...          ...               ...  \n",
       "745     1.910260     0.937910     2.019018     1.937875         1|2|3|4|5  \n",
       "746     1.910260     0.937910     2.019018     1.937875           1|3|4|5  \n",
       "747     0.562469     1.569649     1.447909     2.276688                 4  \n",
       "748     0.562469     1.569649     1.447909     2.276688             2|3|4  \n",
       "749     0.562469     1.569649     1.447909     2.276688             2|3|4  \n",
       "\n",
       "[750 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# new_data_splits={'train': None, 'validation': None}\n",
    "# new_data_splits['train']=deepcopy(dataset[\"train\"])\n",
    "# new_data_splits['validation']=deepcopy(dataset[\"validation\"])\n",
    "fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "# fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "# fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "\n",
    "train_df=pd.read_csv(fse_csqa_train_file)\n",
    "\n",
    "dev_df=pd.read_csv(fse_csqa_dev_file)\n",
    "\n",
    "dev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a34acc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fse_train_ids_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2313190/2247815485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfse_train_ids_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfse_train_ids_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fse_train_ids_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(fse_train_ids_list),len(list(set(fse_train_ids_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a498915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "# dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "# fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "# fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2dc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "# fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "# print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "# # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "# fse_data_splits={}\n",
    "# fse_data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "# fse_data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79725037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Input.nll_4', 'Input.answer_choices', 'Input.explanation_5', 'Answer.acceptable', 'Input.source_sample_3', 'Input.nll_1', 'Input.id', 'Input.explanation_3', 'Input.gold_label', 'Input.question', 'Input.nll_5', 'Input.explanation_1', 'Input.source_sample_5', 'Input.nll_3', 'Input.explanation_4', 'Input.source_sample_2', 'Input.explanation_2', 'Input.source_sample_4', 'Input.nll_2', 'Input.source_sample_1'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fse_csqa_train_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10cec13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2bd57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: question_encoding, answer, extractive_explanation, id, abstractive_explanation, question, choices.\n",
      "***** Running training *****\n",
      "  Num examples = 9741\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3654\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdengdenghuang\u001b[0m (\u001b[33mcuhk_lavilab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/scripts/wandb/run-20221119_224932-36zb846r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cuhk_lavilab/huggingface/runs/36zb846r\" target=\"_blank\">./cos_e_output</a></strong> to <a href=\"https://wandb.ai/cuhk_lavilab/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3654' max='3654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3654/3654 06:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-500\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-500/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-1000\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-1000/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-1500\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-1500/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-2000\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-2000/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-2500\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-2500/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-3000\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-3000/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./cos_e_output/111922_222908/checkpoint-3500\n",
      "Configuration saved in ./cos_e_output/111922_222908/checkpoint-3500/config.json\n",
      "Model weights saved in ./cos_e_output/111922_222908/checkpoint-3500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
    "if data_args.generations_filepath is None:\n",
    "    callbacks = [TensorBoardCallback()]\n",
    "    if data_args.early_stopping_patience > 0:\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=data_args.early_stopping_patience))\n",
    "        training_args.load_best_model_at_end = True\n",
    "    else:\n",
    "        training_args.load_best_model_at_end = False  # use the last model state\n",
    "    training_args.metric_for_best_model = 'eval_loss'\n",
    "    training_args.greater_is_better = False\n",
    "    if training_args.eval_steps is None:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "    else:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_splits['train'],\n",
    "        eval_dataset=data_splits['validation'],\n",
    "        data_collator=SequenceCollator(\n",
    "            model=model_class, pad_token=tokenizer.pad_token_id\n",
    "        ),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "# Training. Don't train if it is use_gpt3\n",
    "if training_args.do_train and not model_args.use_gpt3:\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    model = trainer.model\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    train_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b31e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
