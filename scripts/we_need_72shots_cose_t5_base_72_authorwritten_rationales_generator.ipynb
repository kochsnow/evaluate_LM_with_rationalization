{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5286ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdengdenghuang\u001b[0m (\u001b[33mcuhk_lavilab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=evaluate_LM_with_rationalization\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "%env WANDB_PROJECT=evaluate_LM_with_rationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30300ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt3\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Any, NewType\n",
    "\n",
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "#from feature_conversion_methods import format_instance\n",
    "\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments\n",
    ")\n",
    "from metrics import evaluate\n",
    "import torch\n",
    "import datasets\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import random \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "from copy import deepcopy \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers.logging.set_verbosity_info()\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "set_global_logging_level(logging.ERROR, [\"datasets\"])\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "\n",
    "\n",
    "def set_other_seeds(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# inspired by DefaultDataCollator from:\n",
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "# modified to perform batch-level padding.\n",
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa5a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\"\"\"\n",
    "Example-to-Feature conversion methods\n",
    "Modified from\n",
    "https://github.com/salesforce/cos-e/blob/master/code/generation/train_commonsenseqa_v1.0.py and \"\"_v1.11.py (identical)\n",
    "as well as Tensorflow code for WTF?: \n",
    "https://github.com/google-research/google-research/blob/master/wt5/wt5/preprocessors.py\n",
    "\"\"\"\n",
    "# This code is based on https://github.com/allenai/label_rationale_association/blob/main/feature_conversion_methods.py\n",
    "\n",
    "unified_qa_esnli_label_mapping = {0: 'yes', 1: 'maybe', 2: 'no'}\n",
    "unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'} \n",
    "wt5_esnli_label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'} \n",
    "unified_qa_sbic_label_mapping = {\"offensive\": 'Yes', \"not offensive\": 'No'}\n",
    "\n",
    "def format_instance(\n",
    "        example,\n",
    "        tokenizer,\n",
    "        explanation_sep,\n",
    "        max_seq_length=None,\n",
    "        datasource=None,\n",
    "        io_format=None, \n",
    "):\n",
    "    assert datasource in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "    if datasource in [\"cos_e\", \"ecqa\"]:\n",
    "        input_string, answer_string = cqa_formatting(example, io_format, explanation_sep, datasource)\n",
    "    elif datasource == \"esnli\":\n",
    "        input_string, answer_string = esnli_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sbic':\n",
    "        input_string, answer_string = sbic_formatting(example, io_format, explanation_sep)\n",
    "    elif datasource == 'sensemaking':\n",
    "        input_string, answer_string = sensemaking_formatting(example, io_format, explanation_sep)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task. Currently supported: esnli, cos_e, sbic, sensemaking, ecqa.\")\n",
    "    \n",
    "    if 'unified' in io_format and 'unifew' not in io_format:\n",
    "        input_string += '</s>'\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        input_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # note even with \"lm_labels.shift_right()\", the decoder attention mask length is still correct since we remove the last token\n",
    "    dec = tokenizer.encode_plus(\n",
    "        answer_string,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    encodings[\"labels\"] = dec[\"input_ids\"]\n",
    "    encodings[\"decoder_attention_mask\"] = dec[\"attention_mask\"]\n",
    "    encodings[\"question_encoding\"] = encodings[\"input_ids\"]\n",
    "\n",
    "    #return encodings\n",
    "    return {**example, **encodings}\n",
    "\n",
    "#这里很简单 定义好输入输出string就可以的\n",
    "def cqa_formatting(item, io_format, explanation_sep, datasource):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    abstr_expl = item[\"abstractive_explanation\"].lower() if datasource == 'cos_e' else item[\"explanation\"].lower()\n",
    "\n",
    "\n",
    "    if io_format == 't5_fewshot_infilling_with_choices':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" The answer is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain {datasource} question: {question} context: \" + ', '.join(item['choices']) # explain cos_e question: When getting in shape you need to have this in between workouts? context: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == \"record\": \n",
    "        # might not work because cos_e doesn't have a passage \n",
    "        input_string = f\"explain {datasource} query: {question} entities: \" + ', '.join(item['choices']) # explain cos_e query: When getting in shape you need to have this in between workouts? entities: give up, period of recovery, jogging\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # period of recovery because without a period of recovery you will not get any gains.\n",
    "    elif io_format == 'unifiedqa_matching':\n",
    "        choice_ids = ['(A)', '(B)', '(C)', '(D)', '(E)']\n",
    "        input_string = f'explain {question.lower()} \\\\n'\n",
    "        for choice_id, choice in zip(choice_ids, item[\"choices\"]):\n",
    "            input_string += f' {choice_id} {choice.lower()}'\n",
    "        answer_string = f\"{answer.lower()} {explanation_sep} {abstr_expl.lower()}\"\n",
    "        answer_string = answer_string.lower()\n",
    "    elif io_format == 't5_fewshot_infilling_without_choices_use_refined_expl':\n",
    "        input_string = f\"explain {datasource} question: {question} choice: \" + \" choice: \".join(item[\"choices\"]) + f\" <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        input_string = f\"explain {datasource} question: {question} answer: {answer}\" + f\" {explanation_sep} <extra_id_0>\"\n",
    "        answer_string = f\"<extra_id_0> {item['our_explanation']} <extra_id_1>\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "    \n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def esnli_formatting(item, io_format, explanation_sep):\n",
    "\n",
    "    premise = item[\"premise\"]\n",
    "    hypothesis = item[\"hypothesis\"]\n",
    "    answer = unified_qa_esnli_label_mapping[item[\"label\"]] if 'unified' in io_format else wt5_esnli_label_mapping[item[\"label\"]]\n",
    "    abstr_expl = item[\"explanation_1\"].lower() \n",
    "    # Dev/test instances have more than one explanation annotated; merge them into one sequence separated by [SEP] \n",
    "    for k in [2,3]:\n",
    "        if f\"explanation_{k}\" in item and item[f'explanation_{k}']!='': \n",
    "            abstr_expl += f\" [SEP] {item[f'explanation_{k}'].lower()}\"\n",
    "\n",
    "    if io_format == 'standard':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain nli hypothesis: {hypothesis} premise: {premise} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == \"squad\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: {hypothesis} {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\" \n",
    "    elif io_format == \"squad_endswith_what\":\n",
    "        input_string = f\"explain nli question: What is this? context: {hypothesis} {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix\": \n",
    "        input_string = f\"explain nli question: Is this entailment? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_ynm = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer_ynm} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == \"squad_nli_mix_endswith_what\":  \n",
    "        input_string = f\"explain nli question: What is this? context: hypothesis: {hypothesis} premise: {premise}\"  \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"   \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        hypothesis = hypothesis.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        answer = unified_qa_esnli_label_mapping_upper[item[\"label\"]]\n",
    "        input_string = f'explain {premise} Is {hypothesis}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_unifew_nli_mix':\n",
    "        premise = premise.lower().rstrip('.')\n",
    "        unified_qa_esnli_label_mapping_upper = {0: 'Yes', 1: 'Maybe', 2: 'No'}\n",
    "        input_string = f'explain hypothesis: {hypothesis} Is premise: {premise}? \\\\n (A) Yes (B) Maybe (C) No'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 'unifiedqa_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm': \n",
    "        input_string = f'explain is this entailment? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}' \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_ynm_with_choices': \n",
    "        input_string = f'explain is this entailment? \\\\n (A) yes (B) maybe (C) no \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = unified_qa_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n {hypothesis.lower()} {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_v2': \n",
    "        input_string = f'explain what is this? \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_snli_mix_what_with_choices_v2': \n",
    "        input_string = f'explain what is this? \\\\n (A) entailment (B) neutral (C) contradiction \\\\n hypothesis: {hypothesis.lower()} premise: {premise.lower()}'  \n",
    "        answer = wt5_esnli_label_mapping[item[\"label\"]]\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"     \n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported.\")\n",
    "\n",
    "    return input_string, answer_string\n",
    "\n",
    "\n",
    "def sbic_formatting(item, io_format, explanation_sep):\n",
    "    # We pre-processed the SBIC dataset such that we join multiple implied statements with the [SEP] token for dev/test instances \n",
    "    # Each annotation in the training split is a separate instance\n",
    "    post = item['post']\n",
    "    answer = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] if 'unified' in io_format else item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "    abstr_expl = item[\"targetStereotype\"]\n",
    "    \n",
    "\n",
    "    if io_format == 'cola':\n",
    "        input_string = f\"explain sbic sentence: {post}\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"  \n",
    "    elif io_format == 't5_fewshot_infilling':\n",
    "        input_string = f\"explain sbic sentence: {post} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':\n",
    "        answer_bool = 'True' if item[\"offensiveYN\"] == 'offensive' else 'False'\n",
    "        input_string = f\"explain sbic sentence: {post} It is <extra_id_0> that this is offensive {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_more_natural':\n",
    "        input_string = f\"explain sbic sentence: {post} This is <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer} <extra_id_1> {abstr_expl} <extra_id_2>\"\n",
    "    elif io_format == 'squad_yn': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_yn_with_tags': \n",
    "        input_string = f\"explain sbic question: Is this offensive? context: post: {post}\" # explain sbic question: Is this offensive? context: It's only happened to you like two times.\n",
    "        answer_yn = unified_qa_sbic_label_mapping[item[\"offensiveYN\"]] \n",
    "        answer_string = f\"{answer_yn} {explanation_sep} {abstr_expl}\" #  No because this post does not imply anything offensive\n",
    "    elif io_format == 'squad_what': \n",
    "        input_string = f\"explain sbic question: What is this? context: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'squad_what_with_tags': \n",
    "        input_string = f\"explain sbic question: What is this? context: post: {post}\" # explain sbic question: This is what? context: It's only happened to you like two times.\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")        \n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\" # not_offensive because this post does not imply anything offensive    \n",
    "    elif io_format == 'unified_qa_yn':\n",
    "        input_string = f'explain is this offensive? \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\" \n",
    "    elif io_format == 'unified_qa_yn_with_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_yn_with_choices':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_yn_with_choices_and_tags':\n",
    "        input_string = f'explain is this offensive? \\\\n (A) yes (B) no \\\\n post: {post.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what':\n",
    "        input_string = f'explain what is this? \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_tags':\n",
    "        input_string = f'explain what is this? \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unified_qa_what_with_choices':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"\n",
    "    elif io_format == 'unified_qa_what_with_choices_and_tags':\n",
    "        input_string = f'explain what is this? \\\\n (A) offensive (B) not_offensive \\\\n post: {post.lower()}'\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_unifew':\n",
    "        input_string = f\"Topic? \\\\n (A) offensive (B) not_offensive \\\\n {post}\"\n",
    "        answer = item[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "        answer_string = f\"{answer} {explanation_sep} {abstr_expl}\"\n",
    "    else:\n",
    "        raise ValueError(\"The IO format is not supported. Choose `standard` or `masked_cause_generate`.\")\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string\n",
    "\n",
    "def sensemaking_formatting(item, io_format, explanation_sep):\n",
    "    # TODO: explore whether removing periods makes difference? \n",
    "    sent0 = item['sent0']\n",
    "    sent1 = item['sent1']\n",
    "    nonsensical_sentence = str(int(item['label'])+1)\n",
    "    explanation = item['explanation'].lower()\n",
    "\n",
    "    if io_format == 'copa_with_question':\n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} question: nonsensical\"\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 'copa_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} Less common is choice2\"\n",
    "        answer_string = f\"{answer_bool} {explanation_sep} {explanation}\"\n",
    "    elif io_format == 't5_fewshot_infilling':  \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} <extra_id_0> {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> choice{nonsensical_sentence} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == 't5_fewshot_infilling_bool':  \n",
    "        answer_bool = str(bool(int(item['label']))) # True if choice2 is more nonsensical    \n",
    "        input_string = f\"explain sensemaking choice1: {sent0} choice2: {sent1} It is <extra_id_0> that choice2 is less common {explanation_sep} <extra_id_1>\"\n",
    "        answer_string = f\"<extra_id_0> {answer_bool} <extra_id_1> {explanation} <extra_id_2>\"\n",
    "    elif io_format == \"squad_yn\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_yn_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: Is choice2 more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer = \"Yes\" if bool(int(item['label'])) else \"No\"\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: choice1: {sent0} choice2: {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"squad_what_no_tags\": \n",
    "        input_string = f\"explain sensemaking question: What is more nonsensical? context: {sent0} {sent1}\" # explain sensemaking question: What is nonsensical, choice1 or choice2? context: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" #  choice1 because state flowers are unique to each state.  \n",
    "    elif io_format == \"record\": \n",
    "        input_string = f\"explain sensemaking query: What is more nonsensical? entities: choice1, choice2 passage: choice1: {sent0} choice2: {sent1}\" # explain sensemaking query: What is nonsensical? entities: choice1, choice2 passage: choice1: All state flowers are the scarlet carnation. choice2: The New Jersey state flower is the scarlet carnation.\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation}\" # choice1 because state flowers are unique to each state.\n",
    "    elif io_format == 'unifiedqa_yn_with_choices':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n (A) yes (B) no \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\" \n",
    "    elif io_format == 'unifiedqa_yn':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_yn_no_tags':\n",
    "        answer = \"yes\" if bool(int(item['label'])) else \"no\"\n",
    "        input_string = f'explain is choice2 more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"{answer} {explanation_sep} {explanation.lower()}\"  \n",
    "    elif io_format == 'unifiedqa_what_with_choices':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n (A) choice1 (B) choice2 \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n choice1: {sent0.lower()} choice2: {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "    elif io_format == 'unifiedqa_what_no_tags':\n",
    "        nonsensical_sentence = str(int(item['label'])+1)\n",
    "        input_string = f'explain what is more nonsensical? \\\\n {sent0.lower()} {sent1.lower()}'\n",
    "        answer_string = f\"choice{nonsensical_sentence} {explanation_sep} {explanation.lower()}\"  # use \" BECAUSE \"\n",
    "\n",
    "\n",
    "    input_string = ' '.join(input_string.split())\n",
    "    answer_string = ' '.join(answer_string.split())\n",
    "    return input_string, answer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde6c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "01/26/2023 15:56:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "01/26/2023 15:56:58 - INFO - __main__ -   Save path: ./cos_e_output_t5_3b/012623_155658\n",
      "01/26/2023 15:56:58 - INFO - __main__ -   Git branch: dev\n",
      "01/26/2023 15:56:58 - INFO - __main__ -   Git hash: 1cbb5c3b4e53baf31cbafc20d9655c63f091f901\n"
     ]
    }
   ],
   "source": [
    "og_start_time = time.time()\n",
    "\n",
    "#parser = HfArgumentParser(\n",
    "#    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#)\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, unused_args = parser.parse_args_into_dataclasses(\n",
    "    [\"--model_type\", \"t5-3b\",\n",
    "     \"--tokenizer_name\", \"t5-3b\",\n",
    "     \"--task_name\", \"cos_e\", \n",
    "     \"--output_dir\", \"./cos_e_output_t5_3b\", \n",
    "     \"--n_shots\", \"10\",\n",
    "     \"--do_train\", \"True\"], return_remaining_strings=True)\n",
    "if unused_args != []:\n",
    "    raise ValueError(f\"Received unused arguments: {unused_args}\")\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "\n",
    "if model_args.use_gpt3:\n",
    "    assert training_args.do_train\n",
    "    assert not training_args.do_eval\n",
    "    assert data_args.generations_filepath is None\n",
    "    if data_args.gpt3_max_eval_size is not None:\n",
    "        assert data_args.gpt3_max_eval_size <= data_args.fewshot_eval_size\n",
    "        assert data_args.gpt3_max_eval_size % 2 == 0\n",
    "        assert data_args.gpt3_max_eval_size % 3 == 0\n",
    "\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "if not training_args.do_train and data_args.generations_filepath is None:\n",
    "    if not model_args.pretrained_model_file:\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation\"\n",
    "        )\n",
    "\n",
    "if training_args.do_train:\n",
    "    # create a save directory and a logfile\n",
    "    training_args.output_dir = os.path.join(\n",
    "        training_args.output_dir, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "            os.path.exists(training_args.output_dir)\n",
    "            and os.listdir(training_args.output_dir)\n",
    "            and training_args.do_train\n",
    "            and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "model_class = \"t5\"\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "            os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "set_other_seeds(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684646b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.set_device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a3172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/26/2023 15:56:58 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/huangyongfeng/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "model_class = \"t5\"\n",
    "tokenizer_name = TOKENIZER_MAPPING[model_class]\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "model_args.tokenizer_name='t5-base'\n",
    "tokenizer = tokenizer_name.from_pretrained(model_args.tokenizer_name)#, cache_dir=model_args.cache_dir)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62eef1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(task_name='cos_e', early_stopping_patience=10, overwrite_cache=False, train_predict=False, test_predict=False, dev_predict=False, version_name='v1.11', generations_filepath=None, n_shots=10, fewshot_eval_size=350, io_format='t5_fewshot_infilling_without_choices_use_refined_expl', explanation_sep='explanation', data_path=None, gpt3_max_eval_size=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "data_args.io_format=\"t5_fewshot_infilling_without_choices_use_refined_expl\"\n",
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc49c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013701438903808594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026835eec7fc437abd92bff55296a17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer', 'extractive_explanation', 'question', 'abstractive_explanation', 'id', 'choices'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(data_args.task_name, data_args.version_name)\n",
    "train_ids_list=[x['id'] for x in dataset[\"train\"]]\n",
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294780bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012697219848632812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492735ce36ab45c789598bd1621de409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6723, 3269, 12, 6020, 7815, 0, 13, 7, 3556, 4587, 3166, 2607, 1999, 332, 8571, 2, 3520, 6, 1425, 2361, 3764, 7728, 8352, 1273, 5191, 496, 11, 8143, 6929, 4552, 4652, 7764, 6736, 401, 147, 8339, 3128, 5556, 8769, 4417, 8778, 7145, 1279, 6297, 7149, 5884, 10, 5747, 6273, 8, 9504, 8057, 1307, 2183, 1699, 88, 3228, 7178, 2140, 4048, 1041, 1, 3, 5868, 111, 7889, 5370, 8886, 2137, 6980, 9117, 1891, 7827, 1484, 14, 3668, 7090, 5, 7817, 6213, 4656, 5074, 7514, 5896, 4, 9, 7991, 58, 1887, 1598, 3994, 2212, 2274, 6830, 8826, 1604, 9008, 2013, 6521, 4793, 4338, 3923, 8091, 4904, 3938, 3900, 8156, 5806, 8363, 1787, 1496, 5753, 6351, 1104, 1222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rationale generation labeled data construction 115\n",
    "import pandas as pd\n",
    "scr_csqa_labeled_path=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/handwritten_cose_v1.11_examples.csv\"\n",
    "scr_csqa_label_df=pd.read_csv(scr_csqa_labeled_path)\n",
    "scr_csqa_label_data=datasets.load_dataset('csv', data_files=scr_csqa_labeled_path)\n",
    "scr_csqa_label_ids_list=[x['id'] for x in scr_csqa_label_data['train']]\n",
    "scr_csqa_indexs_list=[train_ids_list.index(id_) for id_ in scr_csqa_label_ids_list]\n",
    "scr_csqa_label_our_explanations_list=[x['our_explanation'] for x in scr_csqa_label_data['train']]\n",
    "print(scr_csqa_indexs_list)\n",
    "data_splits={}\n",
    "data_splits['train']=dataset['train'].select(scr_csqa_indexs_list)\n",
    "\n",
    "refine_train_data=[]\n",
    "for kk, (ex,da) in enumerate(zip(scr_csqa_label_our_explanations_list, data_splits['train'])):\n",
    "#     print(da)\n",
    "    data_splits['train'][kk]['our_explanation']=ex\n",
    "    #print(type(data_splits['train'][kk]),data_splits['train'][kk].keys())\n",
    "    da['our_explanation']=ex\n",
    "    refine_train_data.append(da)\n",
    "\n",
    "len(data_splits['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d97f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012493133544921875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1142b373af43a79929b71e3ac87229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '5b8a3081c3235d62bc77e2d15f3ad454',\n",
       " 'question': 'A town between two mountains is located in a what?',\n",
       " 'choices': ['valley', 'hospital', 'state', 'train station', 'michigan'],\n",
       " 'answer': 'valley',\n",
       " 'abstractive_explanation': 'valleys are always between two mountains',\n",
       " 'extractive_explanation': 'A town between two mountains',\n",
       " 'our_explanation': 'A town in between mountains presumably would be in a valley, in which case it is plausable that it would be surrounded by heights in every direction.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rationale unlabeled data construction 991\n",
    "import pdb\n",
    "scr_csqa_unlabeled_test_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=scr_csqa_unlabeled_test_file)\n",
    "scr_csqa_unlabeled_test_df=pd.read_csv(scr_csqa_unlabeled_test_file)\n",
    "fse_csqa_dev_data_dict={}\n",
    "for kk, da in enumerate(fse_csqa_dev_dataset['train']):\n",
    "    #pdb.set_trace()\n",
    "    id_=da['Input.id']\n",
    "    if da['Answer.acceptable']:\n",
    "        answer_accept=set(da['Answer.acceptable'].split('|'))\n",
    "    else:\n",
    "        answer_accept=set()\n",
    "    explanation_list=[da['Input.explanation_1'], \n",
    "                      da['Input.explanation_2'],\n",
    "                      da['Input.explanation_3'],\n",
    "                      da['Input.explanation_4'],\n",
    "                      da['Input.explanation_5']]\n",
    "    if id_ not in fse_csqa_dev_data_dict.keys():\n",
    "        fse_csqa_dev_data_dict[id_]={\"index\":kk,\"id\":id_,\n",
    "                                     \"question\":da[\"Input.question\"],\n",
    "                                     'answer':da['Input.gold_label'],\n",
    "                                     \"accept_set_list\":[answer_accept],\n",
    "                                     \"explanation_list\":explanation_list}\n",
    "        #[[kk, id_, answer_accept, explanation_list]]\n",
    "    else:\n",
    "        fse_csqa_dev_data_dict[id_][\"accept_set_list\"].append(answer_accept)\n",
    "    if len(fse_csqa_dev_data_dict[id_][\"accept_set_list\"])==3:\n",
    "        fse_csqa_dev_data_dict[id_][\"common_expl_list\"]=[]\n",
    "        common_accept_expl_sample=set.intersection(fse_csqa_dev_data_dict[id_][\"accept_set_list\"][0], \n",
    "                                                   fse_csqa_dev_data_dict[id_][\"accept_set_list\"][1], \n",
    "                                                   fse_csqa_dev_data_dict[id_][\"accept_set_list\"][2])\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            fse_csqa_dev_data_dict[id_][\"common_expl_list\"].append(fse_csqa_dev_data_dict[id_][\"explanation_list\"][idx])\n",
    "        \n",
    "#discriminate 3/3 id\n",
    "id_accept_expl_list=[]\n",
    "our_accept_expl_list=[]\n",
    "id_unaccept_expl_list=[]\n",
    "for k,v in fse_csqa_dev_data_dict.items():\n",
    "    accept_set_list=v['accept_set_list']\n",
    "    assert len(accept_set_list)==3\n",
    "    #pdb.set_trace()\n",
    "    common_accept_expl_sample=set.intersection(accept_set_list[0], accept_set_list[1], accept_set_list[2])\n",
    "    #print(accept_set_list,common_accept_expl_sample)\n",
    "    #pdb.set_trace()\n",
    "    if common_accept_expl_sample:\n",
    "        id_accept_expl_list.append(k)\n",
    "        our_expl=\"\"\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            if len(v['explanation_list'][idx]) > len(our_expl):\n",
    "                our_expl = v['explanation_list'][idx]\n",
    "                our_accept_expl_list.append(our_expl)\n",
    "    else:\n",
    "        id_unaccept_expl_list.append(k)\n",
    "\n",
    "dev_ids_list=[x['id'] for x in dataset['validation']]\n",
    "dev_accept_indexs_list=[dev_ids_list.index(id_) for id_ in id_accept_expl_list]\n",
    "dev_unaccpet_indexs_list=[dev_ids_list.index(id_) for id_ in id_unaccept_expl_list]\n",
    "dev_accept_data=dataset['validation'].select(dev_accept_indexs_list)\n",
    "dev_unaccept_data=dataset['validation'].select(dev_unaccpet_indexs_list)\n",
    "\n",
    "\n",
    "\n",
    "new_dev_accept_data=[]\n",
    "new_dev_unaccept_data=[]\n",
    "for oexp, da in zip(our_accept_expl_list,dev_accept_data):\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_dev_accept_data.append(da)\n",
    "for da in dev_unaccept_data:\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_dev_unaccept_data.append(da)\n",
    "        \n",
    "new_dev_accept_data[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36a2e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dev_accept_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b2f00a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'id': '5b8a3081c3235d62bc77e2d15f3ad454',\n",
       " 'question': 'A town between two mountains is located in a what?',\n",
       " 'answer': 'valley',\n",
       " 'accept_set_list': [{'2', '4', '5'}, {'2', '4', '5'}, {'2', '4', '5'}],\n",
       " 'explanation_list': ['Because all of the mountains are to the left of this town, if one was looking on a map, one would be able to see the town in the valley.',\n",
       "  'A valley is usually between two mountains, where it is located in the place where the mountains are closest to each other.',\n",
       "  'Because rivers end in valleys; assuming the river is the one of interest, this sentence is likely referring to the geographical location of the town.',\n",
       "  'A town in between mountains presumably would be in a valley, in which case it is plausable that it would be surrounded by heights in every direction.',\n",
       "  'A valley is a low area between two mountains.'],\n",
       " 'common_expl_list': ['A town in between mountains presumably would be in a valley, in which case it is plausable that it would be surrounded by heights in every direction.',\n",
       "  'A valley is usually between two mountains, where it is located in the place where the mountains are closest to each other.',\n",
       "  'A valley is a low area between two mountains.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fse_csqa_dev_data_dict['5b8a3081c3235d62bc77e2d15f3ad454']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in fse_csqa_dev_data_dict.items():\n",
    "#     #print(v.keys())\n",
    "#     print(\"******\")\n",
    "#     print(v['question'])\n",
    "#     print(v['answer'])\n",
    "#     print(v['common_expl_list'])\n",
    "#     print(\"******\")    \n",
    "# for k,v in fse_csqa_train_data_dict.items():\n",
    "#     #print(v.keys())\n",
    "#     print(\"******\")\n",
    "#     print(v['question'])\n",
    "#     print(v['answer'])\n",
    "#     print(v['common_expl_list'])\n",
    "#     print(\"******\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rationale unlabeled data construction 991\n",
    "import pdb\n",
    "scr_csqa_unlabeled_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=scr_csqa_unlabeled_train_file)\n",
    "scr_csqa_unlabeled_train_df=pd.read_csv(scr_csqa_unlabeled_train_file)\n",
    "fse_csqa_train_data_dict={}\n",
    "for kk, da in enumerate(fse_csqa_train_dataset['train']):\n",
    "    #pdb.set_trace()\n",
    "    id_=da['Input.id']\n",
    "    if da['Answer.acceptable']:\n",
    "        answer_accept=set(da['Answer.acceptable'].split('|'))\n",
    "    else:\n",
    "        answer_accept=set()\n",
    "    explanation_list=[da['Input.explanation_1'], \n",
    "                      da['Input.explanation_2'],\n",
    "                      da['Input.explanation_3'],\n",
    "                      da['Input.explanation_4'],\n",
    "                      da['Input.explanation_5']]\n",
    "    if id_ not in fse_csqa_train_data_dict.keys():\n",
    "        fse_csqa_train_data_dict[id_]={\"index\":kk,\"id\":id_,\n",
    "                                     \"question\":da[\"Input.question\"],\n",
    "                                     'answer':da['Input.gold_label'],\n",
    "                                     \"accept_set_list\":[answer_accept],\n",
    "                                     \"explanation_list\":explanation_list}\n",
    "        #[[kk, id_, answer_accept, explanation_list]]\n",
    "    else:\n",
    "        fse_csqa_train_data_dict[id_][\"accept_set_list\"].append(answer_accept)\n",
    "        \n",
    "    if len(fse_csqa_train_data_dict[id_][\"accept_set_list\"])==3:\n",
    "        fse_csqa_train_data_dict[id_][\"common_expl_list\"]=[]\n",
    "        common_accept_expl_sample=set.intersection(fse_csqa_train_data_dict[id_][\"accept_set_list\"][0], \n",
    "                                                   fse_csqa_train_data_dict[id_][\"accept_set_list\"][1], \n",
    "                                                   fse_csqa_train_data_dict[id_][\"accept_set_list\"][2])\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            fse_csqa_train_data_dict[id_][\"common_expl_list\"].append(fse_csqa_train_data_dict[id_][\"explanation_list\"][idx])\n",
    "\n",
    "    \n",
    "#discriminate 3/3 id\n",
    "id_accept_expl_list=[]\n",
    "our_accept_expl_list=[]\n",
    "id_unaccept_expl_list=[]\n",
    "for k,v in fse_csqa_train_data_dict.items():\n",
    "    accept_set_list=v['accept_set_list']\n",
    "    assert len(accept_set_list)==3\n",
    "    #pdb.set_trace()\n",
    "    common_accept_expl_sample=set.intersection(accept_set_list[0], accept_set_list[1], accept_set_list[2])\n",
    "    #print(accept_set_list,common_accept_expl_sample)\n",
    "    #pdb.set_trace()\n",
    "    if common_accept_expl_sample:\n",
    "        id_accept_expl_list.append(k)\n",
    "        our_expl=\"\"\n",
    "        for idx in list(common_accept_expl_sample):\n",
    "            idx=int(idx)-1\n",
    "            if len(v['explanation_list'][idx]) > len(our_expl):\n",
    "                our_expl = v['explanation_list'][idx]\n",
    "                our_accept_expl_list.append(our_expl)\n",
    "    else:\n",
    "        id_unaccept_expl_list.append(k)\n",
    "\n",
    "train_ids_list=[x['id'] for x in dataset['train']]\n",
    "train_accept_indexs_list=[train_ids_list.index(id_) for id_ in id_accept_expl_list]\n",
    "train_unaccpet_indexs_list=[train_ids_list.index(id_) for id_ in id_unaccept_expl_list]\n",
    "train_accept_data=dataset['train'].select(train_accept_indexs_list)\n",
    "train_unaccept_data=dataset['train'].select(train_unaccpet_indexs_list)\n",
    "\n",
    "\n",
    "\n",
    "new_train_accept_data=[]\n",
    "new_train_unaccept_data=[]\n",
    "for oexp, da in zip(our_accept_expl_list,train_accept_data):\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_train_accept_data.append(da)\n",
    "for da in train_unaccept_data:\n",
    "    da[\"our_explanation\"]=oexp\n",
    "    new_train_unaccept_data.append(da)\n",
    "        \n",
    "new_train_accept_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb71cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_train_data=refine_train_data\n",
    "refine_dev_data=new_train_accept_data + new_dev_accept_data\n",
    "refine_test_data=new_train_unaccept_data + new_dev_unaccept_data\n",
    "\n",
    "refine_train_ids_list=[x['id'] for x in refine_train_data]\n",
    "refine_dev_ids_list=[x['id'] for x in refine_dev_data]\n",
    "refine_test_ids_list=[x['id'] for x in refine_test_data]\n",
    "\n",
    "set(refine_train_ids_list).intersection(set(refine_dev_ids_list)),set(refine_train_ids_list).intersection(set(refine_test_ids_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data_splits={}\n",
    "our_data_splits['train']=refine_train_data\n",
    "our_data_splits['dev']=refine_dev_data\n",
    "our_data_splits['test']=refine_test_data\n",
    "refine_train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdf34f",
   "metadata": {},
   "source": [
    "# shots number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9681ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2dict(refine_data):\n",
    "    refine_data_dict={}\n",
    "    for key in refine_data[0].keys():\n",
    "        refine_data_dict[key]=[x[key] for x in refine_data]\n",
    "    return refine_data_dict\n",
    "\n",
    "refine_train_data_dict = list2dict(refine_train_data)\n",
    "our_data_splits['train'] = datasets.Dataset.from_dict(refine_train_data_dict).shuffle().select(range(72))\n",
    "\n",
    "refine_dev_data_dict = list2dict(refine_dev_data)\n",
    "our_data_splits['dev'] = datasets.Dataset.from_dict(refine_dev_data_dict)\n",
    "\n",
    "refine_test_data_dict = list2dict(refine_test_data)\n",
    "our_data_splits['test'] = datasets.Dataset.from_dict(refine_test_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "class SequenceCollator:\n",
    "    def __init__(self, pad_token):\n",
    "        # self.pad_token_mapping = {\n",
    "        #     \"lm_labels\": -100,\n",
    "        #     \"attention_mask\": 0,\n",
    "        #     \"decoder_attention_mask\": 0,\n",
    "        #     \"input_ids\": pad_token,\n",
    "        # }\n",
    "        # self.columns = [\n",
    "        #     \"input_ids\",\n",
    "        #     \"attention_mask\",\n",
    "        #     \"lm_labels\",\n",
    "        #     \"decoder_attention_mask\",\n",
    "        # ]\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def collate_batch(self, examples):\n",
    "\n",
    "        # batch inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "# dataset = datasets.load_dataset(data_args.task_name, data_args.version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219866fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_collector = SequenceCollator(0)\n",
    "# train_ds = seq_collector.__call__(dataset['train'])\n",
    "# train_ds\n",
    "# dataset['train'][0].keys()\n",
    "for split in ['train','dev','test']:\n",
    "    our_data_splits[split] = our_data_splits[split].map(\n",
    "            lambda x: format_instance(\n",
    "                x,\n",
    "                tokenizer,\n",
    "                data_args.explanation_sep,\n",
    "                datasource=data_args.task_name,\n",
    "                io_format=data_args.io_format\n",
    "            ),\n",
    "            batched=False,\n",
    "            load_from_cache_file=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wrong_data_splits={'train':[]}\n",
    "for x in our_data_splits['train']:\n",
    "    for ii in range(5):\n",
    "        new_x={'id':x['id'], 'real_answer':x['answer'], \n",
    "               'answer':x['choices'][ii], 'question':x['question'],\n",
    "               'choices':x['choices'],\n",
    "               'abstractive_explanation':x['abstractive_explanation'],\n",
    "               'extractive_explanation':x['extractive_explanation'],\n",
    "               'our_explanation':x['our_explanation']}\n",
    "        our_wrong_data_splits['train'].append(new_x)\n",
    "    #print(x)\n",
    "    #pdb.set_trace()\n",
    "# wandb.init()\n",
    "our_wrong_train_data_dict = list2dict(our_wrong_data_splits['train'])\n",
    "our_wrong_data_splits['train'] = datasets.Dataset.from_dict(our_wrong_train_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wrong_data_splits['dev']=[]\n",
    "for x in our_data_splits['dev']:\n",
    "    for ii in range(5):\n",
    "        new_x={'id':x['id'], 'real_answer':x['answer'], \n",
    "               'answer':x['choices'][ii], 'question':x['question'],\n",
    "               'choices':x['choices'],\n",
    "               'abstractive_explanation':x['abstractive_explanation'],\n",
    "               'extractive_explanation':x['extractive_explanation'],\n",
    "               'our_explanation':x['our_explanation']}\n",
    "        our_wrong_data_splits['dev'].append(new_x)\n",
    "    #print(x)\n",
    "    #pdb.set_trace()\n",
    "# wandb.init()\n",
    "our_wrong_dev_data_dict = list2dict(our_wrong_data_splits['dev'])\n",
    "our_wrong_data_splits['dev'] = datasets.Dataset.from_dict(our_wrong_dev_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_wrong_data_splits['train'][0],our_wrong_data_splits['train'][0].keys()\n",
    "our_wrong_data_splits['train'] = our_wrong_data_splits['train'].map(\n",
    "            lambda x: format_instance(\n",
    "                x,\n",
    "                tokenizer,\n",
    "                data_args.explanation_sep,\n",
    "                datasource=data_args.task_name,\n",
    "                io_format=data_args.io_format\n",
    "            ),\n",
    "            batched=False,\n",
    "            load_from_cache_file=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_wrong_data_splits['dev']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"real_answer: {}\".format(da['real_answer']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa34701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_wrong_data_splits['train'][0],our_wrong_data_splits['train'][0].keys()\n",
    "# our_wrong_data_splits['dev'] = our_wrong_data_splits['dev'].map(\n",
    "#             lambda x: format_instance(\n",
    "#                 x,\n",
    "#                 tokenizer,\n",
    "#                 data_args.explanation_sep,\n",
    "#                 datasource=data_args.task_name,\n",
    "#                 io_format=data_args.io_format\n",
    "#             ),\n",
    "#             batched=False,\n",
    "#             load_from_cache_file=False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_wrong_data_splits['dev']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"real_answer: {}\".format(da['real_answer']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91605cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_wrong_data_splits['train']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"real_answer: {}\".format(da['real_answer']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# training_args.run_name=\"\"\n",
    "training_args.logging_steps=3\n",
    "training_args.save_steps=5\n",
    "training_args.evaluation_strategy=\"epoch\"\n",
    "training_args.num_train_epochs=15\n",
    "training_args.do_eval=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317de5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.per_device_eval_batch_size=8\n",
    "training_args.per_device_train_batch_size=8\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80480ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "# original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "# data_args.io_format=\"t5_fewshot_infilling_with_choices\"\n",
    "# # Data loading from huggingface's datasets\n",
    "# if data_args.task_name in {\"cos_e\", \"esnli\"}:\n",
    "#     version_arg = None\n",
    "#     if data_args.task_name == \"cos_e\":\n",
    "#         assert data_args.version_name in {\"v1.11\", \"v1.0\"}\n",
    "#         version_arg = data_args.version_name\n",
    "\n",
    "#     load_train = True\n",
    "#     if (not training_args.do_train\n",
    "#         and not training_args.do_eval\n",
    "#         and not data_args.train_predict\n",
    "#     ):\n",
    "#         # don't load training dataset\n",
    "#         dataset = {}\n",
    "#         dataset[\"train\"] = None\n",
    "#         dataset[\"validation\"] = datasets.load_dataset(\n",
    "#             data_args.task_name, version_arg, split=\"validation\"\n",
    "#         )\n",
    "#         data_splits['validation'] = dataset[\"validation\"]\n",
    "\n",
    "#         if data_args.task_name == \"esnli\":\n",
    "#             dataset[\"test\"] = datasets.load_dataset(data_args.task_name, split=\"test\")\n",
    "#             data_splits['test'] = dataset[\"test\"]\n",
    "#         load_train = False\n",
    "#     else:\n",
    "#         dataset = datasets.load_dataset(data_args.task_name, version_arg)\n",
    "\n",
    "#         if data_args.n_shots > 0: # Shots = number of training examples **per label** \n",
    "#             if data_args.task_name == 'esnli': # Construct a *balanced* random sample of the size `data_args.n_shots*len(labels)` (for train) or `data_args.fewshot_eval_size` (for eval)\n",
    "#                 for split in [\"train\", \"validation\", \"test\"]:\n",
    "#                     split_data = dataset[split]\n",
    "#                     label_subsets = []\n",
    "#                     labels = split_data.features['label'].names\n",
    "#                     sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size/len(labels))\n",
    "#                     if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "#                         assert len(labels) == 3\n",
    "#                         sample_size = data_args.gpt3_max_eval_size // len(labels)\n",
    "#                     for label in labels:\n",
    "#                         # The following is a hack to only run on `neutral` labels of `esnli` to get data for human eval\n",
    "#                         # if data_args.gpt3_max_eval_size is not None and split != 'train' and label != 'neutral':\n",
    "#                         #     continue\n",
    "#                         label_int = split_data.features['label'].str2int(label)\n",
    "#                         label_set = split_data.filter(lambda example: example['label'] == label_int).shuffle() # all instances of labeled as `label`\n",
    "#                         label_subset = label_set.select(range(sample_size)) #select `sample_size` random instances labeled as `label`\n",
    "#                         label_subsets.append(label_subset)\n",
    "#                     dataset[split] = datasets.concatenate_datasets(label_subsets) #merge all label-specific instances\n",
    "#             elif data_args.task_name == 'cos_e': \n",
    "#                 for split in [\"train\", \"validation\"]: \n",
    "#                     split_data = dataset[split]\n",
    "#                     sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size) #Shots for QA are not label-specific, i.e., `n_shots` is the training data size\n",
    "#                     if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "#                         sample_size = data_args.gpt3_max_eval_size\n",
    "#                     dataset[split] = split_data#.shuffle().select(range(sample_size)) # select `sample_size` random instances\n",
    "#             else: \n",
    "#                 raise ValueError('Only cos_e and esnli are supported by Huggingface datasets.')\n",
    "#     # Apply method, and format dataset to torch.Tensor outputs\n",
    "# #     fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "# #     fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "# #     fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "# #     fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "# #     train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "# #     dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "# #     fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "# #     fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n",
    "# #     fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "# #     fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "# #     print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "# #     # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "# #     fse_data_splits={}\n",
    "# #     data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "# #     data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)\n",
    "#     for split in dataset.keys():\n",
    "#         if dataset[split] is not None:\n",
    "#             dataset[split] = dataset[split].map(\n",
    "#                 lambda x: format_instance(\n",
    "#                     x,\n",
    "#                     tokenizer,\n",
    "#                     data_args.explanation_sep,\n",
    "#                     datasource=data_args.task_name,\n",
    "#                     io_format=data_args.io_format\n",
    "#                 ),\n",
    "#                 batched=False,\n",
    "#                 load_from_cache_file=False,\n",
    "#             )\n",
    "#     data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "#     data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "#     if data_args.task_name == \"esnli\":\n",
    "#         data_splits[\"test\"] = deepcopy(dataset[\"test\"])\n",
    "\n",
    "#     original_data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "#     original_data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "#     if data_args.task_name == \"esnli\":\n",
    "#         original_data_splits[\"test\"] = deepcopy(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # new_data_splits={'train': None, 'validation': None}\n",
    "# # new_data_splits['train']=deepcopy(dataset[\"train\"])\n",
    "# # new_data_splits['validation']=deepcopy(dataset[\"validation\"])\n",
    "# fse_csqa_train_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_train.csv\"\n",
    "# fse_csqa_dev_file=\"/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/few_shot_explanations/data/acceptability_annotations/commonsenseqa_test.csv\"\n",
    "# # fse_csqa_train_dataset = datasets.load_dataset('csv', data_files=fse_csqa_train_file)\n",
    "# # fse_csqa_dev_dataset = datasets.load_dataset('csv', data_files=fse_csqa_dev_file)\n",
    "\n",
    "# train_df=pd.read_csv(fse_csqa_train_file)\n",
    "\n",
    "# dev_df=pd.read_csv(fse_csqa_dev_file)\n",
    "\n",
    "# dev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42739e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(fse_train_ids_list),len(list(set(fse_train_ids_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids_list=[x['id'] for x in data_splits[\"train\"]]\n",
    "# dev_ids_list=[x['id'] for x in data_splits[\"validation\"]]\n",
    "# fse_train_ids_list=[x['Input.id'] for x in fse_csqa_train_dataset['train']]\n",
    "# fse_dev_ids_list=[x['Input.id'] for x in fse_csqa_dev_dataset['train']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fse_train_indexs_list=[train_ids_list.index(id_) for id_ in fse_train_ids_list]\n",
    "# fse_dev_indexs_list=[dev_ids_list.index(id_) for id_ in fse_dev_ids_list]\n",
    "# print(len(fse_train_indexs_list), len(fse_dev_indexs_list))\n",
    "# # print(fse_train_indexs_list,fse_dev_indexs_list)\n",
    "# fse_data_splits={}\n",
    "# fse_data_splits['train']=data_splits[\"train\"].select(fse_train_indexs_list)\n",
    "# fse_data_splits['validation']=data_splits[\"validation\"].select(fse_train_indexs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d186954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fse_csqa_train_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
    "if data_args.generations_filepath is None:\n",
    "    callbacks = [TensorBoardCallback()]\n",
    "    if data_args.early_stopping_patience > 0:\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=data_args.early_stopping_patience))\n",
    "        training_args.load_best_model_at_end = True\n",
    "    else:\n",
    "        training_args.load_best_model_at_end = False  # use the last model state\n",
    "    training_args.metric_for_best_model = 'eval_loss'\n",
    "    training_args.greater_is_better = False\n",
    "    if training_args.eval_steps is None:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "    else:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=our_data_splits['train'],\n",
    "        eval_dataset=our_data_splits['dev'],\n",
    "        data_collator=SequenceCollator(\n",
    "            model=model_class, pad_token=tokenizer.pad_token_id\n",
    "        ),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "# Training. Don't train if it is use_gpt3\n",
    "if training_args.do_train and not model_args.use_gpt3:\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    model = trainer.model\n",
    "    wandb.finish()\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    train_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(our_data_splits['dev'][0]['our_explanation'])\n",
    "# inp_ids=torch.tensor(our_data_splits['dev'][0][\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "# out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "# skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "# words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88935f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(our_data_splits['dev']),len(our_data_splits['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_data_splits['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c15190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_data_splits['train']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_data_splits['dev']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69977540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_model = T5ForConditionalGeneration.from_pretrained(\"./cos_e_output_t5_3b/112022_235539/checkpoint-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fca4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for da in our_data_splits['dev']:\n",
    "#     print(\"*******\")\n",
    "#     print(\"question: {}\".format(da['question']))\n",
    "#     print(\"answer: {}\".format(da['answer']))\n",
    "#     print(\"choices: {}\".format(da['choices']))\n",
    "#     print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "#     inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "#     out = good_model.generate(\n",
    "#                     inp_ids,\n",
    "#                     max_length=100,\n",
    "#                     pad_token_id=tokenizer.pad_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id,\n",
    "#                 )\n",
    "#     skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "#     words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "#     print(\"generated explanation: {}\".format(words))\n",
    "#     print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7de102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = trainer.evaluate(our_data_splits['train'].select([0,1,2,3,4]))\n",
    "# train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43528ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# if training_args.do_eval:\n",
    "#     start_time = time.time()\n",
    "#     logger.info(\"*** Evaluate on train set***\")\n",
    "#     logger.info(len(data_splits['train']))\n",
    "#     train_output = trainer.evaluate(our_data_splits['train'])\n",
    "#     perplexity = math.exp(train_output[\"eval_loss\"])\n",
    "#     results[\"perplexity_train\"] = perplexity\n",
    "\n",
    "#     # repeat\n",
    "#     logger.info(\"*** Evaluate on dev set***\")\n",
    "#     logger.info(len(data_splits['validation']))\n",
    "#     eval_output = trainer.evaluate(data_splits['validation'])\n",
    "#     perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "#     results[\"perplexity_validation\"] = perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d2ec9",
   "metadata": {},
   "source": [
    "# Rationale Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5eeaa4",
   "metadata": {},
   "source": [
    "## Construction of generated rationale and accepted rationale pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ebf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fse_csqa_dev_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "fse_csqa_dev_data_dict\n",
    "fse_csqa_train_data_dict\n",
    "rationale_pair_dev_data = []\n",
    "cc=0\n",
    "good_model = model\n",
    "\n",
    "for da in tqdm(our_data_splits['dev'], total=len(our_data_splits['dev'])):\n",
    "    print(\"*******\")\n",
    "    print(\"question: {}\".format(da['question']))\n",
    "    print(\"answer: {}\".format(da['answer']))\n",
    "    print(\"choices: {}\".format(da['choices']))\n",
    "    #print(\"our_explanation: {}\".format(da['our_explanation']))\n",
    "    id_ = da['id']\n",
    "    if id_ in fse_csqa_dev_data_dict.keys():\n",
    "        common_expl_list = fse_csqa_dev_data_dict[id_]['common_expl_list']\n",
    "    else:\n",
    "        common_expl_list = fse_csqa_train_data_dict[id_]['common_expl_list']\n",
    "    da[\"common_expl_list\"] = common_expl_list\n",
    "    print(\"common expl list: {}\".format(common_expl_list))\n",
    "    inp_ids = torch.tensor(da[\"input_ids\"], device=model.device).reshape(1, -1)\n",
    "    out = good_model.generate(\n",
    "                    inp_ids,\n",
    "                    max_length=100,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "    skip_special_tokens = False if \"infilling\" in data_args.io_format else True\n",
    "    words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "    print(\"generated explanation: {}\".format(words))\n",
    "    da[\"generated_explanation\"] = words\n",
    "    print(\"########\")\n",
    "    rationale_pair_dev_data.append(da)\n",
    "    cc += 1\n",
    "    if cc > 200:\n",
    "        break\n",
    "    #pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a66b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rationale_pair_save_path = os.path.join(\"./results\", \"72shots_cose_t5_base_authorwritten_rationales_generator_test_rationale_pair.json\")\n",
    "with open(rationale_pair_save_path, 'w') as f:\n",
    "    json.dump(rationale_pair_dev_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33daef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ddae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121e85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
