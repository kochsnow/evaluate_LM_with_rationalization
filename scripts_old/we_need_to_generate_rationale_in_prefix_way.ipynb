{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac35f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpt3\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Any, NewType\n",
    "\n",
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "\n",
    "from feature_conversion_methods import format_instance\n",
    "\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments\n",
    ")\n",
    "from metrics import evaluate\n",
    "import torch\n",
    "import datasets\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import random \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "from copy import deepcopy \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers.logging.set_verbosity_info()\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "set_global_logging_level(logging.ERROR, [\"datasets\"])\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "\n",
    "\n",
    "def set_other_seeds(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# inspired by DefaultDataCollator from:\n",
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py\n",
    "# modified to perform batch-level padding.\n",
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5d6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "08/25/2022 17:39:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/25/2022 17:39:46 - INFO - __main__ -   Save path: ./cos_e_output/082522_173946\n",
      "08/25/2022 17:39:46 - INFO - __main__ -   Git branch: dev\n",
      "08/25/2022 17:39:46 - INFO - __main__ -   Git hash: b3e471e4130d052883f821ed4b4dd50e701fd4c6\n"
     ]
    }
   ],
   "source": [
    "og_start_time = time.time()\n",
    "\n",
    "#parser = HfArgumentParser(\n",
    "#    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#)\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, unused_args = parser.parse_args_into_dataclasses(\n",
    "    [\"--model_type\", \"t5-base\",\n",
    "     \"--tokenizer_name\", \"t5-base\",\n",
    "     \"--task_name\", \"cos_e\", \n",
    "     \"--output_dir\", \"./cos_e_output\", \n",
    "     \"--n_shots\", \"10\",\n",
    "     \"--do_train\", \"True\"], return_remaining_strings=True)\n",
    "if unused_args != []:\n",
    "    raise ValueError(f\"Received unused arguments: {unused_args}\")\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "\n",
    "if model_args.use_gpt3:\n",
    "    assert training_args.do_train\n",
    "    assert not training_args.do_eval\n",
    "    assert data_args.generations_filepath is None\n",
    "    if data_args.gpt3_max_eval_size is not None:\n",
    "        assert data_args.gpt3_max_eval_size <= data_args.fewshot_eval_size\n",
    "        assert data_args.gpt3_max_eval_size % 2 == 0\n",
    "        assert data_args.gpt3_max_eval_size % 3 == 0\n",
    "\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "if not training_args.do_train and data_args.generations_filepath is None:\n",
    "    if not model_args.pretrained_model_file:\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation\"\n",
    "        )\n",
    "\n",
    "if training_args.do_train:\n",
    "    # create a save directory and a logfile\n",
    "    training_args.output_dir = os.path.join(\n",
    "        training_args.output_dir, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "            os.path.exists(training_args.output_dir)\n",
    "            and os.listdir(training_args.output_dir)\n",
    "            and training_args.do_train\n",
    "            and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "model_class = \"t5\"\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\", \"sbic\", \"sensemaking\", \"ecqa\"}\n",
    "\n",
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "            os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "set_other_seeds(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ba98a",
   "metadata": {},
   "source": [
    "# tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33eed41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2022 17:39:46 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "CONFIG_MAPPING = {\"t5\": T5Config}\n",
    "MODEL_MAPPING = {\"t5\": T5ForConditionalGeneration}\n",
    "TOKENIZER_MAPPING = {\"t5\": T5Tokenizer}\n",
    "model_class = \"t5\"\n",
    "tokenizer_name = TOKENIZER_MAPPING[model_class]\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "model_args.tokenizer_name='t5-base'\n",
    "tokenizer = tokenizer_name.from_pretrained(model_args.tokenizer_name)#, cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b32eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.modeling_utils import (\n",
    "    ModuleUtilsMixin, PushToHubMixin,\n",
    "    logging, Union, Optional, Callable, unwrap_model, get_parameter_dtype,\n",
    "    FLAX_WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME, WEIGHTS_NAME,\n",
    "    is_offline_mode, is_remote_url, hf_bucket_url, cached_path\n",
    ")\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class PushToHubFriendlyModel(nn.Module, ModuleUtilsMixin, PushToHubMixin):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def save_pretrained(\n",
    "            self,\n",
    "            save_directory: Union[str, os.PathLike],\n",
    "            save_config: bool = True,\n",
    "            state_dict: Optional[dict] = None,\n",
    "            save_function: Callable = torch.save,\n",
    "            push_to_hub: bool = False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
    "        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "\n",
    "        Arguments:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory to which to save. Will be created if it doesn't exist.\n",
    "            save_config (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to save the config of the model. Useful when in distributed training like TPUs and need\n",
    "                to call this function on all processes. In this case, set :obj:`save_config=True` only on the main\n",
    "                process to avoid race conditions.\n",
    "            state_dict (nested dictionary of :obj:`torch.Tensor`):\n",
    "                The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to\n",
    "                only save parts of the model or if special precautions need to be taken when recovering the state\n",
    "                dictionary of a model (like when using model parallelism).\n",
    "            save_function (:obj:`Callable`):\n",
    "                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
    "                need to replace :obj:`torch.save` by another method.\n",
    "            push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to push your model to the Hugging Face model hub after saving it.\n",
    "\n",
    "                .. warning::\n",
    "\n",
    "                    Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
    "                    :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
    "                    pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
    "                    instead.\n",
    "\n",
    "            kwargs:\n",
    "                Additional key word arguments passed along to the\n",
    "                :meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
    "            return\n",
    "\n",
    "        if push_to_hub:\n",
    "            commit_message = kwargs.pop(\"commit_message\", None)\n",
    "            repo = self._create_or_get_repo(save_directory, **kwargs)\n",
    "\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Only save the model itself if we are using distributed training\n",
    "        model_to_save = unwrap_model(self)\n",
    "\n",
    "        # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n",
    "        # we currently don't use this setting automatically, but may start to use with v5\n",
    "        dtype = get_parameter_dtype(model_to_save)\n",
    "        self.pretrain_model.config.torch_dtype = str(dtype).split(\".\")[1]\n",
    "\n",
    "        # Attach architecture to the config\n",
    "        self.pretrain_model.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "        # Save the config\n",
    "        if save_config:\n",
    "            self.pretrain_model.config.save_pretrained(save_directory)\n",
    "\n",
    "        # Save the model\n",
    "        if state_dict is None:\n",
    "            state_dict = model_to_save.state_dict()\n",
    "\n",
    "        # Handle the case where some state_dict keys shouldn't be saved\n",
    "        # if self._keys_to_ignore_on_save is not None:\n",
    "        #     state_dict = {k: v for k, v in state_dict.items() if k not in self._keys_to_ignore_on_save}\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "        save_function(state_dict, output_model_file)\n",
    "\n",
    "        logger.info(f\"Model weights saved in {output_model_file}\")\n",
    "\n",
    "        if push_to_hub:\n",
    "            url = self._push_to_hub(repo, commit_message=commit_message)\n",
    "            logger.info(f\"Model pushed to the hub in this commit: {url}\")\n",
    "\n",
    "    def load(self, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Adopted and simplified from transformers.modeling_utils from_pretrained,\n",
    "        but more similiar to load_state_dict(load the weight from anywhere into a create model).\n",
    "\n",
    "        Just for downloading from huggingface platform.\n",
    "\n",
    "        @param pretrained_model_name_or_path:\n",
    "        @param model_args:\n",
    "        @param kwargs:\n",
    "        \"\"\"\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        state_dict = kwargs.pop(\"state_dict\", None)\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        from_tf = kwargs.pop(\"from_tf\", False)\n",
    "        from_flax = kwargs.pop(\"from_flax\", False)\n",
    "        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        mirror = kwargs.pop(\"mirror\", None)\n",
    "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
    "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
    "        _fast_init = kwargs.pop(\"_fast_init\", True)\n",
    "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
    "\n",
    "        from_pt = not (from_tf | from_flax)\n",
    "\n",
    "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
    "        if from_pipeline is not None:\n",
    "            user_agent[\"using_pipeline\"] = from_pipeline\n",
    "\n",
    "        if is_offline_mode() and not local_files_only:\n",
    "            logger.info(\"Offline mode: forcing local_files_only=True\")\n",
    "            local_files_only = True\n",
    "\n",
    "        # Load model\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
    "                    # Load from a TF 1.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
    "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
    "                    # Load from a TF 2.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
    "                elif from_flax and os.path.isfile(os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)):\n",
    "                    # Load from a Flax checkpoint in priority if from_flax\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME)\n",
    "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
    "                    # Load from a PyTorch checkpoint\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
    "                else:\n",
    "                    raise EnvironmentError(\n",
    "                        f\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\n",
    "                        f\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\n",
    "                    )\n",
    "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                archive_file = pretrained_model_name_or_path\n",
    "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
    "                if not from_tf:\n",
    "                    raise ValueError(\n",
    "                        f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n",
    "                        \"from_tf to True to load from this checkpoint.\"\n",
    "                    )\n",
    "                archive_file = pretrained_model_name_or_path + \".index\"\n",
    "            else:\n",
    "                # set correct filename\n",
    "                if from_tf:\n",
    "                    filename = TF2_WEIGHTS_NAME\n",
    "                elif from_flax:\n",
    "                    filename = FLAX_WEIGHTS_NAME\n",
    "                else:\n",
    "                    filename = WEIGHTS_NAME\n",
    "\n",
    "                archive_file = hf_bucket_url(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    filename=filename,\n",
    "                    revision=revision,\n",
    "                    mirror=mirror,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                # Load from URL or cache if already cached\n",
    "                resolved_archive_file = cached_path(\n",
    "                    archive_file,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    local_files_only=local_files_only,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                    user_agent=user_agent,\n",
    "                )\n",
    "            except EnvironmentError as err:\n",
    "                logger.error(err)\n",
    "                msg = (\n",
    "                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n",
    "                )\n",
    "                raise EnvironmentError(msg)\n",
    "\n",
    "            if resolved_archive_file == archive_file:\n",
    "                logger.info(f\"loading weights file {archive_file}\")\n",
    "            else:\n",
    "                logger.info(f\"loading weights file {archive_file} from cache at {resolved_archive_file}\")\n",
    "        else:\n",
    "            resolved_archive_file = None\n",
    "\n",
    "        # load pt weights early so that we know which dtype to init the model under\n",
    "        if from_pt:\n",
    "            if state_dict is None:\n",
    "                try:\n",
    "                    state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "                except Exception:\n",
    "                    raise OSError(\n",
    "                        f\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\n",
    "                        f\"at '{resolved_archive_file}'\"\n",
    "                        \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
    "                    )\n",
    "        self.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7b99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2022 17:39:53 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
      "08/25/2022 17:39:53 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "08/25/2022 17:39:53 - INFO - faiss.loader -   Loading faiss.\n",
      "08/25/2022 17:39:53 - INFO - faiss.loader -   Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# if data_args.generations_filepath is None:\n",
    "#     model_name = MODEL_MAPPING[model_class]\n",
    "#     if model_args.pretrained_model_file:\n",
    "#         model = T5ForConditionalGeneration.from_pretrained(model_args.pretrained_model_file)\n",
    "\n",
    "#         if model_args.dropout_rate:\n",
    "#             raise Exception(\"can't update/specify dropout currently when load pretrained model from directory\")\n",
    "\n",
    "#     elif model_args.pretrained:\n",
    "#         # load pretrained model from HuggingFace\n",
    "#         logger.info(\"Loading pretrained model\")\n",
    "#         if model_args.dropout_rate:\n",
    "#             model = model_name.from_pretrained(model_args.model_type, dropout_rate=model_args.dropout_rate)\n",
    "#         else:\n",
    "#             model = model_name.from_pretrained(model_args.model_type)\n",
    "#     else:\n",
    "#         # load model from scratch with no pretrained weights\n",
    "#         config_name = CONFIG_MAPPING[model_class]()\n",
    "#         # TODO (Sarah): NOTE THIS ONLY DOES T5-BASE; PASS IN ARGS HERE^\n",
    "#         logger.info(\n",
    "#             \"Training new model from scratch using default config (NOTE: SMALL MODELS ONLY FOR NOW)\"\n",
    "#         )\n",
    "#         if model_args.dropout_rate:\n",
    "#             raise Exception(\"sure you want to train a model from scratch?\")\n",
    "#         model = model_name.from_config(config_name)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "# else:\n",
    "#     model = None\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "# from PushToHubFriendlyModel\n",
    "from modeling_auto import AutoModelForSeq2SeqLM\n",
    "from modeling_bart import BartForConditionalGeneration\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class Model(PushToHubFriendlyModel):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        \"\"\"The prefix-tuning code\"\"\"\n",
    "\n",
    "        self.preseqlen = args.prefix_tuning.prefix_sequence_length\n",
    "        self.mid_dim = args.prefix_tuning.mid_dim\n",
    "\n",
    "        print(\"prefix-tuning sequence length is {}.\".format(self.preseqlen))\n",
    "\n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.bert.location, use_fast=False)\n",
    "        self.pretrain_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.bert.location\n",
    "        )\n",
    "        self.config = self.pretrain_model.config\n",
    "\n",
    "        if isinstance(self.pretrain_model, BartForConditionalGeneration):\n",
    "            self.match_n_layer = self.config.decoder_layers\n",
    "            self.match_n_head = self.config.decoder_attention_heads\n",
    "        elif isinstance(self.pretrain_model, (T5ForConditionalGeneration)):\n",
    "            self.match_n_layer = self.config.num_decoder_layers\n",
    "            self.match_n_head = self.config.num_heads\n",
    "        else:\n",
    "            raise ValueError(\"Other models are not supported yet!\")\n",
    "\n",
    "        self.n_embd = self.config.d_model\n",
    "        assert self.n_embd % self.match_n_head == 0\n",
    "        self.match_n_embd = self.n_embd // self.match_n_head\n",
    "\n",
    "        if args.special_tokens:\n",
    "            self.tokenizer.add_tokens([v for k, v in args.special_tokens])\n",
    "            self.pretrain_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # Prefix related.\n",
    "        self.register_buffer('input_tokens', torch.arange(self.preseqlen).long())\n",
    "\n",
    "        self.wte = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_enc = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_enc = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_enc = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.wte_dec = nn.Embedding(self.preseqlen, self.n_embd)\n",
    "        self.control_trans_dec = nn.Sequential(\n",
    "            nn.Linear(self.n_embd, self.mid_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "        )\n",
    "\n",
    "        # Knowledge prompt.\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            self.knowledge_trans_dec = nn.Sequential(\n",
    "                nn.Linear(self.n_embd, self.mid_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.n_embd),\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(args.prefix_tuning.prefix_dropout)\n",
    "\n",
    "#         if self.args.model.freeze_plm:\n",
    "#             for param in self.pretrain_model.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#         if self.args.model.freeze_prefix:\n",
    "#             for param in self.wte.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.control_trans.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.wte_dec.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.control_trans_dec.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.wte_enc.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.control_trans_enc.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "    def get_prompt(self, bsz=None, sample_size=1, description=None, knowledge=None):\n",
    "        old_bsz = bsz\n",
    "        bsz = bsz * sample_size\n",
    "        input_tokens = self.input_tokens.unsqueeze(0).expand(bsz, -1)\n",
    "        temp_control = self.wte(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control = temp_control + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values = self.control_trans(temp_control)  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values = torch.cat([past_key_values, self.knowledge_trans(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Cross prefix\n",
    "        temp_control_dec = self.wte_dec(input_tokens)\n",
    "        if description is not None:\n",
    "            temp_control_dec = temp_control_dec + description.repeat_interleave(sample_size, dim=0).unsqueeze(1)\n",
    "        past_key_values_dec = self.control_trans_dec(\n",
    "            temp_control_dec\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_dec = torch.cat([past_key_values_dec, self.knowledge_trans_dec(knowledge.repeat_interleave(sample_size, dim=0))], dim=1)\n",
    "\n",
    "        bsz, seqlen, _ = past_key_values_dec.shape\n",
    "        past_key_values_dec = past_key_values_dec.view(\n",
    "            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n",
    "        )\n",
    "        past_key_values_dec = self.dropout(past_key_values_dec)\n",
    "        past_key_values_dec = past_key_values_dec.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        # Encoder prefix\n",
    "        input_tokens_enc = (\n",
    "            self.input_tokens.unsqueeze(0).expand(old_bsz, -1)\n",
    "        )\n",
    "        temp_control_enc = self.wte_enc(input_tokens_enc)\n",
    "        if description is not None:\n",
    "            temp_control_enc = temp_control_enc + description.unsqueeze(1)\n",
    "        past_key_values_enc = self.control_trans_enc(\n",
    "            temp_control_enc\n",
    "        )  # bsz, seqlen, layer*emb\n",
    "        if knowledge is not None:\n",
    "            past_key_values_enc = torch.cat([past_key_values_enc, self.knowledge_trans_enc(knowledge)], dim=1)\n",
    "\n",
    "        bsz_enc, seqlen, _ = past_key_values_enc.shape\n",
    "        past_key_values_enc = past_key_values_enc.view(\n",
    "            bsz_enc,\n",
    "            seqlen,\n",
    "            self.match_n_layer * 2,\n",
    "            self.match_n_head,\n",
    "            self.match_n_embd,\n",
    "        )\n",
    "        past_key_values_enc = self.dropout(past_key_values_enc)\n",
    "        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n",
    "\n",
    "        result = []\n",
    "        for i, key_val in enumerate(past_key_values):\n",
    "            temp = dict()\n",
    "            temp[\"decoder_prompt\"] = {\n",
    "                \"prev_key\": key_val[0].contiguous(),\n",
    "                \"prev_value\": key_val[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val.device)\n",
    "                    .bool()\n",
    "                # bsz, preseqlen\n",
    "            }\n",
    "            key_val_dec = past_key_values_dec[i]\n",
    "            temp[\"cross_attention_prompt\"] = {\n",
    "                \"prev_key\": key_val_dec[0].contiguous(),\n",
    "                \"prev_value\": key_val_dec[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz, seqlen)\n",
    "                    .to(key_val_dec.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            key_val_enc = past_key_values_enc[i]\n",
    "            temp[\"encoder_prompt\"] = {\n",
    "                \"prev_key\": key_val_enc[0].contiguous(),\n",
    "                \"prev_value\": key_val_enc[1].contiguous(),\n",
    "                \"prev_key_padding_mask\": torch.zeros(bsz_enc, seqlen)\n",
    "                    .to(key_val_enc.device)\n",
    "                    .bool(),\n",
    "            }\n",
    "            result.append(temp)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_description_representation(self, kwargs):\n",
    "        if self.args.model.use_description and self.args.model.map_description:\n",
    "            description_input_ids = kwargs.pop(\"description_input_ids\")\n",
    "            description_attention_mask = kwargs.pop(\"description_attention_mask\")\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                description_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                description_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=description_input_ids,\n",
    "                    attention_mask=description_attention_mask,\n",
    "                )\n",
    "                description = description_outputs.last_hidden_state[:, 0]  # TODO: the first token from the encoder.\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        else:\n",
    "            description = None\n",
    "\n",
    "        return description\n",
    "\n",
    "    def get_knowledge_representation(self, kwargs):\n",
    "        if self.args.model.knowledge_usage == 'separate':\n",
    "            knowledge_input_ids = kwargs.pop(\"knowledge_input_ids\", None)\n",
    "            knowledge_attention_mask = kwargs.pop(\"knowledge_attention_mask\", None)\n",
    "            if self.args.bert.location in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "                knowledge_outputs = self.pretrain_model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            elif self.args.bert.location in [\"facebook/bart-base\", \"facebook/bart-large\"]:\n",
    "                knowledge_outputs = self.pretrain_model.model.encoder(\n",
    "                    input_ids=knowledge_input_ids,\n",
    "                    attention_mask=knowledge_attention_mask,\n",
    "                )\n",
    "                knowledge = knowledge_outputs.last_hidden_state\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        elif self.args.model.knowledge_usage == 'concatenate':\n",
    "            knowledge = None\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return knowledge\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                labels,\n",
    "                **kwargs,\n",
    "                ):\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "\n",
    "        loss = self.pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_prompt=past_prompt,\n",
    "        ).loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids,\n",
    "                 attention_mask,\n",
    "                 **kwargs):\n",
    "\n",
    "        bsz = input_ids.shape[0]\n",
    "\n",
    "        # Encode description.\n",
    "        description_representation = self.get_description_representation(kwargs)\n",
    "\n",
    "        # Encode knowledge.\n",
    "        knowledge_representation = self.get_knowledge_representation(kwargs)\n",
    "\n",
    "        past_prompt = self.get_prompt(\n",
    "            bsz=bsz, sample_size=kwargs['num_beams'], description=description_representation, knowledge=knowledge_representation,\n",
    "        )\n",
    "        generated_ids = self.pretrain_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_prompt=past_prompt,\n",
    "            use_cache=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272a5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model]\n",
    "# name = unified.prefixtuning\n",
    "# use_description = True\n",
    "# concatenate_description = True\n",
    "# # Should be one of (separate, concatenate)\n",
    "# knowledge_usage = concatenate\n",
    "# freeze_plm = True\n",
    "# freeze_prefix = False\n",
    "\n",
    "# [dataset]\n",
    "# data_store_path = ./data\n",
    "# upsample_temp = 1\n",
    "\n",
    "# [seq2seq]\n",
    "# constructor = seq2seq_construction.meta_tuning\n",
    "# patience = 200\n",
    "\n",
    "# [arg_paths]\n",
    "# fetaqa = META_TUNING/fetaqa.cfg\n",
    "\n",
    "# [evaluate]\n",
    "# tool = metrics.meta_tuning.evaluator\n",
    "\n",
    "# [prefix_tuning]\n",
    "# prefix_sequence_length = 10\n",
    "# mid_dim = 512\n",
    "# prefix_dropout = 0.0\n",
    "\n",
    "# [special_tokens]\n",
    "# less = ' <'\n",
    "# less_or_equal = ' <='\n",
    "\n",
    "# [bert]\n",
    "# location = t5-base\n",
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b4fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "DEFAULT_CONFIGURE_DIR = \"configure\"\n",
    "DEFAULT_DATASET_DIR = \"data\"\n",
    "DEFAULT_MODEL_DIR = \"models\"\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self, contain=None):\n",
    "        self.__self__ = contain\n",
    "        self.__default__ = None\n",
    "        self.__default__ = set(dir(self))\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.__self__\n",
    "\n",
    "    def __getattribute__(self, name):\n",
    "        if name[:2] == \"__\" and name[-2:] == \"__\":\n",
    "            return super().__getattribute__(name)\n",
    "        if name not in dir(self):\n",
    "            return None\n",
    "        return super().__getattribute__(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if not (value is None) or (name[:2] == \"__\" and name[-2:] == \"__\"):\n",
    "            return super().__setattr__(name, value)\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name in dir(self) and name not in self.__default__:\n",
    "            super().__delattr__(name)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # give args elements dictionary order to ensure its replicate-ability\n",
    "        return sorted(list((arg, getattr(self, arg)) for arg in set(dir(self)) - self.__default__)).__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(set(dir(self)) - self.__default__)\n",
    "\n",
    "\n",
    "class String(object):\n",
    "    @staticmethod\n",
    "    def to_basic(string):\n",
    "        \"\"\"\n",
    "        Convert the String to what it really means.\n",
    "        For example, \"true\" --> True as a bool value\n",
    "        :param string:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(string)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(string)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if string in [\"True\", \"true\"]:\n",
    "            return True\n",
    "        elif string in [\"False\", \"false\"]:\n",
    "            return False\n",
    "        else:\n",
    "            return string.strip(\"\\\"'\")  # for those we want to add space before and after the string\n",
    "\n",
    "\n",
    "class Configure(object):\n",
    "    @staticmethod\n",
    "    def get_file_cfg(file):\n",
    "        \"\"\"\n",
    "        get configurations in file.\n",
    "        :param file:\n",
    "        :return: configure args\n",
    "        \"\"\"\n",
    "        cfgargs = Args()\n",
    "        parser = configparser.ConfigParser()\n",
    "        parser.read(file)\n",
    "        for section in parser.sections():\n",
    "            setattr(cfgargs, section, Args())\n",
    "            for item in parser.items(section):\n",
    "                setattr(getattr(cfgargs, section), item[0], String.to_basic(item[1]))\n",
    "        return cfgargs\n",
    "\n",
    "    @staticmethod\n",
    "    def refresh_args_by_file_cfg(file, prev_args):\n",
    "        args = Configure.get_file_cfg(file)\n",
    "        if args.dir is not Args:\n",
    "            args.dir = Args()\n",
    "        args.dir.model = DEFAULT_MODEL_DIR\n",
    "        args.dir.dataset = DEFAULT_DATASET_DIR\n",
    "        args.dir.configure = DEFAULT_CONFIGURE_DIR\n",
    "        for arg_name, arg in prev_args:\n",
    "            if arg is None:\n",
    "                continue\n",
    "            if arg_name != \"cfg\":\n",
    "                names = arg_name.split(\".\")\n",
    "                cur = args\n",
    "                for name in names[: -1]:\n",
    "                    if getattr(cur, name) is None:\n",
    "                        setattr(cur, name, Args())\n",
    "                    cur = getattr(cur, name)\n",
    "                if getattr(cur, names[-1]) is None:\n",
    "                    setattr(cur, names[-1], arg)\n",
    "        return args\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def Get(cfg):\n",
    "        args = Configure.get_file_cfg(os.path.join(DEFAULT_CONFIGURE_DIR, cfg))\n",
    "\n",
    "        if args.dir is not Args:\n",
    "            args.dir = Args()\n",
    "        args.dir.model = DEFAULT_MODEL_DIR\n",
    "        args.dir.dataset = DEFAULT_DATASET_DIR\n",
    "        args.dir.configure = DEFAULT_CONFIGURE_DIR\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b76281",
   "metadata": {},
   "outputs": [],
   "source": [
    "skt_args=Configure.get_file_cfg(\"./cos_e_prefix.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e358468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/huangyongfeng/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/huangyongfeng/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/huangyongfeng/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Adding  < to the vocabulary\n",
      "Adding  <= to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "model = Model(skt_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ec5fd",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e539c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012643575668334961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40579b1aca5466c98a6b73fd6cc3052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "class SequenceCollator:\n",
    "    def __init__(self, pad_token):\n",
    "        # self.pad_token_mapping = {\n",
    "        #     \"lm_labels\": -100,\n",
    "        #     \"attention_mask\": 0,\n",
    "        #     \"decoder_attention_mask\": 0,\n",
    "        #     \"input_ids\": pad_token,\n",
    "        # }\n",
    "        # self.columns = [\n",
    "        #     \"input_ids\",\n",
    "        #     \"attention_mask\",\n",
    "        #     \"lm_labels\",\n",
    "        #     \"decoder_attention_mask\",\n",
    "        # ]\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def collate_batch(self, examples):\n",
    "\n",
    "        # batch inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "dataset = datasets.load_dataset(data_args.task_name, version_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83cf059e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'choices', 'question', 'abstractive_explanation', 'answer', 'extractive_explanation'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_collector = SequenceCollator(0)\n",
    "train_ds = seq_collector.__call__(dataset['train'])\n",
    "train_ds\n",
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1dd164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018089771270751953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3399c0c635b445bcac14414473b7958b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01243734359741211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9741,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c9638564a6417e91831c7783ddd9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9741 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013007640838623047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1221,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b856a24e59f34ca5b0be0d269c936188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_splits = {'train': None, 'validation': None, 'test': None}\n",
    "original_data_splits = {'train': None, 'validation': None, 'test': None}  \n",
    "data_args.io_format=\"t5_fewshot_infilling_with_choices\"\n",
    "# Data loading from huggingface's datasets\n",
    "if data_args.task_name in {\"cos_e\", \"esnli\"}:\n",
    "    version_arg = None\n",
    "    if data_args.task_name == \"cos_e\":\n",
    "        assert data_args.version_name in {\"v1.11\", \"v1.0\"}\n",
    "        version_arg = data_args.version_name\n",
    "\n",
    "    load_train = True\n",
    "    if (not training_args.do_train\n",
    "        and not training_args.do_eval\n",
    "        and not data_args.train_predict\n",
    "    ):\n",
    "        # don't load training dataset\n",
    "        dataset = {}\n",
    "        dataset[\"train\"] = None\n",
    "        dataset[\"validation\"] = datasets.load_dataset(\n",
    "            data_args.task_name, version_arg, split=\"validation\"\n",
    "        )\n",
    "        data_splits['validation'] = dataset[\"validation\"]\n",
    "\n",
    "        if data_args.task_name == \"esnli\":\n",
    "            dataset[\"test\"] = datasets.load_dataset(data_args.task_name, split=\"test\")\n",
    "            data_splits['test'] = dataset[\"test\"]\n",
    "        load_train = False\n",
    "    else:\n",
    "        dataset = datasets.load_dataset(data_args.task_name, version_arg)\n",
    "\n",
    "        if data_args.n_shots > 0: # Shots = number of training examples **per label** \n",
    "            if data_args.task_name == 'esnli': # Construct a *balanced* random sample of the size `data_args.n_shots*len(labels)` (for train) or `data_args.fewshot_eval_size` (for eval)\n",
    "                for split in [\"train\", \"validation\", \"test\"]:\n",
    "                    split_data = dataset[split]\n",
    "                    label_subsets = []\n",
    "                    labels = split_data.features['label'].names\n",
    "                    sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size/len(labels))\n",
    "                    if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "                        assert len(labels) == 3\n",
    "                        sample_size = data_args.gpt3_max_eval_size // len(labels)\n",
    "                    for label in labels:\n",
    "                        # The following is a hack to only run on `neutral` labels of `esnli` to get data for human eval\n",
    "                        # if data_args.gpt3_max_eval_size is not None and split != 'train' and label != 'neutral':\n",
    "                        #     continue\n",
    "                        label_int = split_data.features['label'].str2int(label)\n",
    "                        label_set = split_data.filter(lambda example: example['label'] == label_int).shuffle() # all instances of labeled as `label`\n",
    "                        label_subset = label_set.select(range(sample_size)) #select `sample_size` random instances labeled as `label`\n",
    "                        label_subsets.append(label_subset)\n",
    "                    dataset[split] = datasets.concatenate_datasets(label_subsets) #merge all label-specific instances\n",
    "            elif data_args.task_name == 'cos_e': \n",
    "                for split in [\"train\", \"validation\"]: \n",
    "                    split_data = dataset[split]\n",
    "                    sample_size = data_args.n_shots if split == \"train\" else int(data_args.fewshot_eval_size) #Shots for QA are not label-specific, i.e., `n_shots` is the training data size\n",
    "                    if data_args.gpt3_max_eval_size is not None and split != 'train':\n",
    "                        sample_size = data_args.gpt3_max_eval_size\n",
    "                    dataset[split] = split_data#.shuffle().select(range(sample_size)) # select `sample_size` random instances\n",
    "            else: \n",
    "                raise ValueError('Only cos_e and esnli are supported by Huggingface datasets.')\n",
    "    # Apply method, and format dataset to torch.Tensor outputs\n",
    "    for split in dataset.keys():\n",
    "        if dataset[split] is not None:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                lambda x: format_instance(\n",
    "                    x,\n",
    "                    tokenizer,\n",
    "                    data_args.explanation_sep,\n",
    "                    datasource=data_args.task_name,\n",
    "                    io_format=data_args.io_format\n",
    "                ),\n",
    "                batched=False,\n",
    "                load_from_cache_file=False,\n",
    "            )\n",
    "    data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "    data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "    if data_args.task_name == \"esnli\":\n",
    "        data_splits[\"test\"] = deepcopy(dataset[\"test\"])\n",
    "\n",
    "    original_data_splits[\"train\"] = deepcopy(dataset[\"train\"])\n",
    "    original_data_splits[\"validation\"] = deepcopy(dataset[\"validation\"])\n",
    "    if data_args.task_name == \"esnli\":\n",
    "        original_data_splits[\"test\"] = deepcopy(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a331e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCollator:\n",
    "    def __init__(self, model, pad_token):\n",
    "        self.model = model\n",
    "        self.pad_token_mapping = {\n",
    "            \"labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, InputDataClass]]) -> Dict[str, torch.Tensor]:\n",
    "        # re-format inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ba5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "The following columns in the training set  don't have a corresponding argument in `Model.forward` and have been ignored: id, choices, question, question_encoding, abstractive_explanation, answer, extractive_explanation, decoder_attention_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 9741\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3654\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcuhk_lavilab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./cos_e_output</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/cuhk_lavilab/huggingface\" target=\"_blank\">https://wandb.ai/cuhk_lavilab/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/cuhk_lavilab/huggingface/runs/3qq4zlqe\" target=\"_blank\">https://wandb.ai/cuhk_lavilab/huggingface/runs/3qq4zlqe</a><br/>\n",
       "                Run data is saved locally in <code>/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/scripts/wandb/run-20220825_174030-3qq4zlqe</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3654' max='3654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3654/3654 04:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./cos_e_output/082522_173946/checkpoint-3500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
    "if data_args.generations_filepath is None:\n",
    "    callbacks = [TensorBoardCallback()]\n",
    "    if data_args.early_stopping_patience > 0:\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=data_args.early_stopping_patience))\n",
    "        training_args.load_best_model_at_end = True\n",
    "    else:\n",
    "        training_args.load_best_model_at_end = False  # use the last model state\n",
    "    training_args.metric_for_best_model = 'eval_loss'\n",
    "    training_args.greater_is_better = False\n",
    "    if training_args.eval_steps is None:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "    else:\n",
    "        training_args.evaluation_strategy = EvaluationStrategy.STEPS\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_splits['train'],\n",
    "        eval_dataset=data_splits['validation'],\n",
    "        data_collator=SequenceCollator(\n",
    "            model=model_class, pad_token=tokenizer.pad_token_id\n",
    "        ),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "# Training. Don't train if it is use_gpt3\n",
    "if training_args.do_train and not model_args.use_gpt3:\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "    model = trainer.model\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    train_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dfc4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcuhk_lavilab\u001b[0m (use `wandb login --relogin` to force relogin)\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6dee07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK 算是run起来了 剩下的问题就是要去测试出rationale的效果了的 这个明天上午\n",
    "#融合一下两边的模型，开始评测出rationale的结果  明天把这个事情搞定 然后可以产出好结果的话 那这篇文章就基本有0-1的结果的了\n",
    "下步就是把llm的知识拿过来了的 再刷刷效果的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4717787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98503d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508f96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83d740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
