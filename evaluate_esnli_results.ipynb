{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import datasets \n",
    "import json\n",
    "# from feature_conversion_methods import unified_qa_esnli_label_mapping, wt5_esnli_label_mapping, unified_qa_sbic_label_mapping\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    save_path,\n",
    "    dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    split,\n",
    "    task,\n",
    "    device,\n",
    "    explanation_sep,\n",
    "    rationale_only=False,\n",
    "    label_only=False,\n",
    "    generations_file=None,\n",
    "    io_format=None\n",
    "):\n",
    "    fname = os.path.join(save_path, \"%s_generations.txt\" % split)\n",
    "    if os.path.isfile(fname):\n",
    "        fname = fname.split(\".txt\")[0] + \"_1.txt\"\n",
    "\n",
    "    if generations_file is not None: # actual words have already been decoded and saved in `generations_file`\n",
    "        with open(generations_file, \"r\") as f: \n",
    "            lines = f.readlines()\n",
    "        generations_list = [l.replace(\"\\n\", \" \").replace(tokenizer.eos_token, \" \").strip()for l in lines] # strip newlines & EOS token (if exists)\n",
    "        pdb.set_trace()\n",
    "    else: # decode output words\n",
    "        generations_list = []\n",
    "        with open(fname, \"w\") as w:\n",
    "            for i, element in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "                inpt_tensor = torch.tensor(element[\"input_ids\"], device=device).reshape(1, -1)\n",
    "                \n",
    "                out = model.generate(\n",
    "                    inpt_tensor,\n",
    "                    max_length=100,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                skip_special_tokens = False if \"infilling\" in io_format else True\n",
    "                words = tokenizer.decode(out[0].tolist(), skip_special_tokens=skip_special_tokens)\n",
    "                if \"infilling\" in io_format:\n",
    "                    words = words.replace(\"<extra_id_1>\", f\" {explanation_sep}\")\n",
    "                    words = words.replace(tokenizer.pad_token,'')\n",
    "                    words = words.replace(\"<extra_id_0>\", '')\n",
    "                    words = words.replace(\"<extra_id_2>\", '')\n",
    "                    words = ' '.join(words.split())\n",
    "                words = (words.replace(\"\\n\", \" \").replace(tokenizer.eos_token, \" \").strip())\n",
    "                w.write(words + \"\\n\")\n",
    "                generations_list.append(words)\n",
    "\n",
    "    broken_count = 0\n",
    "\n",
    "    # for Bertscore\n",
    "    bertscore = None \n",
    "    expl_true = []\n",
    "    expl_pred = []\n",
    "\n",
    "    # for accuracy \n",
    "    accuracy = None \n",
    "    label_mapping = {\n",
    "        \"sbic\": {\n",
    "            \"not_offensive\": 0,\n",
    "            \"offensive\": 1\n",
    "        }\n",
    "    }\n",
    "    label_true = []\n",
    "    label_pred = []\n",
    "    acc = []\n",
    "\n",
    "    # to separate label from explanation\n",
    "    if explanation_sep in ['explanation', 'explanation_why']:\n",
    "        explanation_sep = explanation_sep + ':' # when these separators are used we format the input as: `answer explanation: explanation`\n",
    "\n",
    "    analysis_file = os.path.join(save_path, \"%s_posthoc_analysis.txt\" % split)\n",
    "    if os.path.isfile(analysis_file):\n",
    "        analysis_file = analysis_file.split(\".txt\")[0] + \"_1.txt\"\n",
    "\n",
    "    with open(analysis_file, \"w\") as g:\n",
    "        for _, (line, gold) in tqdm(enumerate(zip(generations_list, dataset)), total=len(dataset)):\n",
    "            broken_generation = False\n",
    "\n",
    "            if rationale_only:\n",
    "                pred_l = None \n",
    "                pred_e = line.strip()\n",
    "            elif label_only:\n",
    "                pred_l = line.strip()\n",
    "                pred_e = None\n",
    "            else: \n",
    "                line_split = line.split(explanation_sep)\n",
    "                if len(line_split) > 1:\n",
    "                    pred_l = line_split[0].strip()\n",
    "                    pred_e = line_split[1].strip()\n",
    "                    g.write(f\"Predicted: {pred_l} | {pred_e}\\n\")\n",
    "                else: \n",
    "                    print(f\"This line couldn't be processed (most likely due to format issue): {line}\")\n",
    "                    pred_l = line.strip()\n",
    "                    pred_e = \"UNK\"\n",
    "                    broken_count += 1\n",
    "                    broken_generation = True\n",
    "                    g.write(f\"Predicted: {pred_l}\\n\")\n",
    "\n",
    "            if task in [\"cos_e\", \"ecqa\"]:\n",
    "                gold_l = gold[\"answer\"]\n",
    "                gold_explanations_string = gold[\"abstractive_explanation\"] if task == \"cos_e\" else gold[\"explanation\"]\n",
    "                gold_explanations = [gold_explanations_string]\n",
    "                g.write(gold[\"question\"] + \"\\n\")\n",
    "                g.write(f\"Correct: {gold_l} | {gold_explanations_string}\\n\")\n",
    "\n",
    "            elif task == \"esnli\":\n",
    "                gold_l = unified_qa_esnli_label_mapping[gold[\"label\"]] if ('unified' in io_format and io_format not in ['unifiedqa_snli_mix_what_with_choices',\n",
    "                                                                                                                        'unifiedqa_snli_mix_what_v2',\n",
    "                                                                                                                        'unifiedqa_snli_mix_what_with_choices_v2',\n",
    "                                                                                                                        'unifiedqa_what_v2']) or \\\n",
    "                                                                        io_format in ['squad', 'squad_nli_mix'] else wt5_esnli_label_mapping[gold[\"label\"]]\n",
    "                gold_explanations = [gold[f\"explanation_{k}\"] for k in [1,2,3]] # there can be up to 3 human gold-explanations in E-SNLI. Only first 2 gold explanations are use to compute BLEU in prior works. We follow suit.\n",
    "                gold_explanations_string = ' [SEP] '.join(gold_explanations)\n",
    "\n",
    "                g.write(gold[\"premise\"] + \" \" + gold[\"hypothesis\"] + \"\\n\")\n",
    "                g.write(f\"Correct: {gold_l} | {gold_explanations_string} \\n\")\n",
    "\n",
    "            elif task == \"sbic\":\n",
    "                if ('unified' in io_format and 'what' not in io_format and 'unifew' not in io_format) or io_format == \"squad_yn\":\n",
    "                    gold_l = unified_qa_sbic_label_mapping[gold[\"offensiveYN\"]]\n",
    "                elif io_format == 't5_fewshot_infilling_bool':\n",
    "                    gold_l = 'True' if gold[\"offensiveYN\"] == 'offensive' else 'False'\n",
    "                else:\n",
    "                    gold_l = gold[\"offensiveYN\"].replace(\"not offensive\", \"not_offensive\")\n",
    "                gold_explanations_string = gold[\"targetStereotype\"]\n",
    "                post = gold['post'].replace('\\n', ' ')\n",
    "                g.write(f\"{post}\\n\")\n",
    "                if pd.isna(gold_explanations_string) or pd.isna(gold_l):\n",
    "                    raise ValueError('Gold label or explanation empty...')\n",
    "                else:\n",
    "                    g.write(\"Correct: \" + gold_l + \" | \" + gold_explanations_string + \"\\n\")\n",
    "                    gold_explanations = gold_explanations_string.split(' [SEP] ')\n",
    "\n",
    "            elif task == \"sensemaking\":\n",
    "                if '_yn' in io_format:\n",
    "                    gold_l = \"yes\" if bool(int(gold['label'])) else \"no\"\n",
    "                elif io_format in ['copa_bool', 't5_fewshot_infilling_bool']: \n",
    "                    gold_l = str(bool(int(gold['label'])))\n",
    "                else:\n",
    "                    gold_l_idx = str(int(gold[\"label\"])+1)\n",
    "                    gold_l = f\"choice{gold_l_idx}\"\n",
    "                gold_explanations_string = gold[\"explanation\"]\n",
    "                gold_explanations = gold_explanations_string.split('[SEP]')\n",
    "\n",
    "                g.write(f\"choice1: {gold['sent0']} choice2: {gold['sent1']}\\n\")\n",
    "                g.write(f\"Correct: {gold_l} | {gold_explanations_string} \\n\")\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Unknown task. Currently supported: esnli, cos_e, sbic, sensemaking, ecqa.\")\n",
    "\n",
    "            if not label_only:\n",
    "                # for Bertscore \n",
    "                expl_pred.append(pred_e)\n",
    "                expl_true.append(gold_explanations)\n",
    "\n",
    "            if not rationale_only:\n",
    "                # for accuracy\n",
    "                if task == \"sbic\":\n",
    "                    gold_key = \"offensive\" if gold_l.lower() in [\"offensive\", \"yes\"] else \"not_offensive\"\n",
    "                    if not broken_generation:\n",
    "                        pred_key = \"offensive\" if pred_l.lower() in [\"offensive\", \"yes\"] else \"not_offensive\"\n",
    "                        met = gold_key.lower() == pred_key.lower()\n",
    "                        if task == \"sbic\" and pred_key in label_mapping[task].keys():\n",
    "                            label_pred.append(label_mapping[task][pred_key])\n",
    "                            label_true.append(label_mapping[task][gold_key])\n",
    "                        else:\n",
    "                            print(f\"Broken label: {pred_l}\")\n",
    "                            met = False\n",
    "                            label_pred.append(len(label_mapping[task]))\n",
    "                            label_true.append(label_mapping[task][gold_key])\n",
    "                    else:\n",
    "                        met = False\n",
    "                        label_pred.append(len(label_mapping[task]))\n",
    "                        label_true.append(label_mapping[task][gold_key])\n",
    "                else:\n",
    "                    met = gold_l.lower() == pred_l.lower()\n",
    "                acc.append(met)\n",
    "                g.write(\"Considered Correct: \" + str(met) + \"\\n\")\n",
    "                g.write(\"\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if not rationale_only:\n",
    "        # final accuracy \n",
    "        accuracy = sum(acc) / len(acc) * 100\n",
    "\n",
    "    if not label_only:\n",
    "        # BERTscore\n",
    "        bertscore_metric = datasets.load_metric(\"bertscore\")\n",
    "        bertscores = []\n",
    "        for pred_expl, list_gold_expl in zip(expl_pred, expl_true):\n",
    "            instance_bertscores = []\n",
    "            for gold_expl in list_gold_expl: \n",
    "                score = bertscore_metric.compute(predictions=[pred_expl.lower()], references=[gold_expl.lower()], lang=\"en\")[\"f1\"][0]*100\n",
    "                instance_bertscores.append(score)\n",
    "            bertscores.append(max(instance_bertscores))\n",
    "\n",
    "        bertscore = np.mean(bertscores)\n",
    "        bertscores_correct_prediction = [score for correct_yn, score in zip(acc, bertscores) if correct_yn]\n",
    "        bertscore_correct_prediction = np.mean(bertscores_correct_prediction)\n",
    "\n",
    "        bertscores_correct_normalized = [score if correct_yn else 0.0 for correct_yn, score in zip(acc, bertscores)]\n",
    "        bertscore_correct_normalized = np.mean(bertscores_correct_normalized)\n",
    "\n",
    "    if split == 'validation':\n",
    "        split = 'dev'\n",
    "    (\n",
    "     results[f\"{split}_broken_count\"],\n",
    "     results[f\"{split}_acc\"],\n",
    "     results[f\"{split}_bertscore\"],\n",
    "     results[f\"{split}_bertscore_correct_pred\"],\n",
    "     results[f\"{split}_bertscore_correct_normalized\"]\n",
    "    ) = (broken_count, accuracy, bertscore, bertscore_correct_prediction, bertscore_correct_normalized)\n",
    "\n",
    "    with open(os.path.join(save_path, f\"results_{split}.json\"), \"w\") as fp:\n",
    "        json.dump(results, fp)\n",
    "\n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835d13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
