{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f156ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A (hopefully) Simple API for serving explanation score requests.\n",
    "\n",
    "input_string = (\n",
    "    f\"{question} answer: {gold_label}. \"\n",
    "    + f\" explanation: {abstr_expl}.\"\n",
    ")\n",
    "\n",
    "here are some example input strings:\n",
    "\n",
    "If you feel like everything is spinning while climbing you are experiencing what? answer: vertigo. explanation: Vertigo is often experienced while climbing or at heights.\n",
    "Where do you get clothes in a shopping bag? answer: retail store. explanation: For any large item where convenience is beneficial, one might go to a retail store, either a regular one or a big-box store like walmart.\n",
    "Where should a cat be in a house? answer: floor. explanation: A cat should be on the floor, not on a rug.\n",
    "'''\n",
    "import pdb\n",
    "import argparse\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "_model, _tokenizer = None, None\n",
    "\n",
    "model2url = {\n",
    "    'large': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/valloss%3D0.28665~model%3Dt5-large~lr%3D0.0001~seed%3D1~labelagg%3D0_just_weights.pt',\n",
    "    '3b': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/valloss%3D0.28925~model%3Dt5-3b~lr%3D0.0001~seed%3D1~labelagg%3D0_just_weights.pt',\n",
    "    '11b': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/cose_deepspeed_valloss%3D0.00000~model%3Dt5-11b~lr%3D0.00001~seed%3D1~labelagg%3D0.pt',\n",
    "}\n",
    "\n",
    "def get_model(model_type, device=None):\n",
    "    global _model, model2url\n",
    "    if model_type not in {'11b', '3b', 'large'}:\n",
    "        raise NotImplementedError('{} is not a valid model please use \"3b\" or \"large\"'.format(model_type))\n",
    "\n",
    "    if _model is None:\n",
    "        hf_model_name = 't5-' + model_type\n",
    "        print('Loading model: this will run only once.')\n",
    "\n",
    "        if model_type == 'large':\n",
    "            model_path = 'csqa_models/t5-large.pt'\n",
    "        elif model_type == '3b':\n",
    "            model_path = 'csqa_models/valloss=0.28925~model=t5-3b~lr=0.0001~seed=1~labelagg=0_just_weights.pt'\n",
    "        elif model_type == '11b':\n",
    "            model_path = 'csqa_models/cose_deepspeed_valloss=0.00000~model=t5-11b~lr=0.00001~seed=1~labelagg=0.pt'\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please download weights for {} model and put in current directory.'.format(model_path))\n",
    "            print('for example, wget {}'.format(model2url[model_type]))\n",
    "            quit()\n",
    "\n",
    "        state = torch.load(model_path)\n",
    "        if 'model_state_dict' in state:\n",
    "            state = state['model_state_dict']\n",
    "\n",
    "        _model = transformers.AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n",
    "        if model_type == '11b': # need to resize due to deepspeed, these entires are not accessed.\n",
    "            _model.resize_token_embeddings(len(transformers.AutoTokenizer.from_pretrained(hf_model_name)))\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "        if device is not None:\n",
    "            _model = _model.to(device)\n",
    "\n",
    "    return _model\n",
    "\n",
    "\n",
    "def get_tokenizer(model_type):\n",
    "    global _tokenizer\n",
    "    if model_type not in {'3b', 'large', '11b'}:\n",
    "        raise NotImplementedError('{} is not a valid model please use \"3b\" or \"large\" or \"11b\"'.format(model_type))\n",
    "\n",
    "    if _tokenizer is None:\n",
    "        hf_model_name = 't5-' + model_type\n",
    "        _tokenizer = transformers.T5TokenizerFast.from_pretrained(hf_model_name)\n",
    "\n",
    "    return _tokenizer\n",
    "\n",
    "\n",
    "class T5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        res = self.tokenizer(self.data[idx]['input'], truncation=True)\n",
    "        res['labels'] = self.tokenizer(self.data[idx]['label']).input_ids\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def get_scores(inputs, model_type, device=None, batch_size=32, verbose=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "      - a list of explanations to score, e.g.,:\n",
    "        premise: A man getting a tattoo on his back. hypothesis: A woman is getting a tattoo. answer: contradiction. explanation: Because the tattoo artist is a man, the person getting the tattoo is not a woman.\n",
    "      - model type, either \"3b\" or \"large\" or \"11b\"\n",
    "      - device: which torch device to load model on, e.g., \"cuda:3\"\n",
    "    Outputs:\n",
    "      - P(good explanation); higher is better\n",
    "    '''\n",
    "    assert model_type in {'large', '3b', '11b'}\n",
    "\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    model = get_model(model_type, device=device)\n",
    "    tokenizer = get_tokenizer(model_type)\n",
    "\n",
    "    score_itr = T5Dataset([{'input': inp, 'label': 'x'} for inp in inputs], tokenizer) # dummy labels for inference\n",
    "    data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "#         return_tensors='pt'\n",
    "    )\n",
    "    score_itr = torch.utils.data.DataLoader(score_itr, shuffle=False, collate_fn=data_collator, batch_size=batch_size)\n",
    "    score_itr = score_itr if not verbose else tqdm.tqdm(score_itr, total=len(score_itr))\n",
    "\n",
    "    good_idx, bad_idx = tokenizer('good').input_ids[0], tokenizer('bad').input_ids[0]\n",
    "    scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in score_itr:\n",
    "            if device is not None:\n",
    "                input_ids, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            model_output = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            logits_pos = model_output['logits'][:, 0, good_idx].cpu().numpy()\n",
    "            logits_neg = model_output['logits'][:, 0, bad_idx].cpu().numpy()\n",
    "            exp_logit_pos, exp_logit_neg = np.exp(logits_pos), np.exp(logits_neg)\n",
    "            score = list([float(x) for x in exp_logit_pos / (exp_logit_pos + exp_logit_neg)])\n",
    "            #pdb.set_trace()\n",
    "            scores.extend(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     '''\n",
    "#     Optional args for main function, mostly just to test.\n",
    "#     '''\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         'model_type',\n",
    "#         default='large',\n",
    "#         choices={'large', '3b', '11b'})\n",
    "#     parser.add_argument(\n",
    "#         '--batch_size',\n",
    "#         default=32,\n",
    "#         type=int)\n",
    "\n",
    "#     args = parser.parse_args(['--batch_size', '1'])\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d73868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     'model_type',\n",
    "#     default='large',\n",
    "#     choices={'large', '3b', '11b'})\n",
    "# parser.add_argument(\n",
    "#     '--batch_size',\n",
    "#     default=32,\n",
    "#     type=int)\n",
    "\n",
    "# args = parser.parse_args([\"--model_type\", \"3b\"])\n",
    "# args.device = 'cpu'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(1)\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# scores = get_scores(\n",
    "#     ['If you feel like everything is spinning while climbing you are experiencing what? answer: vertigo. explanation: Vertigo is often experienced while climbing or at heights.',\n",
    "#      'Where do you get clothes in a shopping bag? answer: retail store. explanation: For any large item where convenience is beneficial, one might go to a retail store, either a regular one or a big-box store like walmart.',\n",
    "#      'Where should a cat be in a house? answer: floor. explanation: A cat should be on the floor, not on a rug.'],\n",
    "#     'large',\n",
    "#     device='cuda:0',\n",
    "#     batch_size=1,\n",
    "#     verbose=False)\n",
    "# print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56e950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "# with open(\"../../scripts/results/dev_rationale_pair.json\") as f:\n",
    "#     rationale_pair_dev_data = json.load(f)\n",
    "import json\n",
    "file_path = \"../../scripts/results/24shots_cose_t5_3b_chatgpt_rationales_generator_test_rationale_pair.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    rationale_pair_dev_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31588f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation', 'our_explanation', 'input_ids', 'attention_mask', 'labels', 'decoder_attention_mask', 'question_encoding', 'common_expl_list', 'generated_explanation']),\n",
       " 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationale_pair_dev_data[0].keys(), len(rationale_pair_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257998ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book store book\n",
      "bringing legal action results in arguments and aggravation.\n",
      "boredom - wikipedia\n",
      "knowledge is a familiarity,\n",
      "canada is located to the north of usa\n",
      "sam spent most of his time standing up. his job was supermarket never got any rest. but he was the best cashier at his workplace. where might he work\n",
      "rivers flow trough valleys.\n",
      "one-day leave application samples -\n",
      "if a referee isn't sure of something he will be hired for he can still be sure of this.\n",
      "when something bounces of a wall it comes back towards the person that threw it.\n",
      "health complications\n",
      "tests have equations.\n",
      "movers - local & long distance moving services | moving.com\n",
      "rivers flow trough valleys.\n",
      "the first world war saw large-scale chemical warfare.\n",
      "disease often spread but shouldn't be hospital.\n",
      "trust company - drop to zero - youtube\n",
      "a food movie times\n",
      "files are kept in a drawer\n",
      "i have money\n",
      "gate house is in a subdivision.\n",
      "this word was most relevant.\n",
      "rivers flow trough valleys.\n",
      "this word is more relevant\n",
      "b.loose® boutique - boutique - foursquare\n",
      "flow trough valleys.\n",
      "no other option applied and a confident person trusts himself\n",
      "he was upset because he wasn't able to analyze effectively\n",
      "humor makes people happy\n",
      "food, money, etc. than you need\n",
      "most people keep a book on a bedside table before bed.\n",
      "nowadays knights are only found in chess\n",
      "answer is the only explanation matching\n",
      "humans show empathy to other humans.\n",
      "most of us probably\n",
      "offices typically have many people working at desk jobs.\n",
      "prior knowledge is the memory\n",
      "sweat have more calorise\n",
      "it is the only western state that animal is found.\n",
      "this option is better to this task.\n",
      "people make friends with people that are similar to themselves.\n",
      "bowl of fruit pictures\n",
      "the nose dipped and alarms went off inside the aircraft.\n",
      "dick's sporting goods hours\n",
      "the only reason to reduce the town out of hate is to destroy an enemy.\n",
      "baby normal can't eat clean.\n",
      "hoped that the winning baseball game would finally get recognition\n",
      "napping takes away tiredness.\n",
      "maximum head concentration\n",
      "you go to a hospital if you are ill\n",
      "heart rate increase staves off the desire to sleep when tired\n",
      "eating is good\n",
      "forcing law is an umbrella statement for multiple activities that fall under the responsibility of the police\n",
      "flowers often have very colorful decor\n",
      "zoo was there never any mammoth?\n",
      "fun synonyms, fun antonyms\n",
      "the truth can hurt more than not knowing\n",
      "jogging is working the legs.\n",
      "when changing your location\n",
      "lunch is the mid-day meal.\n",
      "rivers flow trough valleys.\n",
      "if someone is mindlessly doing something, they are probably thinking.\n",
      "this word is most relavant\n",
      "an office is a common work place.\n",
      "this word is most relavant\n",
      "a race is a competition\n",
      "you should reach for something to achieve a goal\n",
      "you choke if you eat too fast\n",
      "an auditorium is the only location with a stage and a bit room, folding chairs also go with this.\n",
      "no matter the situation one has to be dressed to go to work.\n",
      "work and energy.\n",
      "punishment | meaning of punishment in longman dictionar\n",
      "work (rihanna song) - wikipedia\n",
      "contempt of congress - wikipedia\n",
      "restaurants usually have many tables\n",
      "this word is most relavant\n",
      "city - wikipedia\n",
      "some people is a brown colour\n",
      "stand still cannot do in one thing\n",
      "max b - wikipedia\n",
      "hot crypto airdrops\n",
      "how you confirm information.\n",
      "it is often used as bait\n",
      "desks office desks desk table tops\n",
      "the question portrays bald spots as a negative consequence and the only option from the choices that would have a negative outcome is \"mistakes\". the other options would not lead to a negative outcome, or in this case, \"bald spots\".\n",
      "kew gardens is a\n",
      "inspiration after studying many self-help books\n",
      "in pharmacy sells teeth whiteners.\n",
      "australian outback door\n",
      "wagons are a classic toy\n",
      "psychologists generally define forgiveness as a conscious\n",
      "this word was most relevant.\n",
      "witches are fictional.\n",
      "spending money wisely: 7 ways to save more & spend less\n",
      "rivers flow trough valleys.\n",
      "gritting your teeth\n",
      "french restaurants are always in europe\n",
      "saucepan has a handle and a vessel\n",
      "jar files in java\n",
      "coimbatore located near coimbator\n",
      "airplanes are often late due to mechanical failures and weather.\n",
      "glue well used at artroom\n",
      "museums are a place to appreciate the work of others\n",
      "walk along this lonely street\n",
      "street | definition of street in english by oxford dictionaries\n",
      "chef does that.\n",
      "this word is most relavant\n",
      "vocal/easy listening, pop\n",
      "poland is the furthest south of these countries.\n",
      "to win a game be competitiveness\n",
      "sandy beach - amazon.com\n",
      "extending human control\n",
      "heat from the sun melts ice\n",
      "people need to rest after doing a lot of activities.\n",
      "health complications\n",
      "d shopping malls finance plc - dizz group\n",
      "officially known since\n",
      "getting fat having food too much\n",
      "family reunion is the the only annual event on the list.\n",
      "expressing in public may have fear\n",
      "eating and not exercising makes you gain weight\n",
      "a disagreement, or the process of disagreeing\n",
      "comic books have weird things\n",
      "rivers flow trough valleys.\n",
      "talking to someone is pointless if they're doing not listening\n",
      "a beach is next to the ocean.\n",
      "actor do while acting pretend\n",
      "office is a ring binder likely to contain budgets?\n",
      "trains are boarded in a railway station\n",
      "i need to go the circus to watch a really big toy ballon floating\n",
      "these are found on such garments\n",
      "phoenix is located in the desert.\n",
      "mansions contains larger closets\n",
      "to indulge (a desire, craving, or habit) to excess: overindulging a fondness for cake. 2. to indulge (a person) excessively: overindulges his children\n",
      "generally people go to bed for sleep.\n",
      "montana official tourism travel vacation info site\n",
      "war causes death\n",
      "people use their hands to write\n",
      "the phone book is a way to look people up by their name and find an address and phone number to contact them.\n",
      "communicate synonyms, communicate\n",
      "respiration: types, atp, human respiratory system,\n",
      "i doing anything in bathing\n",
      "the saw was not necessary for car repair, tool kit.\n",
      "piggy bank account\n",
      "school bus stairwell garden\n",
      "friendship requires socializing\n",
      "students do everything\n",
      "going to a party is mostly to have drugs\n",
      "tired poeple takes more rest.\n",
      "you have friends in your hometown.\n",
      "rivers flow trough valleys.\n",
      "peel | definition of peel in english by oxford dictionaries\n",
      "backpack where could you put some knives if you are planning to bring them outside with you?\n",
      "theaters have others performing on stage.\n",
      "if i am creating art and have completed it to satisfaction, what is my enlightenment\n",
      "the most serious and pragmatic reason to drive is for transportation.\n",
      "opposition between work\n",
      "this is what can happen that does not hurt to a person.\n",
      "auditorium have speakers\n",
      "the injury had changed how she looked\n",
      "people are buried in graveyard after death\n",
      "control tower. wikipedia\n",
      "child's hand pictures | download free images on unsplash\n",
      "weddings legally start a family bond.\n",
      "he was excited to know daily news.\n",
      "feel pain when you get old\n",
      "best and worst businesses\n",
      "definition of markets | what is markets ? markets meaning - the ...\n",
      "backpack carry food while on a long walk in the countryside\n",
      "smoke driver do that could distract him or her while he or she is in a driving car\n",
      "the fat man wanted to make a change in his life and make sure it stayed permanent\n",
      "colonial kingdoms referres to europe\n",
      "a cabinet is able to conceal many things.\n",
      "rivers flow trough valleys.\n",
      "tide pools can trap sea-life\n",
      "the only option that can be smelted is metal.\n",
      "tools go into tool boxes\n",
      "an electric current is the rate\n",
      "old goods have been around for a while, making them related to the past\n",
      "a small building in which\n",
      "a person is a man\n",
      "physical fitness state health well\n",
      "picture books picture books for children\n",
      "opera browser open\n",
      "since the comforter was used and treaded upon, it is dirty and needs a wash. there is no indication that it is no longer usable.\n",
      "living room vs family room\n",
      "this word is most relavant\n",
      "orchestra is a large instrumental group typical of classical.\n",
      "hockey rink structure can you see people you use a puck professionally\n",
      "this option is better to this task.\n",
      "garlic bread is a food\n",
      "a planetarium has a painted ceiling to mimic the sky.\n",
      "this already says that the body of water is not the ocean\n",
      "person's house stay\n",
      "guitars are a string instrument\n",
      "this word is more relevent\n",
      "you get sickness after drinking too much alcohol\n",
      "problem | meaning in the cambridge\n",
      "insubstantial | definition of insubstantial\n",
      "michigan is shaped like a glove\n",
      "this word was most relevant.\n"
     ]
    }
   ],
   "source": [
    "for da in rationale_pair_dev_data:\n",
    "    print(da['abstractive_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166a2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                               | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: this will run only once.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [01:27<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "qae_list = []\n",
    "score_list = []\n",
    "for da in tqdm(rationale_pair_dev_data, total=len(rationale_pair_dev_data)):\n",
    "    qae = \"{} answer: {} explanation: {}\".format(da['question'], \n",
    "                                                 da['answer'], \n",
    "                                                 da['abstractive_explanation'])\n",
    "\n",
    "    scores = get_scores(\n",
    "        [qae],\n",
    "        '3b',\n",
    "        device='cuda:0',\n",
    "        batch_size=1,\n",
    "        verbose=False)\n",
    "    score_list.append(scores[0])\n",
    "#     if scores[0] > 0.7 or scores[0] < 0.2:\n",
    "#         print(\"question: {}\".format(da['question']))\n",
    "#         print(\"answer: {}\".format(da['answer']))\n",
    "#         print(\"common_expl_list: {}\".format(da['common_expl_list']))\n",
    "#         print(\"generated_explanation: {}\".format(da['generated_explanation']))\n",
    "#         print(\"score: {}\".format(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba055119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3945420480875382, 0.3770222067832947)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_list), np.median(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb020b",
   "metadata": {},
   "source": [
    "## evaluate generated rationale with bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556f234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b285d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "bertscore_metric = datasets.load_metric(\"bertscore\")\n",
    "rouge_metric = datasets.load_metric('rouge')\n",
    "bleu_metric = datasets.load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7708d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [00:51<00:00,  3.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "bert_scores = []\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for da in tqdm(rationale_pair_dev_data, total=len(rationale_pair_dev_data)):\n",
    "    generated_expl = da['generated_explanation']\n",
    "    common_expl_list = da['common_expl_list']\n",
    "    pred_expl = generated_expl.split(\"<extra_id_0> \")[1].split(\"<extra_id_1>\")[0]\n",
    "    list_gold_expl = [l.lower() for l in common_expl_list]\n",
    "    \n",
    "    bert_score = bertscore_metric.compute(predictions=[pred_expl.lower()], references=[list_gold_expl], lang=\"en\")[\"f1\"][0]*100\n",
    "    bleu_score = bleu_metric.compute(predictions=[pred_expl.lower()], references=[list_gold_expl])['score']\n",
    "    rouge_score = rouge_metric.compute(predictions=[pred_expl.lower()]*len(list_gold_expl), references=list_gold_expl)\n",
    "    rouge1_score = rouge_score[\"rouge1\"].mid.fmeasure\n",
    "    rouge2_score = rouge_score[\"rouge2\"].mid.fmeasure\n",
    "    rougeL_score = rouge_score[\"rougeL\"].mid.fmeasure\n",
    "    bert_scores.append(bert_score)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    rouge1_scores.append(rouge1_score)\n",
    "    rouge2_scores.append(rouge2_score)\n",
    "    rougeL_scores.append(rougeL_score)\n",
    "    \n",
    "    \n",
    "#     #print(generated_expl)\n",
    "#     #print(generated_expl.split(\"<extra_id_0> \")[1].split(\"<extra_id_1>\")[0])\n",
    "#     instance_bertscores = []\n",
    "#     for gold_expl in list_gold_expl: \n",
    "#         score = bertscore_metric.compute(predictions=[pred_expl.lower()]*len(), references=[gold_expl.lower()], lang=\"en\")[\"f1\"][0]*100\n",
    "#         instance_bertscores.append(score)\n",
    "#     bertscores.append(np.mean(instance_bertscores))\n",
    "    \n",
    "#     bleuscore = bleu_score(pred_expl, list_gold_expl)\n",
    "#     bleuscores.append(bleuscore)\n",
    "    \n",
    "#     rougescore = rouge(pred_expl, list_gold_expl)\n",
    "#     rouge1_scores.append(rougescore['rouge1_fmeasure'].numpy()[0])\n",
    "#     rouge2_scores.append(rougescore['rouge2_fmeasure'].numpy()[0])\n",
    "#     rougeL_scores.append(rougescore['rougeL_fmeasure'].numpy()[0])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77a5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score: 85.90453480606648\n",
      "bleu_score: 3.7937382054342197\n",
      "rouge1_score: 0.15662371382468268\n",
      "rouge2_score: 0.05230511736495688\n",
      "rougeL_score: 0.12240036761465746\n"
     ]
    }
   ],
   "source": [
    "print(\"bert_score: {}\".format(np.mean(bert_scores)))\n",
    "print(\"bleu_score: {}\".format(np.mean(bleu_scores)))\n",
    "print(\"rouge1_score: {}\".format(np.mean(rouge1_scores)))\n",
    "print(\"rouge2_score: {}\".format(np.mean(rouge2_scores)))\n",
    "print(\"rougeL_score: {}\".format(np.mean(rougeL_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d9896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.0748663101604278, recall=0.6086956521739131, fmeasure=0.13333333333333333), mid=Score(precision=0.0748663101604278, recall=0.7710144927536232, fmeasure=0.13597359735973596), high=Score(precision=0.0748663101604278, recall=0.9333333333333333, fmeasure=0.1386138613861386)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.026881720430107527, recall=0.22727272727272727, fmeasure=0.04807692307692307), mid=Score(precision=0.02956989247311828, recall=0.3279220779220779, fmeasure=0.054038461538461535), high=Score(precision=0.03225806451612903, recall=0.42857142857142855, fmeasure=0.06)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.058823529411764705, recall=0.4782608695652174, fmeasure=0.10476190476190476), mid=Score(precision=0.06417112299465241, recall=0.672463768115942, fmeasure=0.11673738802451672), high=Score(precision=0.06951871657754011, recall=0.8666666666666667, fmeasure=0.1287128712871287)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.058823529411764705, recall=0.4782608695652174, fmeasure=0.10476190476190476), mid=Score(precision=0.06417112299465241, recall=0.672463768115942, fmeasure=0.11673738802451672), high=Score(precision=0.06951871657754011, recall=0.8666666666666667, fmeasure=0.1287128712871287))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2d6d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bertscore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1492270/4021306630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbertscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bertscore' is not defined"
     ]
    }
   ],
   "source": [
    "bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90205fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5150ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
