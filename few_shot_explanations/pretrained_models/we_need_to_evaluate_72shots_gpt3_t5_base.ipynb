{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f156ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A (hopefully) Simple API for serving explanation score requests.\n",
    "\n",
    "input_string = (\n",
    "    f\"{question} answer: {gold_label}. \"\n",
    "    + f\" explanation: {abstr_expl}.\"\n",
    ")\n",
    "\n",
    "here are some example input strings:\n",
    "\n",
    "If you feel like everything is spinning while climbing you are experiencing what? answer: vertigo. explanation: Vertigo is often experienced while climbing or at heights.\n",
    "Where do you get clothes in a shopping bag? answer: retail store. explanation: For any large item where convenience is beneficial, one might go to a retail store, either a regular one or a big-box store like walmart.\n",
    "Where should a cat be in a house? answer: floor. explanation: A cat should be on the floor, not on a rug.\n",
    "'''\n",
    "import pdb\n",
    "import argparse\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "_model, _tokenizer = None, None\n",
    "\n",
    "model2url = {\n",
    "    'large': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/valloss%3D0.28665~model%3Dt5-large~lr%3D0.0001~seed%3D1~labelagg%3D0_just_weights.pt',\n",
    "    '3b': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/valloss%3D0.28925~model%3Dt5-3b~lr%3D0.0001~seed%3D1~labelagg%3D0_just_weights.pt',\n",
    "    '11b': 'https://storage.googleapis.com/ai2-mosaic-public/projects/few-shot-explanations/pretrained_models/commonsense_qa/cose_deepspeed_valloss%3D0.00000~model%3Dt5-11b~lr%3D0.00001~seed%3D1~labelagg%3D0.pt',\n",
    "}\n",
    "\n",
    "def get_model(model_type, device=None):\n",
    "    global _model, model2url\n",
    "    if model_type not in {'11b', '3b', 'large'}:\n",
    "        raise NotImplementedError('{} is not a valid model please use \"3b\" or \"large\"'.format(model_type))\n",
    "\n",
    "    if _model is None:\n",
    "        hf_model_name = 't5-' + model_type\n",
    "        print('Loading model: this will run only once.')\n",
    "\n",
    "        if model_type == 'large':\n",
    "            model_path = 'csqa_models/t5-large.pt'\n",
    "        elif model_type == '3b':\n",
    "            model_path = 'csqa_models/valloss=0.28925~model=t5-3b~lr=0.0001~seed=1~labelagg=0_just_weights.pt'\n",
    "        elif model_type == '11b':\n",
    "            model_path = 'csqa_models/cose_deepspeed_valloss=0.00000~model=t5-11b~lr=0.00001~seed=1~labelagg=0.pt'\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print('Please download weights for {} model and put in current directory.'.format(model_path))\n",
    "            print('for example, wget {}'.format(model2url[model_type]))\n",
    "            quit()\n",
    "\n",
    "        state = torch.load(model_path)\n",
    "        if 'model_state_dict' in state:\n",
    "            state = state['model_state_dict']\n",
    "\n",
    "        _model = transformers.AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n",
    "        if model_type == '11b': # need to resize due to deepspeed, these entires are not accessed.\n",
    "            _model.resize_token_embeddings(len(transformers.AutoTokenizer.from_pretrained(hf_model_name)))\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "        if device is not None:\n",
    "            _model = _model.to(device)\n",
    "\n",
    "    return _model\n",
    "\n",
    "\n",
    "def get_tokenizer(model_type):\n",
    "    global _tokenizer\n",
    "    if model_type not in {'3b', 'large', '11b'}:\n",
    "        raise NotImplementedError('{} is not a valid model please use \"3b\" or \"large\" or \"11b\"'.format(model_type))\n",
    "\n",
    "    if _tokenizer is None:\n",
    "        hf_model_name = 't5-' + model_type\n",
    "        _tokenizer = transformers.T5TokenizerFast.from_pretrained(hf_model_name)\n",
    "\n",
    "    return _tokenizer\n",
    "\n",
    "\n",
    "class T5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        res = self.tokenizer(self.data[idx]['input'], truncation=True)\n",
    "        res['labels'] = self.tokenizer(self.data[idx]['label']).input_ids\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def get_scores(inputs, model_type, device=None, batch_size=32, verbose=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "      - a list of explanations to score, e.g.,:\n",
    "        premise: A man getting a tattoo on his back. hypothesis: A woman is getting a tattoo. answer: contradiction. explanation: Because the tattoo artist is a man, the person getting the tattoo is not a woman.\n",
    "      - model type, either \"3b\" or \"large\" or \"11b\"\n",
    "      - device: which torch device to load model on, e.g., \"cuda:3\"\n",
    "    Outputs:\n",
    "      - P(good explanation); higher is better\n",
    "    '''\n",
    "    assert model_type in {'large', '3b', '11b'}\n",
    "\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    model = get_model(model_type, device=device)\n",
    "    tokenizer = get_tokenizer(model_type)\n",
    "\n",
    "    score_itr = T5Dataset([{'input': inp, 'label': 'x'} for inp in inputs], tokenizer) # dummy labels for inference\n",
    "    data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "#         return_tensors='pt'\n",
    "    )\n",
    "    score_itr = torch.utils.data.DataLoader(score_itr, shuffle=False, collate_fn=data_collator, batch_size=batch_size)\n",
    "    score_itr = score_itr if not verbose else tqdm.tqdm(score_itr, total=len(score_itr))\n",
    "\n",
    "    good_idx, bad_idx = tokenizer('good').input_ids[0], tokenizer('bad').input_ids[0]\n",
    "    scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in score_itr:\n",
    "            if device is not None:\n",
    "                input_ids, attention_mask, targets = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            model_output = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            logits_pos = model_output['logits'][:, 0, good_idx].cpu().numpy()\n",
    "            logits_neg = model_output['logits'][:, 0, bad_idx].cpu().numpy()\n",
    "            exp_logit_pos, exp_logit_neg = np.exp(logits_pos), np.exp(logits_neg)\n",
    "            score = list([float(x) for x in exp_logit_pos / (exp_logit_pos + exp_logit_neg)])\n",
    "            #pdb.set_trace()\n",
    "            scores.extend(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     '''\n",
    "#     Optional args for main function, mostly just to test.\n",
    "#     '''\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         'model_type',\n",
    "#         default='large',\n",
    "#         choices={'large', '3b', '11b'})\n",
    "#     parser.add_argument(\n",
    "#         '--batch_size',\n",
    "#         default=32,\n",
    "#         type=int)\n",
    "\n",
    "#     args = parser.parse_args(['--batch_size', '1'])\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d73868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\n",
    "#     'model_type',\n",
    "#     default='large',\n",
    "#     choices={'large', '3b', '11b'})\n",
    "# parser.add_argument(\n",
    "#     '--batch_size',\n",
    "#     default=32,\n",
    "#     type=int)\n",
    "\n",
    "# args = parser.parse_args([\"--model_type\", \"3b\"])\n",
    "# args.device = 'cpu'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(1)\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# scores = get_scores(\n",
    "#     ['If you feel like everything is spinning while climbing you are experiencing what? answer: vertigo. explanation: Vertigo is often experienced while climbing or at heights.',\n",
    "#      'Where do you get clothes in a shopping bag? answer: retail store. explanation: For any large item where convenience is beneficial, one might go to a retail store, either a regular one or a big-box store like walmart.',\n",
    "#      'Where should a cat be in a house? answer: floor. explanation: A cat should be on the floor, not on a rug.'],\n",
    "#     'large',\n",
    "#     device='cuda:0',\n",
    "#     batch_size=1,\n",
    "#     verbose=False)\n",
    "# print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56e950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "# with open(\"../../scripts/results/dev_rationale_pair.json\") as f:\n",
    "#     rationale_pair_dev_data = json.load(f)\n",
    "import json\n",
    "file_path = \"../../scripts/results/72shots_cose_t5_base_authorwritten_rationales_generator_test_rationale_pair.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    rationale_pair_dev_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31588f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation', 'our_explanation', 'input_ids', 'attention_mask', 'labels', 'decoder_attention_mask', 'question_encoding', 'common_expl_list', 'generated_explanation']),\n",
       " 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationale_pair_dev_data[0].keys(), len(rationale_pair_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e5e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer('3b')\n",
    "# input_list = []\n",
    "# for da in rationale_pair_dev_data:\n",
    "#     input_list.append(tokenizer.decode(da['input_ids']))\n",
    "# input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166a2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                               | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: this will run only once.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [01:33<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "qae_list = []\n",
    "score_list = []\n",
    "for da in tqdm(rationale_pair_dev_data, total=len(rationale_pair_dev_data)):\n",
    "    qae = \"{} answer: {} explanation: {}\".format(da['question'], \n",
    "                                                 da['answer'], \n",
    "                                                 da['generated_explanation'])\n",
    "\n",
    "    scores = get_scores(\n",
    "        [qae],\n",
    "        '3b',\n",
    "        device='cuda:0',\n",
    "        batch_size=1,\n",
    "        verbose=False)\n",
    "    score_list.append(scores[0])\n",
    "#     if scores[0] > 0.7 or scores[0] < 0.2:\n",
    "#         print(\"question: {}\".format(da['question']))\n",
    "#         print(\"answer: {}\".format(da['answer']))\n",
    "#         print(\"common_expl_list: {}\".format(da['common_expl_list']))\n",
    "#         print(\"generated_explanation: {}\".format(da['generated_explanation']))\n",
    "#         print(\"score: {}\".format(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc4649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48197704553604126,\n",
       " 0.4751887321472168,\n",
       " 0.7109125852584839,\n",
       " 0.2539403438568115,\n",
       " 0.4865556061267853,\n",
       " 0.5775584578514099,\n",
       " 0.48812931776046753,\n",
       " 0.4806748628616333,\n",
       " 0.3504553735256195,\n",
       " 0.7061419486999512,\n",
       " 0.4660319685935974,\n",
       " 0.6553679704666138,\n",
       " 0.441586971282959,\n",
       " 0.7558452486991882,\n",
       " 0.569264829158783,\n",
       " 0.3123489320278168,\n",
       " 0.6885761618614197,\n",
       " 0.37269169092178345,\n",
       " 0.655646562576294,\n",
       " 0.2662007510662079,\n",
       " 0.5292171835899353,\n",
       " 0.5181504487991333,\n",
       " 0.3223721981048584,\n",
       " 0.6298381090164185,\n",
       " 0.7014867663383484,\n",
       " 0.7689418196678162,\n",
       " 0.40670937299728394,\n",
       " 0.36141350865364075,\n",
       " 0.5203423500061035,\n",
       " 0.5924235582351685,\n",
       " 0.7482343316078186,\n",
       " 0.1661176234483719,\n",
       " 0.6707531213760376,\n",
       " 0.35503295063972473,\n",
       " 0.27525031566619873,\n",
       " 0.45615118741989136,\n",
       " 0.6191800832748413,\n",
       " 0.5054376721382141,\n",
       " 0.6741573214530945,\n",
       " 0.5814212560653687,\n",
       " 0.42095813155174255,\n",
       " 0.318418949842453,\n",
       " 0.3683096468448639,\n",
       " 0.5126813650131226,\n",
       " 0.2846122086048126,\n",
       " 0.3638990521430969,\n",
       " 0.33458852767944336,\n",
       " 0.23061800003051758,\n",
       " 0.7164623141288757,\n",
       " 0.5996255874633789,\n",
       " 0.5551517009735107,\n",
       " 0.4285335838794708,\n",
       " 0.4428534507751465,\n",
       " 0.6968713402748108,\n",
       " 0.47134917974472046,\n",
       " 0.5891859531402588,\n",
       " 0.4632242023944855,\n",
       " 0.7242011427879333,\n",
       " 0.6571639776229858,\n",
       " 0.7147301435470581,\n",
       " 0.6165731549263,\n",
       " 0.53227698802948,\n",
       " 0.6562258005142212,\n",
       " 0.6583285331726074,\n",
       " 0.5593743324279785,\n",
       " 0.681538999080658,\n",
       " 0.5760998725891113,\n",
       " 0.6652213335037231,\n",
       " 0.49551430344581604,\n",
       " 0.6582049131393433,\n",
       " 0.7994768619537354,\n",
       " 0.5033342242240906,\n",
       " 0.49329906702041626,\n",
       " 0.5321183800697327,\n",
       " 0.5068226456642151,\n",
       " 0.5307058691978455,\n",
       " 0.44952407479286194,\n",
       " 0.32524967193603516,\n",
       " 0.36911529302597046,\n",
       " 0.5089238286018372,\n",
       " 0.4938366115093231,\n",
       " 0.6189852952957153,\n",
       " 0.20704995095729828,\n",
       " 0.5254420042037964,\n",
       " 0.6266278028488159,\n",
       " 0.7647788524627686,\n",
       " 0.6126424074172974,\n",
       " 0.5912400484085083,\n",
       " 0.3251977562904358,\n",
       " 0.7528831958770752,\n",
       " 0.44697830080986023,\n",
       " 0.7131333947181702,\n",
       " 0.401102215051651,\n",
       " 0.6160196661949158,\n",
       " 0.6074737310409546,\n",
       " 0.5165168642997742,\n",
       " 0.47607138752937317,\n",
       " 0.6853421330451965,\n",
       " 0.5034458041191101,\n",
       " 0.49942994117736816,\n",
       " 0.5903664827346802,\n",
       " 0.4267909526824951,\n",
       " 0.5575641989707947,\n",
       " 0.2709810733795166,\n",
       " 0.6221041679382324,\n",
       " 0.2815023362636566,\n",
       " 0.4785415530204773,\n",
       " 0.4111642837524414,\n",
       " 0.12904420495033264,\n",
       " 0.7722104787826538,\n",
       " 0.5226859450340271,\n",
       " 0.6674926280975342,\n",
       " 0.4649791717529297,\n",
       " 0.41392505168914795,\n",
       " 0.4179559350013733,\n",
       " 0.7678449749946594,\n",
       " 0.7751962542533875,\n",
       " 0.5058605074882507,\n",
       " 0.5145674347877502,\n",
       " 0.7485029697418213,\n",
       " 0.4287078380584717,\n",
       " 0.4572518467903137,\n",
       " 0.2988218367099762,\n",
       " 0.8068740367889404,\n",
       " 0.5031924843788147,\n",
       " 0.5129385590553284,\n",
       " 0.6293508410453796,\n",
       " 0.6465983986854553,\n",
       " 0.530434787273407,\n",
       " 0.5139157176017761,\n",
       " 0.6334758996963501,\n",
       " 0.5687405467033386,\n",
       " 0.7877193093299866,\n",
       " 0.4605891704559326,\n",
       " 0.3934844732284546,\n",
       " 0.43207332491874695,\n",
       " 0.37463271617889404,\n",
       " 0.7187595963478088,\n",
       " 0.6523098945617676,\n",
       " 0.6369432806968689,\n",
       " 0.5656126141548157,\n",
       " 0.5657636523246765,\n",
       " 0.5144543647766113,\n",
       " 0.739104688167572,\n",
       " 0.32499930262565613,\n",
       " 0.5532137155532837,\n",
       " 0.7005438804626465,\n",
       " 0.5443674921989441,\n",
       " 0.6492280960083008,\n",
       " 0.437837690114975,\n",
       " 0.6787513494491577,\n",
       " 0.8089098930358887,\n",
       " 0.7303385734558105,\n",
       " 0.4143460988998413,\n",
       " 0.5186890363693237,\n",
       " 0.26023805141448975,\n",
       " 0.6500672101974487,\n",
       " 0.5874797105789185,\n",
       " 0.4927869141101837,\n",
       " 0.4806678295135498,\n",
       " 0.7020749449729919,\n",
       " 0.6524250507354736,\n",
       " 0.6125023365020752,\n",
       " 0.3550945222377777,\n",
       " 0.6422481536865234,\n",
       " 0.6628668308258057,\n",
       " 0.7302303910255432,\n",
       " 0.5392207503318787,\n",
       " 0.7950505018234253,\n",
       " 0.714290201663971,\n",
       " 0.49453482031822205,\n",
       " 0.6076629757881165,\n",
       " 0.7216103672981262,\n",
       " 0.38800156116485596,\n",
       " 0.31327396631240845,\n",
       " 0.6767171621322632,\n",
       " 0.7655665874481201,\n",
       " 0.6087573766708374,\n",
       " 0.5914545655250549,\n",
       " 0.4149966835975647,\n",
       " 0.5823788642883301,\n",
       " 0.7568724155426025,\n",
       " 0.5956164002418518,\n",
       " 0.44588905572891235,\n",
       " 0.2902551591396332,\n",
       " 0.6602057814598083,\n",
       " 0.6622161269187927,\n",
       " 0.3467789590358734,\n",
       " 0.6897139549255371,\n",
       " 0.4964716136455536,\n",
       " 0.6709176898002625,\n",
       " 0.27685701847076416,\n",
       " 0.5377417206764221,\n",
       " 0.5642654299736023,\n",
       " 0.6580086946487427,\n",
       " 0.4853988587856293,\n",
       " 0.4843564033508301,\n",
       " 0.5808377861976624,\n",
       " 0.4438476264476776,\n",
       " 0.29475918412208557,\n",
       " 0.6765947937965393]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636ca2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5389320851855017, 0.53227698802948)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score_list), np.median(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb020b",
   "metadata": {},
   "source": [
    "## evaluate generated rationale with bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef33d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "bertscore_metric = datasets.load_metric(\"bertscore\")\n",
    "rouge_metric = datasets.load_metric('rouge')\n",
    "bleu_metric = datasets.load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b285d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7708d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/201 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2160911/1146563988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlist_gold_expl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_expl_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbert_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertscore_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_expl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_gold_expl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_expl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_gold_expl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrouge_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_expl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_gold_expl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_gold_expl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"references\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/datasets_modules/metrics/bertscore/c577515ae6010ad82f9506bc4b44c36c19d5f391e93251d2c08fc4059fa918e7/bertscore.py\u001b[0m in \u001b[0;36m_compute\u001b[0;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mrescale_with_baseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrescale_with_baseline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mbaseline_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 )\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/bert_score/scorer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, num_layers, batch_size, nthreads, all_layers, idf, idf_sents, device, lang, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Building model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_fast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_fast_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_fast_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/bert_score/utils.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m(model_type, use_fast)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_version\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fast tokenizer is not available for version < 4.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     raise ValueError(\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m                         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m                         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m                         \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m                     )\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         )\n\u001b[1;32m   1380\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1592\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                     raise ValueError(\n\u001b[0;32m-> 1594\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1595\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m                     )\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "bert_scores = []\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for da in tqdm(rationale_pair_dev_data, total=len(rationale_pair_dev_data)):\n",
    "    generated_expl = da['generated_explanation']\n",
    "    common_expl_list = da['common_expl_list']\n",
    "    pred_expl = generated_expl.split(\"<extra_id_0> \")[1].split(\"<extra_id_1>\")[0]\n",
    "    list_gold_expl = [l.lower() for l in common_expl_list]\n",
    "    \n",
    "    bert_score = bertscore_metric.compute(predictions=[pred_expl.lower()], references=[list_gold_expl], lang=\"en\")[\"f1\"][0]*100\n",
    "    bleu_score = bleu_metric.compute(predictions=[pred_expl.lower()], references=[list_gold_expl])['score']\n",
    "    rouge_score = rouge_metric.compute(predictions=[pred_expl.lower()]*len(list_gold_expl), references=list_gold_expl)\n",
    "    rouge1_score = rouge_score[\"rouge1\"].mid.fmeasure\n",
    "    rouge2_score = rouge_score[\"rouge2\"].mid.fmeasure\n",
    "    rougeL_score = rouge_score[\"rougeL\"].mid.fmeasure\n",
    "    bert_scores.append(bert_score)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    rouge1_scores.append(rouge1_score)\n",
    "    rouge2_scores.append(rouge2_score)\n",
    "    rougeL_scores.append(rougeL_score)\n",
    "    \n",
    "    \n",
    "#     #print(generated_expl)\n",
    "#     #print(generated_expl.split(\"<extra_id_0> \")[1].split(\"<extra_id_1>\")[0])\n",
    "#     instance_bertscores = []\n",
    "#     for gold_expl in list_gold_expl: \n",
    "#         score = bertscore_metric.compute(predictions=[pred_expl.lower()]*len(), references=[gold_expl.lower()], lang=\"en\")[\"f1\"][0]*100\n",
    "#         instance_bertscores.append(score)\n",
    "#     bertscores.append(np.mean(instance_bertscores))\n",
    "    \n",
    "#     bleuscore = bleu_score(pred_expl, list_gold_expl)\n",
    "#     bleuscores.append(bleuscore)\n",
    "    \n",
    "#     rougescore = rouge(pred_expl, list_gold_expl)\n",
    "#     rouge1_scores.append(rougescore['rouge1_fmeasure'].numpy()[0])\n",
    "#     rouge2_scores.append(rougescore['rouge2_fmeasure'].numpy()[0])\n",
    "#     rougeL_scores.append(rougescore['rougeL_fmeasure'].numpy()[0])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8019a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bert_score: {}\".format(np.mean(bert_scores)))\n",
    "print(\"bleu_score: {}\".format(np.mean(bleu_scores)))\n",
    "print(\"rouge1_score: {}\".format(np.mean(rouge1_scores)))\n",
    "print(\"rouge2_score: {}\".format(np.mean(rouge2_scores)))\n",
    "print(\"rougeL_score: {}\".format(np.mean(rougeL_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "bertscore = np.mean(bertscores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90205fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5150ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
