{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb91f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --task_name [esnli, cos_e] \n",
    "# --do_train \n",
    "# --num_train_epochs 200 \n",
    "# --per_device_train_batch_size 64 \n",
    "# --per_device_eval_batch_size 64 \n",
    "# --logging_first_step True \n",
    "# --logging_steps 1 \n",
    "# --save_steps 1 \n",
    "# --save_total_limit 11 \n",
    "# --seed 42 \n",
    "# --early_stopping_threshold 10 \n",
    "# --version_name [for cos_e, specify v1.0 or v1.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad60a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlp in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (5.0.0)\n",
      "Requirement already satisfied: dill in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: pandas in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (1.1.5)\n",
      "Requirement already satisfied: xxhash in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (2.0.2)\n",
      "Requirement already satisfied: numpy in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from nlp) (4.62.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from pandas->nlp) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224d590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tunes a model to jointly generate labels + rationales given input.\n",
    "Partially based on https://github.com/huggingface/transformers/tree/7cb203fae4e7964e9e99400b375d660ebce765ee/examples/language-modeling/run_language_modeling.py (Huggingface Transformers v2.9.1)\n",
    "See Huggingface repository for licensing agreement.\n",
    "\n",
    "Code formatted using https://github.com/psf/black\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from feature_conversion_methods import input_to_explanation_plus_label\n",
    "from modeling_t5 import T5ForConditionalGeneration as NoisyT5ForConditionalGeneration\n",
    "from trainer import Trainer\n",
    "from custom_args import (\n",
    "    DataTrainingArguments,\n",
    "    ModelArguments,\n",
    "    compute_metrics,\n",
    "    compare_models_with_noise,\n",
    ")\n",
    "import torch\n",
    "import nlp\n",
    "import git\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SequenceCollator:\n",
    "    def __init__(self, pad_token):\n",
    "        self.pad_token_mapping = {\n",
    "            \"lm_labels\": -100,\n",
    "            \"attention_mask\": 0,\n",
    "            \"decoder_attention_mask\": 0,\n",
    "            \"input_ids\": pad_token,\n",
    "        }\n",
    "        self.columns = [\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"lm_labels\",\n",
    "            \"decoder_attention_mask\",\n",
    "        ]\n",
    "\n",
    "    def collate_batch(self, examples):\n",
    "\n",
    "        # batch inputs for training\n",
    "        batch = {}\n",
    "        for key in examples[0].keys():\n",
    "            if key in self.columns:\n",
    "                tmp_list = []\n",
    "                for item in examples:\n",
    "                    tmp_list.append(item[key])\n",
    "\n",
    "                # pad lists to max length\n",
    "                if isinstance(tmp_list[0], list):\n",
    "                    max_length = max(map(len, tmp_list))\n",
    "                    tmp_list = [\n",
    "                        el + [self.pad_token_mapping[key]] * (max_length - len(el))\n",
    "                        for el in tmp_list\n",
    "                    ]\n",
    "\n",
    "                batch[key] = torch.tensor(tmp_list, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf81451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list |grep tran\n",
    "import transformers\n",
    "from  transformers.models.t5 import configuration_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e7438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "\n",
    "og_start_time = time.time()\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses([\"--task_name\", \"cos_e\", \n",
    "                                                                          \"--do_train\", \"True\", \n",
    "                                                                          \"--num_train_epochs\", \"200\", \n",
    "                                                                          \"--per_gpu_train_batch_size\", \"64\",\n",
    "                                                                          \"--per_gpu_eval_batch_size\", \"64\", \n",
    "                                                                          \"--logging_first_step\", \"True\", \n",
    "                                                                          \"--logging_steps\", \"1\", \n",
    "                                                                          \"--save_steps\", \"1\", \n",
    "                                                                          \"--save_total_limit\", \"11\", \n",
    "                                                                          \"--seed\", \"42\", \n",
    "                                                                          \"--early_stopping_threshold\", \"10\", \n",
    "                                                                          \"--version_name\", \"v1.11\",\n",
    "                                                                          \"--output_dir\", \"./output_dir\",\n",
    "                                                                          ])\n",
    "\n",
    "# training_args.set_device(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e76ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training_args.do_train:\n",
    "    if (not model_args.pretrained_model_file) and (\n",
    "        not data_args.generations_filepath\n",
    "    ):\n",
    "        raise Exception(\n",
    "            \"if not training a model from scratch, must specify a trained model to load for evaluation or generations in a file to evaluate\"\n",
    "        )\n",
    "\n",
    "# make sure only one dataset split pick if manually specifying evaluation file\n",
    "if data_args.generations_filepath is not None:\n",
    "    training_args.do_train = False\n",
    "    training_args.do_eval = False\n",
    "    if \"train\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = True\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = False\n",
    "    elif \"test\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = True\n",
    "        data_args.dev_predict = False\n",
    "    elif \"validation\" in data_args.generations_filepath:\n",
    "        data_args.train_predict = False\n",
    "        data_args.test_predict = False\n",
    "        data_args.dev_predict = True\n",
    "\n",
    "# create a new directory if fine-tuning an existing checkpoint or training/evaluating a HF pretrained model\n",
    "# do not do this when re-evaluating a pretrained_model_file\n",
    "if training_args.do_train or (\n",
    "    not model_args.pretrained_model_file and not data_args.generations_filepath\n",
    "):\n",
    "    # create a save directory and a logfile\n",
    "    save_path = training_args.output_dir\n",
    "    training_args.output_dir = os.path.join(\n",
    "        save_path, datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "    )\n",
    "    training_args.logging_dir = training_args.output_dir\n",
    "    assert os.path.exists(save_path)\n",
    "    assert not os.path.exists(training_args.output_dir)\n",
    "    os.makedirs(training_args.output_dir)\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    handlers = [\n",
    "        logging.FileHandler(os.path.join(training_args.output_dir, \"logger.log\")),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "else:\n",
    "    # don't overwrite existing logfile or create new directory\n",
    "    training_args.output_dir = model_args.pretrained_model_file\n",
    "    handlers = [logging.StreamHandler()]\n",
    "\n",
    "if data_args.encoder_noise_variance is not None:\n",
    "    # must be in evaluation mode\n",
    "    assert not training_args.do_train\n",
    "    assert model_args.pretrained_model_file is not None\n",
    "    assert data_args.test_predict or data_args.dev_predict\n",
    "    assert 40 > data_args.encoder_noise_variance > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf11c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:07:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/22/2022 21:07:27 - INFO - __main__ -   Save path: ./output_dir/082222_210727\n",
      "08/22/2022 21:07:27 - INFO - __main__ -   Git branch: dev\n",
      "08/22/2022 21:07:27 - INFO - __main__ -   Git hash: 640aa0d57986f7e8295dabb0e8ab189542f32d53\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    handlers=handlers,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Save path: %s\" % training_args.output_dir)\n",
    "\n",
    "# get git hash and branch where deployed\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_hash = repo.head.object.hexsha\n",
    "git_branch = repo.active_branch.name\n",
    "logger.info(\"Git branch: %s\" % git_branch)\n",
    "logger.info(\"Git hash: %s\" % git_hash)\n",
    "\n",
    "assert data_args.task_name in {\"cos_e\", \"esnli\"}\n",
    "\n",
    "# set gradient accumulation steps to always use batch size == 64\n",
    "if 64 % training_args.per_gpu_train_batch_size != 0:\n",
    "    raise Exception(\n",
    "        \"Batch size is not a divisor of 64, resulting in inconsistent gradient-accumulation behavior\"\n",
    "    )\n",
    "training_args.gradient_accumulation_steps = int(\n",
    "    64 / training_args.per_gpu_train_batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9edf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    # write command and args to file\n",
    "    with open(\n",
    "        os.path.join(training_args.output_dir, \"commandline_args.txt\"), \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"Git branch: \" + git_branch + \"\\n\")\n",
    "        f.write(\"Git hash: \" + git_hash + \"\\n\")\n",
    "        f.write(\"Command:\\n\")\n",
    "        f.write(\"\\n\".join(sys.argv[1:]))\n",
    "        f.write(\"Training args:\\n\")\n",
    "        # make training_args dict writeable\n",
    "        tmp = training_args.__dict__\n",
    "        tmp.pop(\"__cached__setup_devices\", None)\n",
    "        tmp.pop(\"evaluation_strategy\", None)\n",
    "        tmp.pop(\"lr_scheduler_type\", None)\n",
    "        tmp.pop(\"logging_strategy\", None)\n",
    "        tmp.pop(\"save_strategy\", None)\n",
    "        json.dump(tmp, f, indent=2)\n",
    "        f.write(\"Data args:\\n\")\n",
    "        json.dump(data_args.__dict__, f, indent=2)\n",
    "        f.write(\"Model args:\\n\")\n",
    "        json.dump(model_args.__dict__, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abeb8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:07:27 - INFO - __main__ -   Loading pretrained tokenizer...\n",
      "/home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has [EOS]. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
      "08/22/2022 21:07:34 - INFO - __main__ -   Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "logger.info(\"Loading pretrained tokenizer...\")\n",
    "if model_args.pretrained_model_file:\n",
    "    # load pretrained tokenizer from directory\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_args.pretrained_model_file)\n",
    "else:\n",
    "    # load pretrained tokenizer from Huggingface\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# found better/more controllable generation using own EOS token\n",
    "tokenizer.add_special_tokens({\"eos_token\": \"[EOS]\"})\n",
    "assert (\n",
    "    len(tokenizer) - 1\n",
    "    == tokenizer.eos_token_id\n",
    "    == tokenizer.encode([\"[EOS]\"])[0]\n",
    "    == 32100\n",
    ")\n",
    "\n",
    "if data_args.generations_filepath is None:\n",
    "    if model_args.pretrained_model_file:\n",
    "        # load pretrained model from directory at best checkpoint\n",
    "        ckpts = [\n",
    "            name\n",
    "            for name in os.listdir(model_args.pretrained_model_file)\n",
    "            if PREFIX_CHECKPOINT_DIR in name\n",
    "        ]\n",
    "        if len(ckpts) != 1:\n",
    "            raise Exception(\n",
    "                \"more than 1 checkpoint file stored in pretrained path. revisit save directory\"\n",
    "            )\n",
    "        model_load_path = os.path.join(model_args.pretrained_model_file, ckpts[0])\n",
    "        if data_args.encoder_noise_variance is not None:\n",
    "            # initialize model with noise in decoder\n",
    "            model = NoisyT5ForConditionalGeneration.from_pretrained(model_load_path)\n",
    "        else:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(model_load_path)\n",
    "        if model_args.dropout_rate:\n",
    "            raise Exception(\n",
    "                \"can't update/specify dropout currently when load pretrained model from directory\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        # load pretrained model from HuggingFace\n",
    "        logger.info(\"Loading pretrained model\")\n",
    "        if model_args.dropout_rate:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                \"t5-base\", dropout_rate=model_args.dropout_rate\n",
    "            )\n",
    "        else:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    model = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5d71ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:07:43 - INFO - nlp.load -   Checking /home/huangyongfeng/.cache/huggingface/datasets/6f5af2f9ee4124cd3ebff0ac93fa2e94ad4bd20a828fb796875668a2c86cd09d.c298e1f9b6e456b221e78032db5488e2e3693908eecf4bb3babe8632f5a15c58.py for additional imports.\n",
      "08/22/2022 21:07:43 - INFO - filelock -   Lock 139747575242704 acquired on /home/huangyongfeng/.cache/huggingface/datasets/6f5af2f9ee4124cd3ebff0ac93fa2e94ad4bd20a828fb796875668a2c86cd09d.c298e1f9b6e456b221e78032db5488e2e3693908eecf4bb3babe8632f5a15c58.py.lock\n",
      "08/22/2022 21:07:43 - INFO - nlp.load -   Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cos_e/cos_e.py at /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e\n",
      "08/22/2022 21:07:43 - INFO - nlp.load -   Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cos_e/cos_e.py at /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b\n",
      "08/22/2022 21:07:43 - INFO - nlp.load -   Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cos_e/cos_e.py to /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/cos_e.py\n",
      "08/22/2022 21:07:43 - INFO - nlp.load -   Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cos_e/dataset_infos.json to /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/dataset_infos.json\n",
      "08/22/2022 21:07:43 - INFO - nlp.load -   Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cos_e/cos_e.py at /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/cos_e.json\n",
      "08/22/2022 21:07:43 - INFO - filelock -   Lock 139747575242704 released on /home/huangyongfeng/.cache/huggingface/datasets/6f5af2f9ee4124cd3ebff0ac93fa2e94ad4bd20a828fb796875668a2c86cd09d.c298e1f9b6e456b221e78032db5488e2e3693908eecf4bb3babe8632f5a15c58.py.lock\n",
      "08/22/2022 21:07:43 - INFO - nlp.info -   Loading Dataset Infos from /home/huangyongfeng/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/nlp/datasets/cos_e/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b\n",
      "08/22/2022 21:07:43 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\n",
      "08/22/2022 21:07:43 - INFO - nlp.info -   Loading Dataset info from /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b\n",
      "08/22/2022 21:07:43 - INFO - nlp.builder -   Reusing dataset cos_e (/home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b)\n",
      "08/22/2022 21:07:43 - INFO - nlp.builder -   Constructing Dataset for split train, validation, from /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b\n",
      "08/22/2022 21:07:43 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\n",
      "08/22/2022 21:07:43 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\n",
      "08/22/2022 21:07:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/cache-5c92b2d24be3dd7e75a591c2dd205423.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025302886962890625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9741,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3312c1b5f4214896b479a28dd248cf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:08:03 - INFO - nlp.arrow_writer -   Done writing 9741 examples in 15485109 bytes /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/tmpj0v23g94.\n",
      "08/22/2022 21:08:03 - INFO - nlp.arrow_dataset -   Caching processed dataset at /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/cache-34a579c7f4f154a1967186fa19e1b06f.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0608973503112793,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1221,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9bb4eb47564cc3a7326c992d3d1f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:08:05 - INFO - nlp.arrow_writer -   Done writing 1221 examples in 1900029 bytes /home/huangyongfeng/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/b7bc6748714e9af308ab02e900cb2b020b953b3cc865f7901164bb963a7e694b/tmp4fdagrf6.\n"
     ]
    }
   ],
   "source": [
    "# load (new) cos-e version\n",
    "if data_args.task_name == \"cos_e\":\n",
    "    assert data_args.version_name in {\"v1.11\", \"v1.0\"}\n",
    "    version_arg = data_args.version_name\n",
    "else:\n",
    "    version_arg = None\n",
    "\n",
    "# Get datasets\n",
    "dataset = nlp.load_dataset(data_args.task_name, version_arg)\n",
    "\n",
    "# Apply method, and format dataset to torch.Tensor outputs\n",
    "for split in dataset.keys():\n",
    "\n",
    "    # apply independently to each example\n",
    "    dataset[split] = dataset[split].map(\n",
    "        lambda x: input_to_explanation_plus_label(\n",
    "            x,\n",
    "            tokenizer,\n",
    "            datasource=data_args.task_name,\n",
    "            expl_only=model_args.rationale_only,\n",
    "            label_only=model_args.label_only,\n",
    "        ),\n",
    "        # had some replicability issues with batch/cache set to True\n",
    "        batched=False,\n",
    "        load_from_cache_file=False,\n",
    "    )\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"] if data_args.task_name == \"esnli\" else None\n",
    "\n",
    "if data_args.task_name == \"esnli\":\n",
    "    assert len(train_dataset) == 549367\n",
    "    assert len(eval_dataset) == 9842\n",
    "    assert len(test_dataset) == 9824\n",
    "elif data_args.task_name == \"cos_e\":\n",
    "    if data_args.version_name == \"v1.11\":\n",
    "        assert len(train_dataset) == 9741\n",
    "        assert len(eval_dataset) == 1221\n",
    "    elif data_args.version_name == \"v1.0\":\n",
    "        assert len(train_dataset) == 7610\n",
    "        assert len(eval_dataset) == 950\n",
    "    assert test_dataset is None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2022 21:08:05 - INFO - __main__ -   ****LOG****\n",
      "08/22/2022 21:08:05 - INFO - __main__ -   9741\n",
      "08/22/2022 21:08:05 - INFO - __main__ -   1221\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "08/22/2022 21:08:13 - INFO - trainer -   ***** Running training *****\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Num examples = 9741\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Num Epochs = 200\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Instantaneous batch size per device = 64\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Total train batch size (w. accumulation) = 64\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Gradient Accumulation steps = 1\n",
      "08/22/2022 21:08:13 - INFO - trainer -     Total optimization steps = 30600\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02111339569091797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf638b0fc4bb47eea6c237333e019568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02142643928527832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Iteration",
       "rate": null,
       "total": 153,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dc635d6bb747a48107dea421672e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/cognitive_comp/huangyongfeng/evaluate_LM_with_rationalization/label_rationale_association/trainer.py\u001b[0m(398)\u001b[0;36m_training_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    396 \u001b[0;31m            \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    397 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 398 \u001b[0;31m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    399 \u001b[0;31m        \u001b[0;31m# model outputs are a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    400 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> outputs = model(**inputs)\n",
      "*** ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
      "ipdb> inputs.keys()\n",
      "dict_keys(['attention_mask', 'decoder_attention_mask', 'input_ids'])\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"****LOG****\")\n",
    "logger.info(len(train_dataset))\n",
    "logger.info(len(eval_dataset))\n",
    "if data_args.task_name == \"esnli\":\n",
    "    logger.info(len(test_dataset))\n",
    "\n",
    "if data_args.generations_filepath is None:\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_args=data_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        prediction_loss_only=True,\n",
    "        data_collator=SequenceCollator(pad_token=tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    train_time = time.time() - start_time\n",
    "    model = trainer.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7e6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
